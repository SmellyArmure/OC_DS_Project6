{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Brouillons.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyP91Jg4rXV94doTN3KGANi3"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"sxaj1pdB6l-p"},"source":["# Version qui marche mais la sortie de .cv_results_ n'a pas le mÃªme format qu'un\n","# gridsearch standard\n","\n","'''\n","Class to optimize clustering score.\n","Instantiate with a clusterer (estimator), a grid parameter (param_grid)\n","and a scoring function or a dictionary of functions (scoring)\n","'''\n","\n","import numpy as np\n","from sklearn.base import BaseEstimator, TransformerMixin\n","from sklearn.model_selection import ParameterGrid\n","\n","class GridSearchClust(BaseEstimator, TransformerMixin):\n","\n","    def __init__(self, estimator, param_grid_estim, param_grid_preproc=None,\n","                 scoring=None, refit=silhouette_score, greater_is_better=True):\n","\n","        # Getting parameters\n","        self.estimator = estimator\n","        self.param_grid_estim = param_grid_estim\n","        self.param_grid_preproc = param_grid_preproc\n","        self.scoring = scoring\n","        self.refit = refit\n","        self.greater_is_better = greater_is_better\n","\n","    def fit(self, X, verbose=False):\n","\n","        # Initialization of the dict of results\n","        self.results_ = {\"scores\": [],\n","                         \"params\": [],\n","                         \"models\": [],\n","                        #  \"fit_times\": [],\n","                         \"nb_clusters\": [],\n","                         \"refit_score\": []}\n","\n","        # iterating upon all combinations of parameters\n","        for param in ParameterGrid(self.param_grid_estim):\n","\n","            # instanciation of the model with selected parameters\n","            model = self.estimator.set_params(**param)\n","\n","            # fitting the model\n","            model.fit(X)\n","\n","            # computing labels\n","            labels = model.labels_\n","\n","            # # Measuring training time while fitting the model on the data\n","            # time_train = %timeit -n1 -r1 -o -q model.fit(X)\n","            # time_train = time_train.average\n","\n","            # Refit score\n","            try:\n","                refit_score = self.refit(X, labels) # self.scoring['silh'](X, labels)\n","            except:\n","                refit_score = np.nan\n","            \n","            # Other scores (scoring)\n","            model_score = {}\n","            if not self.scoring:  # if scoring parameter not defined\n","                model_score['score'] = model.score(X) # default score\n","            else:  # if scoring parameter is defined\n","                if type(self.scoring) != dict:\n","                    self.scoring = {'score': self.scoring}\n","                else:\n","                    for n_sco, sco in self.scoring.items():\n","                        try:\n","                            model_score[n_sco] = sco(X, labels)\n","                        except:\n","                            model_score[n_sco] = np.nan\n","            if verbose: print(model_score)\n","            \n","            # Computing number of clusters, excluding noise (#-1)\n","            nb_clusters = \\\n","                len(set(model.labels_)) - (1 if -1 in set(model.labels_) else 0)\n","            nb_clusters = int(nb_clusters)\n","            if verbose: print(nb_clusters)\n","\n","            # saving results, parameters and models in a dict\n","            self.results_[\"refit_score\"].append(refit_score)  # refit score\n","            self.results_[\"scores\"].append(model_score)  # dict of scores\n","            self.results_[\"params\"].append(param)  # parameters\n","            self.results_[\"models\"].append(model)  # trained models\n","            # self.results_[\"fit_times\"].append(time_train)  # training time\n","            self.results_[\"nb_clusters\"].append(nb_clusters)  # nb of clusters\n","        \n","        # Selecting best model based on the refit_score\n","        # -----------------------------------\n","        # initialisation\n","        best_model_index, best_score = None, None  \n","        # iterating over scores\n","        for index, score in enumerate(self.results_[\"refit_score\"]):\n","\n","            # initialisation\n","            if not best_score:\n","                best_score = score\n","                best_model_index = index\n","\n","            # if score is better than current best_score\n","            cond = score > best_score if self.greater_is_better\\\n","                                                 else score < best_score\n","            if cond:\n","                    # update the current best_score and current best_model_index\n","                    best_score = score\n","                    best_model_index = index\n","        \n","        # Update attributes of the instance\n","        self.best_refit_score_ = self.results_[\"refit_score\"][best_model_index]\n","        self.best_score_ = self.results_[\"scores\"][best_model_index]\n","        self.best_params_ = self.results_[\"params\"][best_model_index]\n","        self.best_estimator_ = self.results_[\"models\"][best_model_index]\n","        self.best_index_ = best_model_index\n","        # self.refit_time_ = self.results_[\"fit_times\"][best_model_index]\n","\n","        # refit the best model\n","        self.best_estimator_.fit(X)\n","        \n","        return self\n","\n","    def predict(self, X_test):\n","\n","        # use the .predict method of the estimator on the best model\n","        return self.best_model.predict(X_test)"],"execution_count":null,"outputs":[]}]}