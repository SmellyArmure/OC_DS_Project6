{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"P6_Cleaning_NLP.ipynb","provenance":[{"file_id":"1qAxkvKD2odWAQUnotM2Dwijh45Ko0iCO","timestamp":1587461379791},{"file_id":"1wX6ZnTUKkvXaWwnL6E51NNuJkxEdSK-h","timestamp":1582132452639},{"file_id":"144JCi9-nMiX9eD3AccG2sgaTbug2wQTa","timestamp":1581758429991},{"file_id":"1kRjoSOVhLf1GbPOY6C2MGdpHdjUsJGe1","timestamp":1581430098593},{"file_id":"1SzuDOE2ejfYYNHpdu1hTrvQZd5phpvyP","timestamp":1581092731309},{"file_id":"1NHY7TNgChDa8i5eggkSxZV133o6RB2pP","timestamp":1580472396109},{"file_id":"1CjFqLqI3e83aWkErpy2_tDoqWWSc5V7K","timestamp":1567509524556},{"file_id":"1oFtNqY9sTtyX09HnsLY5GOpchE7TWSLc","timestamp":1567440734485},{"file_id":"1kO3qnFJ8XAhA2WzueAy6Gwr0KwfoSF2z","timestamp":1566893631574},{"file_id":"1rI7P6dn7-IGK6p8HX7dvNP93roGSzXeX","timestamp":1566833630097},{"file_id":"1cAbXwtjxfOIVmnecCFlHrVryskoYJCXA","timestamp":1566734390006},{"file_id":"https://github.com/SmellyArmure/PROJECT3/blob/master/NOTEBOOKS/P3_Cleaning_v1_0.ipynb","timestamp":1566726579304}],"collapsed_sections":[],"toc_visible":true,"machine_shape":"hm"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"pVbrKjWrGl4m"},"source":["# \"Classifiez automatiquement des biens de consommation\"\n","_NLP Cleaning Notebook_"]},{"cell_type":"markdown","metadata":{"id":"AXy2xt5wB3ZD"},"source":["## 0 Preliminaries"]},{"cell_type":"markdown","metadata":{"id":"NQ8_ZaJvGl4o"},"source":["### 0.0 Importing Packages and Modules"]},{"cell_type":"markdown","metadata":{"id":"XKUzl6mHOcZa"},"source":["Checking whether the notebook is on Colab or PC"]},{"cell_type":"code","metadata":{"id":"S7MAxokr4UmP","executionInfo":{"status":"ok","timestamp":1603368292430,"user_tz":-120,"elapsed":1857,"user":{"displayName":"Maryse Muller","photoUrl":"","userId":"13230828717959946182"}},"outputId":"b533d4f6-bb04-43b1-fc2b-84fb45d02a44","colab":{"base_uri":"https://localhost:8080/","height":33}},"source":["import sys\n","is_colab = 'google.colab' in sys.modules\n","is_colab, sys.executable"],"execution_count":1,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(True, '/usr/bin/python3')"]},"metadata":{"tags":[]},"execution_count":1}]},{"cell_type":"markdown","metadata":{"id":"g9oJU8UHOaRC"},"source":["Mounting my Drive if on Colab"]},{"cell_type":"code","metadata":{"id":"l5RrOSXvfGrC","executionInfo":{"status":"ok","timestamp":1603368323699,"user_tz":-120,"elapsed":33083,"user":{"displayName":"Maryse Muller","photoUrl":"","userId":"13230828717959946182"}},"outputId":"34096db2-f134-4de8-e47b-d5f729331cf7","colab":{"base_uri":"https://localhost:8080/","height":66}},"source":["if is_colab==True:\n","    from google.colab import files, output, drive\n","    drive.mount('/gdrive')\n","    %cd /gdrive\n","    print(\"You're on Google Colab\")\n","else:\n","    print(\"You're on a PC\")"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Mounted at /gdrive\n","/gdrive\n","You're on Google Colab\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"F7E_-Zd9OgY7"},"source":["Installations and importations required in the virtual environment."]},{"cell_type":"code","metadata":{"id":"VZ__n1yHHrQJ","executionInfo":{"status":"ok","timestamp":1603368323701,"user_tz":-120,"elapsed":33064,"user":{"displayName":"Maryse Muller","photoUrl":"","userId":"13230828717959946182"}}},"source":["# import os\n","# if os.getcwd()!='/gdrive/My Drive/--DATA SCIENCE/PROJET6/NOTEBOOKS':\n","#     os.chdir('/gdrive/My Drive/--DATA SCIENCE/PROJET6/NOTEBOOKS')"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"TCg0w1hgprYQ","outputId":"a082cc87-de44-4f76-c235-b5237277751c","colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["import os\n","if is_colab==True:\n","    if os.getcwd()!='/gdrive/My Drive/--DATA SCIENCE/PROJET6/NOTEBOOKS':\n","        os.chdir('/gdrive/My Drive/--DATA SCIENCE/PROJET6/NOTEBOOKS')\n","else:\n","    if not (os.path.exists(os.getcwd()+'/requirements_nlp.txt') \\\n","                     and os.path.exists(os.getcwd()+'/P6_functions.py')):\n","        print(\"ERROR: Make sure 'P6_functions.py' and \\\n","'requirements_nlp.txt' are in the current working directory\")\n","\n","!pip install -r requirements_nlp.txt"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: absl-py==0.10.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 1)) (0.10.0)\n","Requirement already satisfied: alabaster==0.7.12 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 2)) (0.7.12)\n","Requirement already satisfied: albumentations==0.1.12 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 3)) (0.1.12)\n","Requirement already satisfied: altair==4.1.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 4)) (4.1.0)\n","Requirement already satisfied: argon2-cffi==20.1.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 5)) (20.1.0)\n","Requirement already satisfied: asgiref==3.2.10 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 6)) (3.2.10)\n","Requirement already satisfied: astor==0.8.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 7)) (0.8.1)\n","Requirement already satisfied: astropy==4.0.1.post1 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 8)) (4.0.1.post1)\n","Requirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 9)) (1.6.3)\n","Requirement already satisfied: async-generator==1.10 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 10)) (1.10)\n","Requirement already satisfied: atari-py==0.2.6 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 11)) (0.2.6)\n","Requirement already satisfied: atomicwrites==1.4.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 12)) (1.4.0)\n","Requirement already satisfied: attrs==20.2.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 13)) (20.2.0)\n","Requirement already satisfied: audioread==2.1.8 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 14)) (2.1.8)\n","Requirement already satisfied: autograd==1.3 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 15)) (1.3)\n","Requirement already satisfied: Babel==2.8.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 16)) (2.8.0)\n","Requirement already satisfied: backcall==0.2.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 17)) (0.2.0)\n","Requirement already satisfied: beautifulsoup4==4.6.3 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 18)) (4.6.3)\n","Requirement already satisfied: bleach==3.2.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 19)) (3.2.1)\n","Requirement already satisfied: blis==0.4.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 20)) (0.4.1)\n","Requirement already satisfied: bokeh==2.1.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 21)) (2.1.1)\n","Requirement already satisfied: Bottleneck==1.3.2 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 22)) (1.3.2)\n","Requirement already satisfied: branca==0.4.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 23)) (0.4.1)\n","Requirement already satisfied: bs4==0.0.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 24)) (0.0.1)\n","Requirement already satisfied: CacheControl==0.12.6 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 25)) (0.12.6)\n","Requirement already satisfied: cachetools==4.1.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 26)) (4.1.1)\n","Requirement already satisfied: catalogue==1.0.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 27)) (1.0.0)\n","Requirement already satisfied: certifi==2020.6.20 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 28)) (2020.6.20)\n","Requirement already satisfied: cffi==1.14.3 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 29)) (1.14.3)\n","Requirement already satisfied: chainer==7.4.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 30)) (7.4.0)\n","Requirement already satisfied: chardet==3.0.4 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 31)) (3.0.4)\n","Requirement already satisfied: click==7.1.2 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 32)) (7.1.2)\n","Requirement already satisfied: cloudpickle==1.3.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 33)) (1.3.0)\n","Requirement already satisfied: cmake==3.12.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 34)) (3.12.0)\n","Requirement already satisfied: cmdstanpy==0.9.5 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 35)) (0.9.5)\n","Requirement already satisfied: colorlover==0.3.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 36)) (0.3.0)\n","Requirement already satisfied: community==1.0.0b1 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 37)) (1.0.0b1)\n","Requirement already satisfied: contextlib2==0.5.5 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 38)) (0.5.5)\n","Requirement already satisfied: convertdate==2.2.2 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 39)) (2.2.2)\n","Requirement already satisfied: coverage==3.7.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 40)) (3.7.1)\n","Requirement already satisfied: coveralls==0.5 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 41)) (0.5)\n","Requirement already satisfied: crcmod==1.7 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 42)) (1.7)\n","Requirement already satisfied: cufflinks==0.17.3 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 43)) (0.17.3)\n","Requirement already satisfied: cvxopt==1.2.5 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 44)) (1.2.5)\n","Requirement already satisfied: cvxpy==1.0.31 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 45)) (1.0.31)\n","Requirement already satisfied: cycler==0.10.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 46)) (0.10.0)\n","Requirement already satisfied: cymem==2.0.3 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 47)) (2.0.3)\n","Requirement already satisfied: Cython==0.29.21 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 48)) (0.29.21)\n","Requirement already satisfied: daft==0.0.4 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 49)) (0.0.4)\n","Requirement already satisfied: dask==2.12.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 50)) (2.12.0)\n","Requirement already satisfied: dataclasses==0.7 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 51)) (0.7)\n","Requirement already satisfied: datascience==0.10.6 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 52)) (0.10.6)\n","Requirement already satisfied: debugpy==1.0.0rc2 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 53)) (1.0.0rc2)\n","Requirement already satisfied: decorator==4.4.2 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 54)) (4.4.2)\n","Requirement already satisfied: defusedxml==0.6.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 55)) (0.6.0)\n","Requirement already satisfied: descartes==1.1.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 56)) (1.1.0)\n","Requirement already satisfied: dill==0.3.2 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 57)) (0.3.2)\n","Requirement already satisfied: distributed==1.25.3 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 58)) (1.25.3)\n","Requirement already satisfied: Django==3.1.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 59)) (3.1.1)\n","Requirement already satisfied: dlib==19.18.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 60)) (19.18.0)\n","Requirement already satisfied: dm-tree==0.1.5 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 61)) (0.1.5)\n","Requirement already satisfied: docopt==0.6.2 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 62)) (0.6.2)\n","Requirement already satisfied: docutils==0.16 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 63)) (0.16)\n","Requirement already satisfied: dopamine-rl==1.0.5 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 64)) (1.0.5)\n","Requirement already satisfied: earthengine-api==0.1.236 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 65)) (0.1.236)\n","Requirement already satisfied: easydict==1.9 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 66)) (1.9)\n","Requirement already satisfied: ecos==2.0.7.post1 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 67)) (2.0.7.post1)\n","Requirement already satisfied: editdistance==0.5.3 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 68)) (0.5.3)\n","Requirement already satisfied: en-core-web-sm==2.2.5 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 69)) (2.2.5)\n","Requirement already satisfied: entrypoints==0.3 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 70)) (0.3)\n","Requirement already satisfied: ephem==3.7.7.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 71)) (3.7.7.1)\n","Requirement already satisfied: et-xmlfile==1.0.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 72)) (1.0.1)\n","Requirement already satisfied: fa2==0.3.5 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 73)) (0.3.5)\n","Requirement already satisfied: fancyimpute==0.4.3 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 74)) (0.4.3)\n","Requirement already satisfied: fastai==1.0.61 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 75)) (1.0.61)\n","Requirement already satisfied: fastdtw==0.3.4 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 76)) (0.3.4)\n","Requirement already satisfied: fastprogress==1.0.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 77)) (1.0.0)\n","Requirement already satisfied: fastrlock==0.5 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 78)) (0.5)\n","Requirement already satisfied: fbprophet==0.7.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 79)) (0.7.1)\n","Requirement already satisfied: feather-format==0.4.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 80)) (0.4.1)\n","Requirement already satisfied: filelock==3.0.12 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 81)) (3.0.12)\n","Requirement already satisfied: firebase-admin==4.4.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 82)) (4.4.0)\n","Requirement already satisfied: fix-yahoo-finance==0.0.22 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 83)) (0.0.22)\n","Requirement already satisfied: Flask==1.1.2 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 84)) (1.1.2)\n","Requirement already satisfied: folium==0.8.3 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 85)) (0.8.3)\n","Requirement already satisfied: future==0.16.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 86)) (0.16.0)\n","Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 87)) (0.3.3)\n","Requirement already satisfied: GDAL==2.2.2 in /usr/lib/python3/dist-packages (from -r requirements_nlp.txt (line 88)) (2.2.2)\n","Requirement already satisfied: gdown==3.6.4 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 89)) (3.6.4)\n","Requirement already satisfied: gensim==3.6.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 90)) (3.6.0)\n","Requirement already satisfied: geographiclib==1.50 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 91)) (1.50)\n","Requirement already satisfied: geopy==1.17.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 92)) (1.17.0)\n","Requirement already satisfied: gin-config==0.3.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 93)) (0.3.0)\n","Requirement already satisfied: glob2==0.7 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 94)) (0.7)\n","Requirement already satisfied: google==2.0.3 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 95)) (2.0.3)\n","Requirement already satisfied: google-api-core==1.16.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 96)) (1.16.0)\n","Requirement already satisfied: google-api-python-client==1.7.12 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 97)) (1.7.12)\n","Requirement already satisfied: google-auth==1.17.2 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 98)) (1.17.2)\n","Requirement already satisfied: google-auth-httplib2==0.0.4 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 99)) (0.0.4)\n","Requirement already satisfied: google-auth-oauthlib==0.4.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 100)) (0.4.1)\n","Requirement already satisfied: google-cloud-bigquery==1.21.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 101)) (1.21.0)\n","Requirement already satisfied: google-cloud-core==1.0.3 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 102)) (1.0.3)\n","Requirement already satisfied: google-cloud-datastore==1.8.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 103)) (1.8.0)\n","Requirement already satisfied: google-cloud-firestore==1.7.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 104)) (1.7.0)\n","Requirement already satisfied: google-cloud-language==1.2.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 105)) (1.2.0)\n","Requirement already satisfied: google-cloud-storage==1.18.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 106)) (1.18.1)\n","Requirement already satisfied: google-cloud-translate==1.5.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 107)) (1.5.0)\n","Requirement already satisfied: google-colab==1.0.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 108)) (1.0.0)\n","Requirement already satisfied: google-pasta==0.2.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 109)) (0.2.0)\n","Requirement already satisfied: google-resumable-media==0.4.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 110)) (0.4.1)\n","Requirement already satisfied: googleapis-common-protos==1.52.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 111)) (1.52.0)\n","Requirement already satisfied: googledrivedownloader==0.4 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 112)) (0.4)\n","Requirement already satisfied: graphviz==0.10.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 113)) (0.10.1)\n","Requirement already satisfied: grpcio==1.32.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 114)) (1.32.0)\n","Requirement already satisfied: gspread==3.0.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 115)) (3.0.1)\n","Requirement already satisfied: gspread-dataframe==3.0.8 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 116)) (3.0.8)\n","Collecting gTTS==2.1.1\n","  Downloading https://files.pythonhosted.org/packages/a1/0c/4ca77eca3b739a4a08360930643f58d714e302fee0d2f8c654e67d9af8e7/gTTS-2.1.1-py3-none-any.whl\n","Collecting gTTS-token==1.1.3\n","  Downloading https://files.pythonhosted.org/packages/e7/25/ca6e9cd3275bfc3097fe6b06cc31db6d3dfaf32e032e0f73fead9c9a03ce/gTTS-token-1.1.3.tar.gz\n","Requirement already satisfied: gym==0.17.2 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 119)) (0.17.2)\n","Requirement already satisfied: h5py==2.10.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 120)) (2.10.0)\n","Requirement already satisfied: HeapDict==1.0.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 121)) (1.0.1)\n","Requirement already satisfied: holidays==0.10.3 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 122)) (0.10.3)\n","Requirement already satisfied: holoviews==1.13.4 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 123)) (1.13.4)\n","Requirement already satisfied: html5lib==1.0.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 124)) (1.0.1)\n","Requirement already satisfied: httpimport==0.5.18 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 125)) (0.5.18)\n","Requirement already satisfied: httplib2==0.17.4 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 126)) (0.17.4)\n","Requirement already satisfied: httplib2shim==0.0.3 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 127)) (0.0.3)\n","Requirement already satisfied: humanize==0.5.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 128)) (0.5.1)\n","Requirement already satisfied: hyperopt==0.1.2 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 129)) (0.1.2)\n","Requirement already satisfied: ideep4py==2.0.0.post3 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 130)) (2.0.0.post3)\n","Requirement already satisfied: idna==2.10 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 131)) (2.10)\n","Requirement already satisfied: image==1.5.32 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 132)) (1.5.32)\n","Requirement already satisfied: imageio==2.4.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 133)) (2.4.1)\n","Requirement already satisfied: imagesize==1.2.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 134)) (1.2.0)\n","Requirement already satisfied: imbalanced-learn==0.4.3 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 135)) (0.4.3)\n","Requirement already satisfied: imblearn==0.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 136)) (0.0)\n","Requirement already satisfied: imgaug==0.2.9 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 137)) (0.2.9)\n","Requirement already satisfied: importlib-metadata==2.0.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 138)) (2.0.0)\n","Requirement already satisfied: imutils==0.5.3 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 139)) (0.5.3)\n","Requirement already satisfied: inflect==2.1.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 140)) (2.1.0)\n","Collecting iniconfig==1.0.1\n","  Downloading https://files.pythonhosted.org/packages/20/46/d2f4919cc48c39c2cb48b589ca9016aae6bad050b8023667eb86950d3da2/iniconfig-1.0.1-py3-none-any.whl\n","Requirement already satisfied: intel-openmp==2020.0.133 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 142)) (2020.0.133)\n","Requirement already satisfied: intervaltree==2.1.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 143)) (2.1.0)\n","Requirement already satisfied: ipykernel==4.10.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 144)) (4.10.1)\n","Requirement already satisfied: ipython==5.5.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 145)) (5.5.0)\n","Requirement already satisfied: ipython-genutils==0.2.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 146)) (0.2.0)\n","Requirement already satisfied: ipython-sql==0.3.9 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 147)) (0.3.9)\n","Requirement already satisfied: ipywidgets==7.5.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 148)) (7.5.1)\n","Requirement already satisfied: itsdangerous==1.1.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 149)) (1.1.0)\n","Requirement already satisfied: jax==0.2.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 150)) (0.2.0)\n","Requirement already satisfied: jaxlib==0.1.55 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 151)) (0.1.55)\n","Requirement already satisfied: jdcal==1.4.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 152)) (1.4.1)\n","Requirement already satisfied: jedi==0.17.2 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 153)) (0.17.2)\n","Requirement already satisfied: jieba==0.42.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 154)) (0.42.1)\n","Requirement already satisfied: Jinja2==2.11.2 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 155)) (2.11.2)\n","Requirement already satisfied: joblib==0.16.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 156)) (0.16.0)\n","Requirement already satisfied: jpeg4py==0.1.4 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 157)) (0.1.4)\n","Requirement already satisfied: jsonschema==2.6.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 158)) (2.6.0)\n","Requirement already satisfied: jupyter==1.0.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 159)) (1.0.0)\n","Requirement already satisfied: jupyter-client==5.3.5 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 160)) (5.3.5)\n","Requirement already satisfied: jupyter-console==5.2.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 161)) (5.2.0)\n","Requirement already satisfied: jupyter-core==4.6.3 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 162)) (4.6.3)\n","Requirement already satisfied: jupyterlab-pygments==0.1.2 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 163)) (0.1.2)\n","Requirement already satisfied: kaggle==1.5.8 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 164)) (1.5.8)\n","Requirement already satisfied: kapre==0.1.3.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 165)) (0.1.3.1)\n","Requirement already satisfied: Keras==2.4.3 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 166)) (2.4.3)\n","Requirement already satisfied: Keras-Preprocessing==1.1.2 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 167)) (1.1.2)\n","Requirement already satisfied: keras-vis==0.4.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 168)) (0.4.1)\n","Requirement already satisfied: kiwisolver==1.2.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 169)) (1.2.0)\n","Requirement already satisfied: knnimpute==0.1.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 170)) (0.1.0)\n","Requirement already satisfied: korean-lunar-calendar==0.2.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 171)) (0.2.1)\n","Requirement already satisfied: librosa==0.6.3 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 172)) (0.6.3)\n","Requirement already satisfied: lightgbm==2.2.3 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 173)) (2.2.3)\n","Requirement already satisfied: llvmlite==0.31.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 174)) (0.31.0)\n","Requirement already satisfied: lmdb==0.99 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 175)) (0.99)\n","Requirement already satisfied: lucid==0.3.8 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 176)) (0.3.8)\n","Requirement already satisfied: LunarCalendar==0.0.9 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 177)) (0.0.9)\n","Requirement already satisfied: lxml==4.2.6 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 178)) (4.2.6)\n","Collecting marisa-trie==0.7.5\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/20/95/d23071d0992dabcb61c948fb118a90683193befc88c23e745b050a29e7db/marisa-trie-0.7.5.tar.gz (270kB)\n","\u001b[K     |████████████████████████████████| 276kB 9.4MB/s \n","\u001b[?25hRequirement already satisfied: Markdown==3.2.2 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 180)) (3.2.2)\n","Requirement already satisfied: MarkupSafe==1.1.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 181)) (1.1.1)\n","Requirement already satisfied: matplotlib==3.2.2 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 182)) (3.2.2)\n","Requirement already satisfied: matplotlib-venn==0.11.5 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 183)) (0.11.5)\n","Requirement already satisfied: missingno==0.4.2 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 184)) (0.4.2)\n","Requirement already satisfied: mistune==0.8.4 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 185)) (0.8.4)\n","Requirement already satisfied: mizani==0.6.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 186)) (0.6.0)\n","Requirement already satisfied: mkl==2019.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 187)) (2019.0)\n","Requirement already satisfied: mlxtend==0.14.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 188)) (0.14.0)\n","Requirement already satisfied: more-itertools==8.5.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 189)) (8.5.0)\n","Requirement already satisfied: moviepy==0.2.3.5 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 190)) (0.2.3.5)\n","Requirement already satisfied: mpmath==1.1.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 191)) (1.1.0)\n","Requirement already satisfied: msgpack==1.0.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 192)) (1.0.0)\n","Requirement already satisfied: multiprocess==0.70.10 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 193)) (0.70.10)\n","Requirement already satisfied: multitasking==0.0.9 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 194)) (0.0.9)\n","Requirement already satisfied: murmurhash==1.0.2 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 195)) (1.0.2)\n","Requirement already satisfied: music21==5.5.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 196)) (5.5.0)\n","Collecting mwparserfromhell==0.5.4\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/23/03/4fb04da533c7e237c0104151c028d8bff856293d34e51d208c529696fb79/mwparserfromhell-0.5.4.tar.gz (135kB)\n","\u001b[K     |████████████████████████████████| 143kB 15.8MB/s \n","\u001b[?25hRequirement already satisfied: natsort==5.5.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 198)) (5.5.0)\n","Requirement already satisfied: nbclient==0.5.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 199)) (0.5.0)\n","Requirement already satisfied: nbconvert==5.6.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 200)) (5.6.1)\n","Requirement already satisfied: nbformat==5.0.7 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 201)) (5.0.7)\n","Requirement already satisfied: nest-asyncio==1.4.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 202)) (1.4.1)\n","Requirement already satisfied: networkx==2.5 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 203)) (2.5)\n","Requirement already satisfied: nibabel==3.0.2 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 204)) (3.0.2)\n","Requirement already satisfied: nltk==3.2.5 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 205)) (3.2.5)\n","Requirement already satisfied: notebook==5.3.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 206)) (5.3.1)\n","Requirement already satisfied: np-utils==0.5.12.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 207)) (0.5.12.1)\n","Requirement already satisfied: numba==0.48.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 208)) (0.48.0)\n","Requirement already satisfied: numexpr==2.7.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 209)) (2.7.1)\n","Requirement already satisfied: numpy==1.18.5 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 210)) (1.18.5)\n","Requirement already satisfied: nvidia-ml-py3==7.352.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 211)) (7.352.0)\n","Requirement already satisfied: oauth2client==4.1.3 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 212)) (4.1.3)\n","Requirement already satisfied: oauthlib==3.1.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 213)) (3.1.0)\n","Requirement already satisfied: okgrade==0.4.3 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 214)) (0.4.3)\n","Requirement already satisfied: opencv-contrib-python==4.1.2.30 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 215)) (4.1.2.30)\n","Requirement already satisfied: opencv-python==4.1.2.30 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 216)) (4.1.2.30)\n","Requirement already satisfied: openpyxl==2.5.9 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 217)) (2.5.9)\n","Requirement already satisfied: opt-einsum==3.3.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 218)) (3.3.0)\n","Requirement already satisfied: osqp==0.6.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 219)) (0.6.1)\n","Requirement already satisfied: packaging==20.4 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 220)) (20.4)\n","Requirement already satisfied: palettable==3.3.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 221)) (3.3.0)\n","Requirement already satisfied: pandas==1.1.2 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 222)) (1.1.2)\n","Requirement already satisfied: pandas-datareader==0.9.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 223)) (0.9.0)\n","Requirement already satisfied: pandas-gbq==0.13.2 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 224)) (0.13.2)\n","Requirement already satisfied: pandas-profiling==1.4.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 225)) (1.4.1)\n","Requirement already satisfied: pandocfilters==1.4.2 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 226)) (1.4.2)\n","Requirement already satisfied: panel==0.9.7 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 227)) (0.9.7)\n","Requirement already satisfied: param==1.9.3 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 228)) (1.9.3)\n","Requirement already satisfied: parso==0.7.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 229)) (0.7.1)\n","Requirement already satisfied: pathlib==1.0.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 230)) (1.0.1)\n","Requirement already satisfied: patsy==0.5.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 231)) (0.5.1)\n","Requirement already satisfied: pexpect==4.8.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 232)) (4.8.0)\n","Requirement already satisfied: pickleshare==0.7.5 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 233)) (0.7.5)\n","Requirement already satisfied: Pillow==7.0.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 234)) (7.0.0)\n","Requirement already satisfied: pip-tools==4.5.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 235)) (4.5.1)\n","Requirement already satisfied: plac==1.1.3 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 236)) (1.1.3)\n","Requirement already satisfied: plotly==4.4.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 237)) (4.4.1)\n","Requirement already satisfied: plotnine==0.6.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 238)) (0.6.0)\n","Requirement already satisfied: pluggy==0.7.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 239)) (0.7.1)\n","Requirement already satisfied: portpicker==1.3.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 240)) (1.3.1)\n","Requirement already satisfied: prefetch-generator==1.0.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 241)) (1.0.1)\n","Requirement already satisfied: preshed==3.0.2 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 242)) (3.0.2)\n","Requirement already satisfied: prettytable==0.7.2 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 243)) (0.7.2)\n","Requirement already satisfied: progressbar2==3.38.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 244)) (3.38.0)\n","Requirement already satisfied: prometheus-client==0.8.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 245)) (0.8.0)\n","Requirement already satisfied: promise==2.3 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 246)) (2.3)\n","Requirement already satisfied: prompt-toolkit==1.0.18 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 247)) (1.0.18)\n","Requirement already satisfied: protobuf==3.12.4 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 248)) (3.12.4)\n","Requirement already satisfied: psutil==5.4.8 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 249)) (5.4.8)\n","Requirement already satisfied: psycopg2==2.7.6.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 250)) (2.7.6.1)\n","Requirement already satisfied: ptyprocess==0.6.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 251)) (0.6.0)\n","Requirement already satisfied: py==1.9.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 252)) (1.9.0)\n","Requirement already satisfied: pyarrow==0.14.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 253)) (0.14.1)\n","Requirement already satisfied: pyasn1==0.4.8 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 254)) (0.4.8)\n","Requirement already satisfied: pyasn1-modules==0.2.8 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 255)) (0.2.8)\n","Requirement already satisfied: pycocotools==2.0.2 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 256)) (2.0.2)\n","Requirement already satisfied: pycparser==2.20 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 257)) (2.20)\n","Requirement already satisfied: pyct==0.4.8 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 258)) (0.4.8)\n","Requirement already satisfied: pydata-google-auth==1.1.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 259)) (1.1.0)\n","Requirement already satisfied: pydot==1.3.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 260)) (1.3.0)\n","Requirement already satisfied: pydot-ng==2.0.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 261)) (2.0.0)\n","Requirement already satisfied: pydotplus==2.0.2 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 262)) (2.0.2)\n","Requirement already satisfied: PyDrive==1.3.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 263)) (1.3.1)\n","Requirement already satisfied: pyemd==0.5.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 264)) (0.5.1)\n","Requirement already satisfied: pyglet==1.5.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 265)) (1.5.0)\n","Requirement already satisfied: Pygments==2.6.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 266)) (2.6.1)\n","Requirement already satisfied: pygobject==3.26.1 in /usr/lib/python3/dist-packages (from -r requirements_nlp.txt (line 267)) (3.26.1)\n","Requirement already satisfied: pymc3==3.7 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 268)) (3.7)\n","Requirement already satisfied: PyMeeus==0.3.7 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 269)) (0.3.7)\n","Requirement already satisfied: pymongo==3.11.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 270)) (3.11.0)\n","Requirement already satisfied: pymystem3==0.2.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 271)) (0.2.0)\n","Requirement already satisfied: PyOpenGL==3.1.5 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 272)) (3.1.5)\n","Requirement already satisfied: pyparsing==2.4.7 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 273)) (2.4.7)\n","Requirement already satisfied: pyrsistent==0.17.3 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 274)) (0.17.3)\n","Requirement already satisfied: pysndfile==1.3.8 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 275)) (1.3.8)\n","Requirement already satisfied: PySocks==1.7.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 276)) (1.7.1)\n","Requirement already satisfied: pystan==2.19.1.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 277)) (2.19.1.1)\n","Requirement already satisfied: pytest==3.6.4 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 278)) (3.6.4)\n","Requirement already satisfied: python-apt==1.6.5+ubuntu0.3 in /usr/lib/python3/dist-packages (from -r requirements_nlp.txt (line 279)) (1.6.5+ubuntu0.3)\n","Requirement already satisfied: python-chess==0.23.11 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 280)) (0.23.11)\n","Requirement already satisfied: python-dateutil==2.8.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 281)) (2.8.1)\n","Requirement already satisfied: python-louvain==0.14 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 282)) (0.14)\n","Requirement already satisfied: python-slugify==4.0.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 283)) (4.0.1)\n","Requirement already satisfied: python-utils==2.4.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 284)) (2.4.0)\n","Requirement already satisfied: pytz==2018.9 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 285)) (2018.9)\n","Requirement already satisfied: pyviz-comms==0.7.6 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 286)) (0.7.6)\n","Requirement already satisfied: PyWavelets==1.1.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 287)) (1.1.1)\n","Requirement already satisfied: PyYAML==3.13 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 288)) (3.13)\n","Requirement already satisfied: pyzmq==19.0.2 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 289)) (19.0.2)\n","Requirement already satisfied: qtconsole==4.7.7 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 290)) (4.7.7)\n","Requirement already satisfied: QtPy==1.9.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 291)) (1.9.0)\n","Requirement already satisfied: regex==2019.12.20 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 292)) (2019.12.20)\n","Requirement already satisfied: requests==2.23.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 293)) (2.23.0)\n","Requirement already satisfied: requests-oauthlib==1.3.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 294)) (1.3.0)\n","Requirement already satisfied: resampy==0.2.2 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 295)) (0.2.2)\n","Requirement already satisfied: retrying==1.3.3 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 296)) (1.3.3)\n","Requirement already satisfied: rpy2==3.2.7 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 297)) (3.2.7)\n","Requirement already satisfied: rsa==4.6 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 298)) (4.6)\n","Requirement already satisfied: scikit-image==0.16.2 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 299)) (0.16.2)\n","Requirement already satisfied: scikit-learn==0.22.2.post1 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 300)) (0.22.2.post1)\n","Requirement already satisfied: scipy==1.4.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 301)) (1.4.1)\n","Requirement already satisfied: screen-resolution-extra==0.0.0 in /usr/lib/python3/dist-packages (from -r requirements_nlp.txt (line 302)) (0.0.0)\n","Requirement already satisfied: scs==2.1.2 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 303)) (2.1.2)\n","Requirement already satisfied: seaborn==0.11.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 304)) (0.11.0)\n","Requirement already satisfied: Send2Trash==1.5.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 305)) (1.5.0)\n","Requirement already satisfied: setuptools-git==1.2 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 306)) (1.2)\n","Requirement already satisfied: Shapely==1.7.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 307)) (1.7.1)\n","Requirement already satisfied: simplegeneric==0.8.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 308)) (0.8.1)\n","Requirement already satisfied: six==1.15.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 309)) (1.15.0)\n","Requirement already satisfied: sklearn==0.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 310)) (0.0)\n","Requirement already satisfied: sklearn-pandas==1.8.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 311)) (1.8.0)\n","Requirement already satisfied: slugify==0.0.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 312)) (0.0.1)\n","Requirement already satisfied: smart-open==2.2.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 313)) (2.2.0)\n","Requirement already satisfied: snowballstemmer==2.0.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 314)) (2.0.0)\n","Requirement already satisfied: sortedcontainers==2.2.2 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 315)) (2.2.2)\n","Requirement already satisfied: spacy==2.2.4 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 316)) (2.2.4)\n","Requirement already satisfied: Sphinx==1.8.5 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 317)) (1.8.5)\n","Requirement already satisfied: sphinxcontrib-serializinghtml==1.1.4 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 318)) (1.1.4)\n","Requirement already satisfied: sphinxcontrib-websupport==1.2.4 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 319)) (1.2.4)\n","Requirement already satisfied: SQLAlchemy==1.3.19 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 320)) (1.3.19)\n","Requirement already satisfied: sqlparse==0.3.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 321)) (0.3.1)\n","Requirement already satisfied: srsly==1.0.2 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 322)) (1.0.2)\n","Requirement already satisfied: statsmodels==0.10.2 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 323)) (0.10.2)\n","Requirement already satisfied: sympy==1.1.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 324)) (1.1.1)\n","Requirement already satisfied: tables==3.4.4 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 325)) (3.4.4)\n","Requirement already satisfied: tabulate==0.8.7 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 326)) (0.8.7)\n","Requirement already satisfied: tblib==1.7.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 327)) (1.7.0)\n","Requirement already satisfied: tensorboard==2.3.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 328)) (2.3.0)\n","Requirement already satisfied: tensorboard-plugin-wit==1.7.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 329)) (1.7.0)\n","Requirement already satisfied: tensorboardcolab==0.0.22 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 330)) (0.0.22)\n","Requirement already satisfied: tensorflow==2.3.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 331)) (2.3.0)\n","Requirement already satisfied: tensorflow-addons==0.8.3 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 332)) (0.8.3)\n","Requirement already satisfied: tensorflow-datasets==2.1.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 333)) (2.1.0)\n","Requirement already satisfied: tensorflow-estimator==2.3.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 334)) (2.3.0)\n","Requirement already satisfied: tensorflow-gcs-config==2.3.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 335)) (2.3.0)\n","Requirement already satisfied: tensorflow-hub==0.9.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 336)) (0.9.0)\n","Requirement already satisfied: tensorflow-metadata==0.24.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 337)) (0.24.0)\n","Requirement already satisfied: tensorflow-privacy==0.2.2 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 338)) (0.2.2)\n","Requirement already satisfied: tensorflow-probability==0.11.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 339)) (0.11.0)\n","Requirement already satisfied: termcolor==1.1.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 340)) (1.1.0)\n","Requirement already satisfied: terminado==0.9.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 341)) (0.9.1)\n","Requirement already satisfied: testpath==0.4.4 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 342)) (0.4.4)\n","Requirement already satisfied: text-unidecode==1.3 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 343)) (1.3)\n","Requirement already satisfied: textblob==0.15.3 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 344)) (0.15.3)\n","Requirement already satisfied: textgenrnn==1.4.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 345)) (1.4.1)\n","Requirement already satisfied: Theano==1.0.5 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 346)) (1.0.5)\n","Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 347)) (7.4.0)\n","Requirement already satisfied: tifffile==2020.9.3 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 348)) (2020.9.3)\n","Requirement already satisfied: toml==0.10.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 349)) (0.10.1)\n","Requirement already satisfied: toolz==0.11.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 350)) (0.11.1)\n","Requirement already satisfied: torch==1.6.0+cu101 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 351)) (1.6.0+cu101)\n","Requirement already satisfied: torchsummary==1.5.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 352)) (1.5.1)\n","Requirement already satisfied: torchtext==0.3.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 353)) (0.3.1)\n","Requirement already satisfied: torchvision==0.7.0+cu101 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 354)) (0.7.0+cu101)\n","Requirement already satisfied: tornado==5.1.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 355)) (5.1.1)\n","Requirement already satisfied: tqdm==4.41.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 356)) (4.41.1)\n","Requirement already satisfied: traitlets==4.3.3 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 357)) (4.3.3)\n","Requirement already satisfied: tweepy==3.6.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 358)) (3.6.0)\n","Requirement already satisfied: typeguard==2.7.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 359)) (2.7.1)\n","Requirement already satisfied: typing-extensions==3.7.4.3 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 360)) (3.7.4.3)\n","Requirement already satisfied: tzlocal==1.5.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 361)) (1.5.1)\n","Requirement already satisfied: umap-learn==0.4.6 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 362)) (0.4.6)\n","Requirement already satisfied: uritemplate==3.0.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 363)) (3.0.1)\n","Requirement already satisfied: urllib3==1.24.3 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 364)) (1.24.3)\n","Requirement already satisfied: vega-datasets==0.8.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 365)) (0.8.0)\n","Requirement already satisfied: wasabi==0.8.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 366)) (0.8.0)\n","Requirement already satisfied: wcwidth==0.2.5 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 367)) (0.2.5)\n","Requirement already satisfied: webencodings==0.5.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 368)) (0.5.1)\n","Requirement already satisfied: Werkzeug==1.0.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 369)) (1.0.1)\n","Requirement already satisfied: widgetsnbextension==3.5.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 370)) (3.5.1)\n","Collecting wikipedia2vec==0.2.2\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/93/83/0442721cf062741e87c512554e6cd2e40ccd7b0d85278657c857281357e4/wikipedia2vec-0.2.2.tar.gz (1.1MB)\n","\u001b[K     |████████████████████████████████| 1.1MB 17.1MB/s \n","\u001b[?25hRequirement already satisfied: wordcloud==1.5.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 372)) (1.5.0)\n","Requirement already satisfied: wrapt==1.12.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 373)) (1.12.1)\n","Requirement already satisfied: xarray==0.15.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 374)) (0.15.1)\n","Requirement already satisfied: xgboost==0.90 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 375)) (0.90)\n","Requirement already satisfied: xkit==0.0.0 in /usr/lib/python3/dist-packages (from -r requirements_nlp.txt (line 376)) (0.0.0)\n","Requirement already satisfied: xlrd==1.1.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 377)) (1.1.0)\n","Requirement already satisfied: xlwt==1.3.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 378)) (1.3.0)\n","Requirement already satisfied: yellowbrick==0.9.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 379)) (0.9.1)\n","Requirement already satisfied: zict==2.0.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 380)) (2.0.0)\n","Requirement already satisfied: zipp==3.2.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 381)) (3.2.0)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.6/dist-packages (from astunparse==1.6.3->-r requirements_nlp.txt (line 9)) (0.35.1)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from chainer==7.4.0->-r requirements_nlp.txt (line 30)) (50.3.0)\n","Building wheels for collected packages: gTTS-token, marisa-trie, mwparserfromhell, wikipedia2vec\n","  Building wheel for gTTS-token (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for gTTS-token: filename=gTTS_token-1.1.3-cp36-none-any.whl size=4096 sha256=ecd0f026c75dc8aa4b78544fa9d5c26a81b47dfe71add9742c2b0b2e1226d1e4\n","  Stored in directory: /root/.cache/pip/wheels/dd/11/61/33f7e51bf545e910552b2255eead2a7cd8ef54064b46dceb34\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"x1xzfQL5OvRk"},"source":["from P6_functions import *"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gvdHBoIuH90C"},"source":["Installations (creating the requirements file)"]},{"cell_type":"code","metadata":{"id":"H1oa9ebJHSOm"},"source":["# !pip install gtts\n","# !pip install wikipedia2vec==0.2.2\n","# !pip install category-encoders"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gQ-WJqLWFtzu"},"source":["# !pip freeze > requirements_cleaning_nlp.txt"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xBgiuFEdQq78"},"source":["Importation of modules and packages. "]},{"cell_type":"code","metadata":{"id":"oDhE9utOlwJe"},"source":["import io\n","\n","import string\n","\n","import pandas as pd\n","import seaborn as sns\n","import numpy as np\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","plt.rcParams['figure.facecolor']='w'\n","\n","import nltk\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","\n","# import warnings\n","# warnings.filterwarnings('ignore')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xNSTytWrQ0De"},"source":["Setting pandas display options."]},{"cell_type":"code","metadata":{"id":"G0rRvyJaWO2h"},"source":["dictPdSettings = {'display.max_rows': 500, 'display.width': 100,\n","                  'display.max_colwidth': 100,\n","                  'display.float_format': lambda x: '%.2f' % x}\n","for k,v in dictPdSettings.items():\n","  pd.set_option(k,v)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"z9APZjLzQ_sp"},"source":["To play audio text-to-speech during execution."]},{"cell_type":"code","metadata":{"id":"vgvmjvZ_Y6-s"},"source":["from IPython.display import Audio\n","from gtts import gTTS\n","\n","def speak(text, lang='en'):\n","    with io.BytesIO() as f:\n","        gTTS(text=text, lang=lang).write_to_fp(f)\n","        f.seek(0)\n","        return Audio(f.read(), autoplay=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"X5b_ibTl83bD"},"source":["speak('Packages and modules successfully imported')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fmDu0rMFGl5A"},"source":["### 0.1 Importing the datasets"]},{"cell_type":"markdown","metadata":{"id":"R_c4YLdXQAzn"},"source":["Data is composed of 9 distinct .csv files we'll load in a dictionnary of dataframes."]},{"cell_type":"code","metadata":{"id":"KhVW-wxr30Ia"},"source":["if is_colab==True:\n","    # Importing database from my Drive\n","    print(\"Try to import data files in the notebook from myDrive...\")\n","else:\n","    # Importing database from PC\n","    print(\"Try to import data files in the notebook from PC ('DATA')...\")\n","\n","df = pd.read_csv(\"../DATA/flipkart_com-ecommerce_sample_1050.csv\",\n","                 sep=',', \n","                 index_col = 'uniq_id',\n","                 encoding ='utf-8')\n","\n","print(\"-----> Importation of .csv in the notebook: OK\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"61owD2pedhh5"},"source":["speak('Datasets successfully imported')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bGmveNrz8Eh_"},"source":["## 1 Data extraction"]},{"cell_type":"markdown","metadata":{"id":"hBfDUUQyzD_v"},"source":["### 1.1 Categories"]},{"cell_type":"markdown","metadata":{"id":"FM9aXq1jUY-k"},"source":["Unfolding categories using the 'product_category_tree' colum"]},{"cell_type":"code","metadata":{"id":"6c7InsBZU6UM"},"source":["# sample checking\n","df['product_category_tree'][743]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MIoFWkVgfRw3"},"source":["# determining the maximum tree depth of categories\n","ser_depth = df['product_category_tree'].apply(lambda x: x.count('>>'))\n","max_depth = ser_depth.max()\n","max_depth"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"J7gmladEPgJq"},"source":["# Converting the strings in 'product_category_tree' column in 6 categ columns\n","\n","def str_cleaning(ind, my_str, name_level_cols):\n","    my_str = my_str.replace(\"[\\\"\", \"\").replace(\"\\\"]\", \"\")\n","    tab_str = my_str.split(\">>\")\n","    size_tab_str = len(tab_str)\n","    tup_str = tuple([tab_str[i].strip() if i<size_tab_str else \"\" \\\n","                     for i in np.arange(max_depth) ])\n","    return tup_str\n","\n","name_level_cols = ['cat_level_'+str(i) for i in np.arange(max_depth)]\n","ser_tuple = df['product_category_tree']\\\n","    .apply(lambda s: str_cleaning(s.index, s, name_level_cols))\n","df_cat_level = pd.DataFrame([[a,'/'.join([a,b]),'/'.join([a,b,c]),\n","                              '/'.join([a,b,c,d]),'/'.join([a,b,c,d,e]),\n","                              '/'.join([a,b,c,d,e,f])] \\\n","                             for a,b,c,d,e,f in ser_tuple.values],\n","                            columns=name_level_cols, index=df.index)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jNUQB7sBlpvt"},"source":["# printing number of categories in each level and a sample\n","display(df_cat_level.nunique(), df_cat_level.sample(3))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ehwO2aIQ0ETe"},"source":["Let's see how much items are in each category"]},{"cell_type":"code","metadata":{"id":"-IBKwHniz9L5"},"source":["fig = plt.figure(figsize=(25,4))\n","for i, col in enumerate(df_cat_level.columns,1):\n","    ax = fig.add_subplot(1,len(df_cat_level.columns), i)\n","    ser = df_cat_level.groupby(col).size().sort_values(ascending=False)\n","    ser[0:20].plot.bar(width=0.75, color='grey', ec='k', ax=ax)\n","    ax.set_title(col+f' ({ser.shape[0]} categories)', fontweight='bold')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"p1HfAME-4QvR"},"source":["The only level that has a balanced set of items is level 0, with 7 categories.\n","Let's rename these 7 categories:"]},{"cell_type":"code","metadata":{"id":"PShh6D7X5TMH"},"source":["df_cat_level['cat_level_0'].unique()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NSwQ4zUa5CQ4"},"source":["df_cat_level['category'] = \\\n","    df_cat_level['cat_level_0'].replace({'Home Furnishing': 'Furnishing',\n","                                        'Baby Care': 'Baby', \n","                                        'Watches': 'Watches',\n","                                        'Home Decor & Festive Needs': 'Decor',\n","                                        'Kitchen & Dining': 'Kitchen',\n","                                        'Beauty and Personal Care': 'Beauty',\n","                                        'Computers': 'Computers'})"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QBcXz5nNz9xp"},"source":["### 1.2 Products descriptions"]},{"cell_type":"code","metadata":{"id":"5oQmXYsvPgC8"},"source":["# extracting only useful data\n","df_desc_cat = pd.concat([df_cat_level['category'],\n","                         df[[\"product_name\", \"description\"]]], axis=1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fhfvTAuR3-4d"},"source":["df_desc_cat.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tadkUfoS7ddV"},"source":["# creation of a corpus of all the descriptions\n","corpus = ' '.join(df_desc_cat['description'].values)\n","print(\"total nb of words in the whole corpus: \", len(corpus.split()))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7fSxU4NRINAy"},"source":["df_desc_cat"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1JiTjU-IIdQx"},"source":["## 2 Optimisation of the text preprocessing and clustering"]},{"cell_type":"code","metadata":{"id":"TH8P4LEAWP-0"},"source":["df_res_clust = pd.DataFrame()\n","df_res_clust['categories'] = df_desc_cat['category']"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8aQxshTj0M1h"},"source":["### FUNCTIONS"]},{"cell_type":"markdown","metadata":{"id":"582OuQHV-mf6"},"source":["#### tokenize_clean"]},{"cell_type":"code","metadata":{"id":"3Fpz-d444Wyb"},"source":["''' from a sentence, containing words (document):\n","- tokenizes the words if only composed of alphanumerical data,\n","- removes stopwords if list is given (stopwords)\n","- stems the words if stemmer given\n","NB: This pre-processing function can be used to prepare data for Word2Vec\n","'''\n","from nltk.stem.snowball import EnglishStemmer\n","import spacy\n","\n","def tokenize_clean(document, stopwords=None, lemmatizer=None, stemmer=None):\n","    # 1 - tokenizing the words in each description\n","    tokenizer = nltk.RegexpTokenizer(r'[a-z]+')\n","    li_words = tokenizer.tokenize(document)\n","    if stopwords is None: stopwords=[]\n","    # 2 - lemmatizing or stemming\n","    if lemmatizer is not None:\n","        lem_doc = lemmatizer(' '.join(li_words))\n","        li_words = [token.lemma_ for token in lem_doc]\n","    elif stemmer is not None:\n","        li_words = [stemmer.stem(s) for s in li_words]\n","    # 3 - removing stopwords\n","    li_words = [s for s in li_words if s not in stopwords]\n","    # 4 - lower case\n","    li_words = [s.lower() for s in li_words]\n","    return li_words"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"F-bqJZtz-psh"},"source":["#### compute_doc_terms_df"]},{"cell_type":"code","metadata":{"id":"rJ-DoGpRLtV9"},"source":["''' Takes a pd.Series containing the texts of each description\n","applies a preprocessing function if given (stopwords, stemming...)\n","then turn the descriptions in vectors (bow of tf-idf, depending on the avlue of\n"," tfidf_on)\n"," returns document term matrix as a dataframe and the list of new excluded words.\n","'''\n","\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","\n","def compute_doc_terms_df(ser_desc, \n","                         preproc_func=None,\n","                         preproc_func_params=None,\n","                         vec_params = {'min_df': 1},\n","                         tfidf_on=False,\n","                         print_opt=False):\n","\n","    # ---- Apply a stemming of lemmatization prior to vectorization\n","    if preproc_func is not None:\n","        ser_desc = ser_desc.apply(lambda x: preproc_func(x,\n","                                                         **preproc_func_params))\n","        ser_desc = ser_desc.apply(lambda x: ' '.join(x))\n","    else:\n","        ser_desc = ser_desc\n","    \n","    # ---- Vectorization of each of the texts (row)\n","    if tfidf_on:\n","        # TF-IDF matrix\n","        vec = TfidfVectorizer(**vec_params)\n","    else:\n","        # BOW matrix (count)\n","        vec = CountVectorizer(**vec_params)\n","\n","    doc_term = vec.fit_transform(ser_desc)\n","    if print_opt:\n","        print( \"Created %d X %d doc_term matrix\" % (doc_term.shape[0],\n","                                                    doc_term.shape[1]))\n","\n","    # ---- Vocabulary of the document_term matrix\n","    doc_term_voc = vec.get_feature_names()\n","    if print_opt:\n","        print(\"Vocabulary has %d distinct terms\" % len(doc_term_voc))\n","\n","    # ---- Get the list of the new stop-words\n","    new_sw = vec.stop_words_\n","    if print_opt:\n","        print(\"Old stop-words list has %d entries\" % len(sw) )\n","        print(\"New stop-words list has %d entries\" % len(new_sw))\n","\n","    doc_term_df = pd.DataFrame(doc_term.todense(),\n","                index=ser_desc.index, # each item\n","                columns=doc_term_voc) # each word\n","\n","    # document term matrix as a dataframe and the list of new excluded words\n","    return doc_term_df, new_sw\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NPB746p6-s49"},"source":["#### CustNLPTransformer"]},{"cell_type":"code","metadata":{"id":"VexwgKSz-MNx"},"source":["\n","''' Builds a customizable column_transformer which parameters can be optimized in a GridSearchCV\n","CATEGORICAL : three differents startegies for 3 different types of\n","categorical variables:\n","- low cardinality: customizable strategy (strat_low_card)\n","- high cardinality: customizable strategy (strat_high_card)\n","- boolean or equivalent (2 categories): ordinal\n","QUANTITATIVE (remainder): \n","- StandardScaler\n","\n","-> EXAMPLE (to use apart from gscv):\n","cust_enc = CustTransformer(thresh_card=12,\n","                       strat_binary = 'ord',\n","                       strat_low_card = 'ohe',\n","                       strat_high_card = 'loo',\n","                       strat_quant = 'stand')\n","cust_enc.fit(X_tr, y1_tr)\n","cust_enc.transform(X_tr).shape, X_tr.shape\n","\n","-> EXAMPLE (to fetch names of the modified dataframe):\n","small_df = df[['Outlier', 'Neighborhood', 'CertifiedPreviousYear',\n","               'NumberofFloors','ExtsurfVolRatio']]\n","# small_df.head(2)\n","cust_trans = CustTransformer()\n","cust_trans.fit(small_df)\n","df_enc = cust_trans.transform(small_df)\n","cust_trans.get_feature_names(small_df)\n","\n","'''\n","from sklearn.base import BaseEstimator\n","from sklearn.compose import ColumnTransformer\n","from sklearn.pipeline import Pipeline\n","from sklearn.preprocessing import *\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","import numpy as np\n","import pandas as pd\n","\n","\n","class CustNLPTransformer(BaseEstimator):\n","\n","    def __init__(self, stopwords=None, stemmer=None, lemmatizer=None,\n","                 min_df=0, max_df=10000, max_features=10000, tfidf_on=False):\n","        self.stopwords = stopwords\n","        self.lemmatizer = lemmatizer\n","        self.stemmer = stemmer\n","        self.min_df = min_df\n","        self.max_df = max_df\n","        self.max_features = max_features\n","        self.tfidf_on = tfidf_on\n","        self.preproc_func_params={'stopwords': self.stopwords,\n","                                  'lemmatizer': self.lemmatizer,\n","                                  'stemmer': self.stemmer}\n","        self.vec_params = {'min_df': self.min_df,\n","                           'max_df': self.max_df,\n","                           'max_features': self.max_features}\n","\n","    def __tokenize_clean(self, document, stopwords, lemmatizer, stemmer):\n","        # 1 - tokenizing the words in each description\n","        tokenizer = nltk.RegexpTokenizer(r'[a-z]+')\n","        li_words = tokenizer.tokenize(document)\n","        if stopwords is None: stopwords=[]\n","        # 2 - lemmatizing or stemming\n","        if lemmatizer is not None:\n","            lem_doc = lemmatizer(' '.join(li_words))\n","            li_words = [token.lemma_ for token in lem_doc]\n","        elif stemmer is not None:\n","            li_words = [stemmer.stem(s) for s in li_words]\n","        # 3 - removing stopwords\n","        li_words = [s for s in li_words if s not in stopwords]\n","        # 4 - lower case\n","        li_words = [s.lower() for s in li_words]\n","        return li_words\n","\n","    # \"private\" method to be used to apply transformation and get a df\n","    def __compute_doc_terms_df(self, ser_desc, preproc_func,\n","                             preproc_func_params,\n","                            vec_params, tfidf_on):\n","        # ---- Apply a stemming or lemmatization prior to vectorization\n","        if preproc_func is not None:\n","            ser_desc = ser_desc.apply(lambda x: \\\n","                                      preproc_func(x, **preproc_func_params))\n","            ser_desc = ser_desc.apply(lambda x: ' '.join(x))\n","        else:\n","            ser_desc = ser_desc\n","        # ---- Vectorization of each of the texts (row)\n","        if tfidf_on:\n","            # TF-IDF matrix\n","            vec = TfidfVectorizer(**vec_params)\n","        else:\n","            # BOW matrix (count)\n","            vec = CountVectorizer(**vec_params)\n","        doc_term = vec.fit_transform(ser_desc)\n","        # ---- Vocabulary of the document_term matrix\n","        doc_term_voc = vec.get_feature_names()\n","        # ---- Get the list of the new stop-words\n","        new_sw = vec.stop_words_\n","        doc_term_df = pd.DataFrame(doc_term.todense(),\n","                                   index=ser_desc.index, # each item\n","                                   columns=doc_term_voc) # each word\n","        # document term matrix as a dataframe and the list of new excluded words\n","        return doc_term_df\n","\n","    def fit(self, X, y=None):\n","        # nothing to fit\n","        return self\n","\n","    def transform(self, X, y=None):  # to get a dataframe\n","        df_trans = \\\n","            self.__compute_doc_terms_df(\n","                ser_desc=X,\n","                preproc_func=self.__tokenize_clean,\n","                preproc_func_params=self.preproc_func_params,\n","                vec_params=self.vec_params,\n","                tfidf_on=self.tfidf_on\n","                                   )\n","        return df_trans\n","\n","    def fit_transform(self, X, y=None): # self, \n","        return self.transform(X, y)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pwVgwbtC-ybs"},"source":["#### GridSearchClust"]},{"cell_type":"code","metadata":{"id":"6tqC1ZvDZidO"},"source":["'''\n","Class to optimize clustering score.\n","Instantiate with a clusterer (estimator), a grid parameter (param_grid)\n","and a scoring function or a dictionary of functions (scoring)\n","'''\n","\n","import numpy as np\n","from sklearn.base import BaseEstimator, TransformerMixin\n","from sklearn.model_selection import ParameterGrid\n","from collections import defaultdict\n","\n","class GridSearchClust(BaseEstimator, TransformerMixin):\n","\n","    def __init__(self, estimator, param_grid_estim, param_grid_preproc=None,\n","                 scoring=None, refit=silhouette_score, greater_is_better=True):\n","\n","        # Get the parameters\n","        self.estimator = estimator\n","        self.param_grid_estim = param_grid_estim\n","        self.param_grid_preproc = param_grid_preproc\n","        self.scoring = scoring\n","        self.refit = refit\n","        self.greater_is_better = greater_is_better\n","\n","    def fit(self, X, verbose=False):\n","\n","        # Initialize the dict of results\n","        self.results_ = {\"scores\": {},\n","                         \"params\": [],\n","                         \"estimators\": [],\n","                        #  \"fit_times\": [],\n","                        #  \"nb_clusters\": [],\n","                         \"refit_score\": []}\n","\n","        # Iterate upon all combinations of parameters\n","        estim_score = defaultdict(list)\n","        for param in ParameterGrid(self.param_grid_estim):\n","\n","            # Instanciate of the whole estimator with selected parameters\n","            self.estimator = self.estimator.set_params(**param)\n","\n","            # Fit the model\n","            self.estimator.fit(X)\n","\n","            # If the estimator is a pipe, compute the first steps separately\n","            if hasattr(self.estimator, 'steps'): # if estimator is a pipeline\n","                pipe_wo_last_estim = Pipeline(self.estimator.steps[0:-1])\n","                X_trans = pipe_wo_last_estim.fit_transform(X)\n","\n","            # Compute the labels\n","            labels = self.estimator.predict(X)\n","\n","            # # Measure training time while fitting the model on the data\n","            # time_train = %timeit -n1 -r1 -o -q self.estimator.fit(X)\n","            # time_train = time_train.average\n","\n","            # Compute the refit score\n","            try:\n","                if hasattr(self.estimator, 'steps'): # if estimator is a pipeline\n","                    refit_score = self.refit(X_trans, labels)\n","                else:\n","                    refit_score = self.refit(X, labels) # self.scoring['silh'](X, labels)\n","            except:\n","                print('calcul du refit score ne marche pas')\n","                refit_score = np.nan\n","            \n","            # Other scores (scoring)\n","            if not self.scoring:  # if scoring parameter is/are not defined\n","                estim_score['score'] = {'default_score': self.estimator.score(X)} # default score\n","            else:  # If scoring parameter is/are defined\n","                if type(self.scoring) != dict:\n","                    self.scoring = {'score': self.scoring}\n","                else:\n","                    # looping over each score\n","                    for n_sco, sco in self.scoring.items():\n","                        try:\n","                            if hasattr(self.estimator, 'steps'): # if estimator is a pipeline\n","                                estim_score[n_sco] = estim_score[n_sco] + [sco(X_trans, labels)]\n","                        except:\n","                            estim_score[n_sco] = estim_score[n_sco] + [np.nan]\n","                            print(\"calcul des scores ne marche pas\")\n","            if verbose: print(estim_score)\n","            \n","            # # Computing number of clusters, excluding noise (#-1)\n","            # nb_clusters = \\\n","            #     len(set(labels)) - (1 if -1 in set(labels) else 0)\n","            # nb_clusters = int(nb_clusters)\n","            # if verbose: print(nb_clusters)\n","\n","            # saving results, parameters and models in a dict\n","            self.results_[\"refit_score\"].append(refit_score)  # refit score\n","            # self.results_[\"scores\"].append(estim_score)  # dict of lists of scores\n","            self.results_[\"params\"].append(param)  # parameters\n","            self.results_[\"estimators\"].append(self.estimator)  # trained models\n","            # self.results_[\"fit_times\"].append(time_train)  # training time\n","            # self.results_[\"nb_clusters\"].append(nb_clusters)  # nb of clusters\n","\n","        self.results_[\"scores\"] = dict(estim_score)  # dict of lists of scores\n","\n","        # Selecting best model based on the refit_score\n","        # -----------------------------------\n","        # initialisation\n","        best_estim_index, best_score = None, None  \n","        # iterating over scores\n","        for index, score in enumerate(self.results_[\"refit_score\"]):\n","\n","            # initialisation\n","            if not best_score:\n","                best_score = score\n","                best_estim_index = index\n","\n","            # if score is better than current best_score\n","            cond = score > best_score if self.greater_is_better\\\n","                                                 else score < best_score\n","            if cond:\n","                    # update the current best_score and current best_estim_index\n","                    best_score = score\n","                    best_estim_index = index\n","        \n","        # Update attributes of the instance\n","        self.best_score_ = self.results_[\"refit_score\"][best_estim_index]\n","        self.best_params_ = self.results_[\"params\"][best_estim_index]\n","        self.best_estimator_ = self.results_[\"estimators\"][best_estim_index]\n","        self.best_index_ = best_estim_index\n","        # self.refit_time_ = self.results_[\"fit_times\"][best_estim_index]\n","\n","        # refit the best model\n","        self.best_estimator_.fit(X)\n","        \n","        return self\n","\n","    def predict(self, X):\n","\n","        # use the .predict method of the estimator on the best model\n","        return self.best_estim.predict(X)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Izmu6JFSL4fM"},"source":["#### plot_scv_multiscore"]},{"cell_type":"code","metadata":{"id":"uhVrMNGgL3Wv"},"source":["''' Plots the training and test scores obtained during the SearchCV (either Randomized or Grid)\n","the other parameters are parameters of the best estimator (found by gridsearch)'''\n","\n","\n","def plot_scv_multi_scores(name_reg, scv, param, title = None, x_log=False, loc='best', figsize = (12, 4)):\n","\n","    if name_reg is None :\n","        name_reg = scv.estimator.steps[2][0]\n","\n","    best_params, df_sel, df_gscv_filt = filters_cv_results(scv,param)\n","    results = df_gscv_filt\n","\n","    scoring = scv.scoring\n","    fig, axs = plt.subplots(1,len(scoring))\n","    fig.set_size_inches(figsize)\n","    \n","    li_colors = ['b', 'r', 'g', 'purple', 'orange', 'pink']\n","    if len(axs)==1 : axs = [axs]\n","\n","    # Get the regular np array from the MaskedArray\n","       \n","    X_axis = np.array(results['param_'+param], dtype='float')\n","        \n","    for scorer, color, ax in zip(sorted(scoring), li_colors[:len(scoring)], axs):\n","        for sample, style in (('train', '--'), ('test', '-')):\n","            sample_score_mean = results['mean_%s_%s' % (sample, scorer)].values\n","            sample_score_std = results['std_%s_%s' % (sample, scorer)].values\n","            alpha = 0.2 if sample == 'test' else 0.1\n","            df_ = pd.DataFrame({'param': X_axis,\n","                                'mean': sample_score_mean,\n","                                'std': sample_score_std}).sort_values(by='param')\n","            ax.fill_between(df_['param'],\n","                            df_['mean'] - df_['std'],\n","                            df_['mean'] + df_['std'],\n","                            alpha=alpha, color=color)\n","            ax.plot(df_['param'], df_['mean'], style, marker='o', markersize=3,\n","                color=color, alpha=1 if sample == 'test' else 0.7, label=f\"{sample}\")\n","            if x_log: ax.set_xscale('log')\n","            ax.set_title(scorer)\n","            \n","        y_min, y_max = ax.get_ylim()\n","        \n","        # Plot a dotted vertical line at the best score for that scorer marked by x\n","        best_index = results['rank_test_%s' % scorer].argmin()\n","        best_score = results['mean_test_%s' % scorer].iloc[best_index]\n","        ax.plot([X_axis[best_index], ] * 2, [y_min - abs(y_min)*0.1, best_score],\n","            linestyle='dotted', color=color, marker='x', markeredgewidth=3, ms=8)\n","        ax.set_ylim(y_min, y_max)\n","        ax.set_xlabel(param)\n","        ax.set_ylabel(\"Score\")\n","        ax.legend(loc=loc)\n","\n","        # Annotate the best score for that scorer\n","        len_str = len(\"{:.2f}\".format(best_score))\n","        if X_axis[best_index] < np.mean(X_axis):\n","            x_pos = X_axis[best_index]*(1+0.015*len_str)\n","        else:\n","            x_pos = X_axis[best_index]*(1-0.015*len_str)\n","        y_pos = best_score*1+(y_max-y_min)*0.05\n","        ax.annotate(\"{:0.2f}\".format(best_score), \n","                    (x_pos, y_pos),\n","                    color = color)  \n","    if title is not None:\n","        fig.suptitle(title, fontsize=16, fontweight='bold')\n","        plt.tight_layout(rect=(0,0,1,0.92))\n","    else:\n","        plt.tight_layout()\n","    plt.show()\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Gu-0OVgo-3-9"},"source":["#### --- others ---"]},{"cell_type":"code","metadata":{"id":"dtO5xWRVINGG"},"source":["from sklearn.cluster import KMeans\n","\n","def clustering_doc_matrix(doc_matrix_df, name, n_clusters=7):\n","    # Creating the Kmeans model\n","    km = KMeans(n_clusters = n_clusters)\n","    # Fitting the Kmeans model\n","    km.fit(doc_matrix_df)\n","    ser = pd.Series(km.labels_,\n","                    index = doc_matrix_df.index,\n","                    name = name)\n","    return ser"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"acyh8guQ0QU5"},"source":["### Simple example"]},{"cell_type":"code","metadata":{"id":"qWgSqvYqSApq"},"source":["# Stopwords\n","english_sw = nltk.corpus.stopwords.words('english')\n","single_let_sw = list(string.ascii_lowercase)\n","sw = list(set(english_sw + single_let_sw))\n","len(sw)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QK2QYLS74Wtk"},"source":["# Stemmer or lemmatizer\n","from nltk.stem.snowball import EnglishStemmer\n","import spacy\n","\n","stemmer = EnglishStemmer()\n","lemmatizer = spacy.load('en', disable=['parser', 'ner'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5iOMtCtHMAQf"},"source":["# Vectorization of the descriptions prior to applying NMF\n","\n","doc_matrix_df, _ = \\\n","    compute_doc_terms_df(df_desc_cat['description'],\n","                         preproc_func= tokenize_clean,\n","                         preproc_func_params = {'stopwords': sw,\n","                                                'stemmer': None,\n","                                                'lemmatizer': lemmatizer},\n","                         vec_params = {'min_df': 5, # min nb of descriptions that must contain the word\n","                                       'max_features':500}, # max nb of words to keep among the most used\n","                         tfidf_on=False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9kVtgf_jXeMl"},"source":["# Getting the labels of clustering for each set of param (1 param)\n","\n","max_features_list = [30, 70, 100, 150, 250, 500, 1000]\n","\n","for max_feat in max_features_list:\n","    doc_matrix_df, _ = \\\n","        compute_doc_terms_df(df_desc_cat['description'],\n","                         preproc_func= tokenize_clean,\n","                         preproc_func_params = {'stopwords': sw,\n","                                                'stemmer': None,\n","                                                'lemmatizer': lemmatizer},\n","                         vec_params = {'min_df': 5, # min nb of descriptions that must contain the word\n","                                       'max_features':max_feat}, # max nb of words to keep among the most used\n","                         tfidf_on=False)\n","    # Appending the best results of the Kmeans clustering\n","    df_res_clust = pd.concat([df_res_clust,\n","                            clustering_doc_matrix(doc_matrix_df,\n","                                                    str(max_feat))], axis=1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lwsdaIlXINQQ"},"source":["df_res_clust.sample(5)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wG78-TpyqFSx"},"source":["# Comparison of clusters labels with true categories\n","\n","from sklearn.metrics import adjusted_rand_score\n","\n","ser_ari_pairs_models = ARI_column_pairs(df_res_clust, first_vs_others=True,\n","                                        print_opt=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Wn977U9eeZ2G"},"source":["# Plotting the results\n","\n","fig = plt.figure(figsize=(2,3))\n","ser_ari_pairs_models.plot.bar(width=0.7, color='grey', ec='k')\n","plt.ylabel('ARI score')\n","# plt.title('ARI score comparing the cluster\\nlabel prediction of pairs of models')\n","# plt.gca().set(ylim=(0.85,1))\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wDrsjSfz0b6R"},"source":["### GridSearch"]},{"cell_type":"markdown","metadata":{"id":"nGpDxTlA-XmE"},"source":["#### GridSearch without preprocessing"]},{"cell_type":"code","metadata":{"id":"h_p4Bus2c2Cv"},"source":["from sklearn.metrics import silhouette_score,calinski_harabasz_score, davies_bouldin_score\n","\n","# Definition of the search space for hyperparameters\n","param_grid = {\"n_clusters\": np.arange(3,10)}\n","\n","# Instanciation of the GridSearch object\n","gsc = GridSearchClust(estimator=KMeans(random_state=14),\n","                      param_grid_estim=param_grid,\n","                      scoring={'silh': silhouette_score,\n","                               'dav_bould': davies_bouldin_score,\n","                               'cal_har': calinski_harabasz_score},\n","                      refit=silhouette_score,\n","                      greater_is_better=True, # for the refit_score\n","                      )\n","\n","# Proceeding the grid search\n","gsc.fit(doc_matrix_df, verbose=False);\n","\n","# Displays best parameters\n","print(\"Best hyperparameters:\", gsc.best_params_)\n","print(\"Best Silhouette score:\", gsc.best_score_)\n","gsc_res = gsc.results_\n","scores_df = pd.DataFrame(gsc_res['scores'],\n","                         index = pd.DataFrame(gsc_res['params']).iloc[:,0])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HvSsEXzbvH2q"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MjVjEjU6r3lR"},"source":["#### GridSearch with preprocessing optimization"]},{"cell_type":"markdown","metadata":{"id":"VpIEXMuu0TV8"},"source":["\n","- créer un custom transformer pour le nlp (init : paramètres) avec .transform\n","- appliquer un predict dans le Gridsearch qui pourrait s'appliquer à un km ou à un pipe\n","- vérifier que les fonctions d'affichage (heatmap, courbes) des scores en fonction des paramètres sont compatibles avec ma fonction GridSearchClust (marche toujours sans cv ?)"]},{"cell_type":"code","metadata":{"id":"A7bfrsiWr28E"},"source":["from sklearn.pipeline import Pipeline\n","from sklearn.metrics import silhouette_score,calinski_harabasz_score, davies_bouldin_score\n","\n","# Stemmer or lemmatizer\n","from nltk.stem.snowball import EnglishStemmer\n","import spacy\n","stemmer = EnglishStemmer()\n","lemmatizer = spacy.load('en', disable=['parser', 'ner'])\n","\n","# clusterer\n","km = KMeans(random_state=14)\n","\n","custtrans = CustNLPTransformer(stopwords=sw,\n","                               stemmer=None,\n","                               lemmatizer=None,\n","                               min_df=1,\n","                               max_df=200, # nb max de documents dans lequel le mot apparaît (150 documents par catégorie)\n","                               max_features=500,\n","                               tfidf_on=False\n","                               )\n","\n","# Defining the pipeline to be executed and optimized by the GridSearch\n","pipe = Pipeline([('custtrans', custtrans),\n","                 ('clusterer', km)])\n","\n","# Defining the list of params to be tested in the GridSearchClust\n","param_grid = {'custtrans__stopwords': [None], # [sw, None],\n","              'custtrans__stemmer': [None],#[stemmer, None],\n","              'custtrans__lemmatizer': [None],\n","              'custtrans__min_df': [1],\n","              'custtrans__max_df': [200], # nb max de documents dans lequel le mot apparaît (150 documents par catégorie)\n","              'custtrans__max_features': [300,500,1000],\n","              'custtrans__tfidf_on': [True], #[True, False],\n","              'clusterer__n_clusters': np.arange(3,6)\n","              }\n","\n","# Instanciation of the GridSearch object\n","gsc = GridSearchClust(estimator=pipe,\n","                      param_grid_estim=param_grid,\n","                      scoring={'silh': silhouette_score,\n","                               'dav_bould': davies_bouldin_score,\n","                               'cal_har': calinski_harabasz_score},\n","                      refit=silhouette_score,\n","                      greater_is_better=True) # for the refit_score\n","\n","# Proceeding the grid search\n","gsc.fit(df_desc_cat['description'], verbose=False);\n","\n","# Displays best parameters\n","print(\"Best hyperparameters:\", gsc.best_params_)\n","print(\"Best Silhouette score:\", gsc.best_score_)\n","gsc_res = gsc.results_\n","scores_df = pd.DataFrame(gsc_res['scores'],\n","                         index = pd.DataFrame(gsc_res['params']).iloc[:,0])\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DrsvfXSFAtRE"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_4-JqF3BwoWn"},"source":["gsc_res.keys()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0fRqKKGH85r_"},"source":["gsc_res['scores']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ieq3eNmY85zd"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pBZbTTpm85wn"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ooniIC-qMBI4"},"source":["#### Plotting the results of the clustering optimization"]},{"cell_type":"code","metadata":{"id":"3S7irZ5C8dJp"},"source":["dict_scv_params, models_file_name, l_curves_file_name, perm_imp_file_name = \\\n","        set_dict_scv_params(X_tr, y_tr, target, log_on=False, refit=refit_score)\n","\n","df_res = run_optimization(name_reg, reg, param_grid, models_file_name, dict_models,\n","                          pipe, dict_scv_params, skf, df_res, search_strat, n_iter)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oPoVz3ye8dGo"},"source":["# Best parameters\n","dict_models[name_reg].best_params_"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3LABn76_8dDu"},"source":["# Cross-validation scores on the training set\n","df_res[name_reg].to_frame().T.dropna(1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"c9ZK277f8kpf"},"source":["# Multiscoring with hyperparameter tuning\n","scv = dict_models[name_reg]\n","plot_scv_multi_scores(name_reg, scv, param = name_reg+'__n_neighbors',\n","                      title = None, figsize = (15, 3))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dh8W2Ojk8k0N"},"source":["# Effect of encoding on performance\n","\n","param1 = name_reg+'__'+'n_neighbors'\n","param2 = name_reg+'__'+'p'\n","param3 = 'preproc__cust_trans__strat_low_card'\n","param4 = 'preproc__cust_trans__strat_high_card'\n","param5 = 'preproc__cust_trans__strat_quant'\n","score = 'r2'\n","\n","fig = plt.figure(figsize=(20,3))\n","ax = fig.add_subplot(1,4,1)\n","plot_2D_hyperparam_opt(scv=dict_models[name_reg], params=[param1, param2],\n","                       score = score, ax=ax)\n","ax = fig.add_subplot(1,4,2)\n","plot_2D_hyperparam_opt(scv=dict_models[name_reg], params=[param1, param3],\n","                       score = score, ax=ax)\n","ax = fig.add_subplot(1,4,3)\n","plot_2D_hyperparam_opt(scv=dict_models[name_reg], params=[param1, param4],\n","                       score = score, ax=ax)\n","ax = fig.add_subplot(1,4,4)\n","plot_2D_hyperparam_opt(scv=dict_models[name_reg], params=[param1, param5],\n","                       score = score, ax=ax)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ES_X92UL8k4r"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"To72UV-T8dAx"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_BRj7Qw8r22x"},"source":["from sklearn.metrics import silhouette_score,calinski_harabasz_score, davies_bouldin_score\n","\n","# Definition of the search space for hyperparameters\n","param_grid = {\"n_clusters\": np.arange(3,10)}\n","\n","# Instanciation of the GridSearch object\n","gsc = GridSearchClust(estimator=KMeans(random_state=14),\n","                      param_grid_estim=param_grid,\n","                      scoring={'silh': silhouette_score,\n","                               'dav_bould': davies_bouldin_score,\n","                               'cal_har': calinski_harabasz_score},\n","                      refit=silhouette_score,\n","                      greater_is_better=True, # for the refit_score\n","                      )\n","\n","# Proceeding the grid search\n","gsc.fit(doc_matrix_df, verbose=False);\n","\n","# Displays best parameters\n","print(\"Best hyperparameters:\", gsc.best_params_)\n","print(\"Best Silhouette score:\", gsc.best_score_)\n","gsc_res = gsc.results_\n","scores_df = pd.DataFrame(gsc_res['scores'],\n","                         index = pd.DataFrame(gsc_res['params']).iloc[:,0])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-rht6qVn6DSl"},"source":["scores_df"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ol9vr-bM6D3B"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZoxKnI-i6D_j"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VSd-EaP16D9o"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KqKhl3s46D7H"},"source":[""],"execution_count":null,"outputs":[]}]}