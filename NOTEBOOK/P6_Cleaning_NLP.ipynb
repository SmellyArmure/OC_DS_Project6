{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"P6_Cleaning_NLP.ipynb","provenance":[{"file_id":"1qAxkvKD2odWAQUnotM2Dwijh45Ko0iCO","timestamp":1587461379791},{"file_id":"1wX6ZnTUKkvXaWwnL6E51NNuJkxEdSK-h","timestamp":1582132452639},{"file_id":"144JCi9-nMiX9eD3AccG2sgaTbug2wQTa","timestamp":1581758429991},{"file_id":"1kRjoSOVhLf1GbPOY6C2MGdpHdjUsJGe1","timestamp":1581430098593},{"file_id":"1SzuDOE2ejfYYNHpdu1hTrvQZd5phpvyP","timestamp":1581092731309},{"file_id":"1NHY7TNgChDa8i5eggkSxZV133o6RB2pP","timestamp":1580472396109},{"file_id":"1CjFqLqI3e83aWkErpy2_tDoqWWSc5V7K","timestamp":1567509524556},{"file_id":"1oFtNqY9sTtyX09HnsLY5GOpchE7TWSLc","timestamp":1567440734485},{"file_id":"1kO3qnFJ8XAhA2WzueAy6Gwr0KwfoSF2z","timestamp":1566893631574},{"file_id":"1rI7P6dn7-IGK6p8HX7dvNP93roGSzXeX","timestamp":1566833630097},{"file_id":"1cAbXwtjxfOIVmnecCFlHrVryskoYJCXA","timestamp":1566734390006},{"file_id":"https://github.com/SmellyArmure/PROJECT3/blob/master/NOTEBOOKS/P3_Cleaning_v1_0.ipynb","timestamp":1566726579304}],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"pVbrKjWrGl4m"},"source":["# \"Classifiez automatiquement des biens de consommation\"\n","_NLP Cleaning Notebook_"]},{"cell_type":"markdown","metadata":{"id":"AXy2xt5wB3ZD"},"source":["## 0 Preliminaries"]},{"cell_type":"markdown","metadata":{"id":"NQ8_ZaJvGl4o"},"source":["### 0.0 Importing Packages and Modules"]},{"cell_type":"markdown","metadata":{"id":"XKUzl6mHOcZa"},"source":["Checking whether the notebook is on Colab or PC"]},{"cell_type":"code","metadata":{"id":"S7MAxokr4UmP","executionInfo":{"status":"ok","timestamp":1603917348929,"user_tz":-60,"elapsed":5174,"user":{"displayName":"Maryse Muller","photoUrl":"","userId":"13230828717959946182"}},"outputId":"c24a72ed-ca11-466f-9b3b-4b0430444bc3","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["import sys\n","is_colab = 'google.colab' in sys.modules\n","is_colab, sys.executable"],"execution_count":165,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(True, '/usr/bin/python3')"]},"metadata":{"tags":[]},"execution_count":165}]},{"cell_type":"markdown","metadata":{"id":"g9oJU8UHOaRC"},"source":["Mounting my Drive if on Colab"]},{"cell_type":"code","metadata":{"id":"l5RrOSXvfGrC","executionInfo":{"status":"ok","timestamp":1603917350151,"user_tz":-60,"elapsed":6273,"user":{"displayName":"Maryse Muller","photoUrl":"","userId":"13230828717959946182"}},"outputId":"b694d04b-be2b-46e8-bbbc-92559fa73f7d","colab":{"base_uri":"https://localhost:8080/","height":69}},"source":["if is_colab==True:\n","    from google.colab import files, output, drive\n","    drive.mount('/gdrive')\n","    %cd /gdrive\n","    print(\"You're on Google Colab\")\n","else:\n","    print(\"You're on a PC\")"],"execution_count":166,"outputs":[{"output_type":"stream","text":["Drive already mounted at /gdrive; to attempt to forcibly remount, call drive.mount(\"/gdrive\", force_remount=True).\n","/gdrive\n","You're on Google Colab\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"F7E_-Zd9OgY7"},"source":["Installations and importations required in the virtual environment."]},{"cell_type":"code","metadata":{"id":"VZ__n1yHHrQJ","executionInfo":{"status":"ok","timestamp":1603917350153,"user_tz":-60,"elapsed":6217,"user":{"displayName":"Maryse Muller","photoUrl":"","userId":"13230828717959946182"}}},"source":["# import os\n","# if os.getcwd()!='/gdrive/My Drive/--DATA SCIENCE/PROJET6/NOTEBOOKS':\n","#     os.chdir('/gdrive/My Drive/--DATA SCIENCE/PROJET6/NOTEBOOKS')"],"execution_count":167,"outputs":[]},{"cell_type":"code","metadata":{"id":"TCg0w1hgprYQ","executionInfo":{"status":"ok","timestamp":1603917353693,"user_tz":-60,"elapsed":9655,"user":{"displayName":"Maryse Muller","photoUrl":"","userId":"13230828717959946182"}},"outputId":"75510552-b0d6-44b6-8099-d00e03d59c37","colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["import os\n","if is_colab==True:\n","    if os.getcwd()!='/gdrive/My Drive/--DATA SCIENCE/PROJET6/NOTEBOOKS':\n","        os.chdir('/gdrive/My Drive/--DATA SCIENCE/PROJET6/NOTEBOOKS')\n","else:\n","    if not (os.path.exists(os.getcwd()+'/requirements_nlp.txt') \\\n","                     and os.path.exists(os.getcwd()+'/P6_functions.py')):\n","        print(\"ERROR: Make sure 'P6_functions.py' and \\\n","'requirements_nlp.txt' are in the current working directory\")\n","\n","!pip install -r requirements_nlp.txt"],"execution_count":168,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: absl-py==0.10.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 1)) (0.10.0)\n","Requirement already satisfied: alabaster==0.7.12 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 2)) (0.7.12)\n","Requirement already satisfied: albumentations==0.1.12 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 3)) (0.1.12)\n","Requirement already satisfied: altair==4.1.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 4)) (4.1.0)\n","Requirement already satisfied: argon2-cffi==20.1.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 5)) (20.1.0)\n","Requirement already satisfied: asgiref==3.2.10 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 6)) (3.2.10)\n","Requirement already satisfied: astor==0.8.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 7)) (0.8.1)\n","Requirement already satisfied: astropy==4.0.1.post1 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 8)) (4.0.1.post1)\n","Requirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 9)) (1.6.3)\n","Requirement already satisfied: async-generator==1.10 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 10)) (1.10)\n","Requirement already satisfied: atari-py==0.2.6 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 11)) (0.2.6)\n","Requirement already satisfied: atomicwrites==1.4.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 12)) (1.4.0)\n","Requirement already satisfied: attrs==20.2.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 13)) (20.2.0)\n","Requirement already satisfied: audioread==2.1.8 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 14)) (2.1.8)\n","Requirement already satisfied: autograd==1.3 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 15)) (1.3)\n","Requirement already satisfied: Babel==2.8.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 16)) (2.8.0)\n","Requirement already satisfied: backcall==0.2.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 17)) (0.2.0)\n","Requirement already satisfied: beautifulsoup4==4.6.3 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 18)) (4.6.3)\n","Requirement already satisfied: bleach==3.2.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 19)) (3.2.1)\n","Requirement already satisfied: blis==0.4.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 20)) (0.4.1)\n","Requirement already satisfied: bokeh==2.1.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 21)) (2.1.1)\n","Requirement already satisfied: Bottleneck==1.3.2 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 22)) (1.3.2)\n","Requirement already satisfied: branca==0.4.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 23)) (0.4.1)\n","Requirement already satisfied: bs4==0.0.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 24)) (0.0.1)\n","Requirement already satisfied: CacheControl==0.12.6 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 25)) (0.12.6)\n","Requirement already satisfied: cachetools==4.1.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 26)) (4.1.1)\n","Requirement already satisfied: catalogue==1.0.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 27)) (1.0.0)\n","Requirement already satisfied: category-encoders==2.2.2 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 28)) (2.2.2)\n","Requirement already satisfied: certifi==2020.6.20 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 29)) (2020.6.20)\n","Requirement already satisfied: cffi==1.14.3 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 30)) (1.14.3)\n","Requirement already satisfied: chainer==7.4.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 31)) (7.4.0)\n","Requirement already satisfied: chardet==3.0.4 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 32)) (3.0.4)\n","Requirement already satisfied: click==7.1.2 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 33)) (7.1.2)\n","Requirement already satisfied: cloudpickle==1.3.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 34)) (1.3.0)\n","Requirement already satisfied: cmake==3.12.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 35)) (3.12.0)\n","Requirement already satisfied: cmdstanpy==0.9.5 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 36)) (0.9.5)\n","Requirement already satisfied: colorlover==0.3.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 37)) (0.3.0)\n","Requirement already satisfied: community==1.0.0b1 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 38)) (1.0.0b1)\n","Requirement already satisfied: contextlib2==0.5.5 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 39)) (0.5.5)\n","Requirement already satisfied: convertdate==2.2.2 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 40)) (2.2.2)\n","Requirement already satisfied: coverage==3.7.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 41)) (3.7.1)\n","Requirement already satisfied: coveralls==0.5 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 42)) (0.5)\n","Requirement already satisfied: crcmod==1.7 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 43)) (1.7)\n","Requirement already satisfied: cufflinks==0.17.3 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 44)) (0.17.3)\n","Requirement already satisfied: cvxopt==1.2.5 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 45)) (1.2.5)\n","Requirement already satisfied: cvxpy==1.0.31 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 46)) (1.0.31)\n","Requirement already satisfied: cycler==0.10.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 47)) (0.10.0)\n","Requirement already satisfied: cymem==2.0.3 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 48)) (2.0.3)\n","Requirement already satisfied: Cython==0.29.21 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 49)) (0.29.21)\n","Requirement already satisfied: daft==0.0.4 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 50)) (0.0.4)\n","Requirement already satisfied: dask==2.12.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 51)) (2.12.0)\n","Requirement already satisfied: dataclasses==0.7 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 52)) (0.7)\n","Requirement already satisfied: datascience==0.10.6 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 53)) (0.10.6)\n","Requirement already satisfied: debugpy==1.0.0rc2 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 54)) (1.0.0rc2)\n","Requirement already satisfied: decorator==4.4.2 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 55)) (4.4.2)\n","Requirement already satisfied: defusedxml==0.6.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 56)) (0.6.0)\n","Requirement already satisfied: descartes==1.1.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 57)) (1.1.0)\n","Requirement already satisfied: dill==0.3.2 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 58)) (0.3.2)\n","Requirement already satisfied: distributed==1.25.3 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 59)) (1.25.3)\n","Requirement already satisfied: Django==3.1.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 60)) (3.1.1)\n","Requirement already satisfied: dlib==19.18.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 61)) (19.18.0)\n","Requirement already satisfied: dm-tree==0.1.5 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 62)) (0.1.5)\n","Requirement already satisfied: docopt==0.6.2 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 63)) (0.6.2)\n","Requirement already satisfied: docutils==0.16 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 64)) (0.16)\n","Requirement already satisfied: dopamine-rl==1.0.5 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 65)) (1.0.5)\n","Requirement already satisfied: earthengine-api==0.1.236 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 66)) (0.1.236)\n","Requirement already satisfied: easydict==1.9 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 67)) (1.9)\n","Requirement already satisfied: ecos==2.0.7.post1 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 68)) (2.0.7.post1)\n","Requirement already satisfied: editdistance==0.5.3 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 69)) (0.5.3)\n","Requirement already satisfied: en-core-web-sm==2.2.5 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 70)) (2.2.5)\n","Requirement already satisfied: entrypoints==0.3 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 71)) (0.3)\n","Requirement already satisfied: ephem==3.7.7.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 72)) (3.7.7.1)\n","Requirement already satisfied: et-xmlfile==1.0.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 73)) (1.0.1)\n","Requirement already satisfied: fa2==0.3.5 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 74)) (0.3.5)\n","Requirement already satisfied: fancyimpute==0.4.3 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 75)) (0.4.3)\n","Requirement already satisfied: fastai==1.0.61 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 76)) (1.0.61)\n","Requirement already satisfied: fastdtw==0.3.4 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 77)) (0.3.4)\n","Requirement already satisfied: fastprogress==1.0.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 78)) (1.0.0)\n","Requirement already satisfied: fastrlock==0.5 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 79)) (0.5)\n","Requirement already satisfied: fbprophet==0.7.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 80)) (0.7.1)\n","Requirement already satisfied: feather-format==0.4.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 81)) (0.4.1)\n","Requirement already satisfied: filelock==3.0.12 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 82)) (3.0.12)\n","Requirement already satisfied: firebase-admin==4.4.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 83)) (4.4.0)\n","Requirement already satisfied: fix-yahoo-finance==0.0.22 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 84)) (0.0.22)\n","Requirement already satisfied: Flask==1.1.2 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 85)) (1.1.2)\n","Requirement already satisfied: folium==0.8.3 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 86)) (0.8.3)\n","Requirement already satisfied: future==0.16.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 87)) (0.16.0)\n","Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 88)) (0.3.3)\n","Requirement already satisfied: GDAL==2.2.2 in /usr/lib/python3/dist-packages (from -r requirements_nlp.txt (line 89)) (2.2.2)\n","Requirement already satisfied: gdown==3.6.4 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 90)) (3.6.4)\n","Requirement already satisfied: gensim==3.6.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 91)) (3.6.0)\n","Requirement already satisfied: geographiclib==1.50 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 92)) (1.50)\n","Requirement already satisfied: geopy==1.17.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 93)) (1.17.0)\n","Requirement already satisfied: gin-config==0.3.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 94)) (0.3.0)\n","Requirement already satisfied: glob2==0.7 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 95)) (0.7)\n","Requirement already satisfied: google==2.0.3 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 96)) (2.0.3)\n","Requirement already satisfied: google-api-core==1.16.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 97)) (1.16.0)\n","Requirement already satisfied: google-api-python-client==1.7.12 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 98)) (1.7.12)\n","Requirement already satisfied: google-auth==1.17.2 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 99)) (1.17.2)\n","Requirement already satisfied: google-auth-httplib2==0.0.4 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 100)) (0.0.4)\n","Requirement already satisfied: google-auth-oauthlib==0.4.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 101)) (0.4.1)\n","Requirement already satisfied: google-cloud-bigquery==1.21.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 102)) (1.21.0)\n","Requirement already satisfied: google-cloud-core==1.0.3 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 103)) (1.0.3)\n","Requirement already satisfied: google-cloud-datastore==1.8.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 104)) (1.8.0)\n","Requirement already satisfied: google-cloud-firestore==1.7.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 105)) (1.7.0)\n","Requirement already satisfied: google-cloud-language==1.2.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 106)) (1.2.0)\n","Requirement already satisfied: google-cloud-storage==1.18.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 107)) (1.18.1)\n","Requirement already satisfied: google-cloud-translate==1.5.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 108)) (1.5.0)\n","Requirement already satisfied: google-colab==1.0.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 109)) (1.0.0)\n","Requirement already satisfied: google-pasta==0.2.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 110)) (0.2.0)\n","Requirement already satisfied: google-resumable-media==0.4.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 111)) (0.4.1)\n","Requirement already satisfied: googleapis-common-protos==1.52.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 112)) (1.52.0)\n","Requirement already satisfied: googledrivedownloader==0.4 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 113)) (0.4)\n","Requirement already satisfied: graphviz==0.10.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 114)) (0.10.1)\n","Requirement already satisfied: grpcio==1.32.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 115)) (1.32.0)\n","Requirement already satisfied: gspread==3.0.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 116)) (3.0.1)\n","Requirement already satisfied: gspread-dataframe==3.0.8 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 117)) (3.0.8)\n","Requirement already satisfied: gTTS==2.1.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 118)) (2.1.1)\n","Requirement already satisfied: gTTS-token==1.1.3 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 119)) (1.1.3)\n","Requirement already satisfied: gym==0.17.2 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 120)) (0.17.2)\n","Requirement already satisfied: h5py==2.10.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 121)) (2.10.0)\n","Requirement already satisfied: HeapDict==1.0.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 122)) (1.0.1)\n","Requirement already satisfied: holidays==0.10.3 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 123)) (0.10.3)\n","Requirement already satisfied: holoviews==1.13.4 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 124)) (1.13.4)\n","Requirement already satisfied: html5lib==1.0.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 125)) (1.0.1)\n","Requirement already satisfied: httpimport==0.5.18 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 126)) (0.5.18)\n","Requirement already satisfied: httplib2==0.17.4 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 127)) (0.17.4)\n","Requirement already satisfied: httplib2shim==0.0.3 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 128)) (0.0.3)\n","Requirement already satisfied: humanize==0.5.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 129)) (0.5.1)\n","Requirement already satisfied: hyperopt==0.1.2 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 130)) (0.1.2)\n","Requirement already satisfied: ideep4py==2.0.0.post3 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 131)) (2.0.0.post3)\n","Requirement already satisfied: idna==2.10 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 132)) (2.10)\n","Requirement already satisfied: image==1.5.32 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 133)) (1.5.32)\n","Requirement already satisfied: imageio==2.4.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 134)) (2.4.1)\n","Requirement already satisfied: imagesize==1.2.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 135)) (1.2.0)\n","Requirement already satisfied: imbalanced-learn==0.4.3 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 136)) (0.4.3)\n","Requirement already satisfied: imblearn==0.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 137)) (0.0)\n","Requirement already satisfied: imgaug==0.2.9 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 138)) (0.2.9)\n","Requirement already satisfied: importlib-metadata==2.0.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 139)) (2.0.0)\n","Requirement already satisfied: imutils==0.5.3 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 140)) (0.5.3)\n","Requirement already satisfied: inflect==2.1.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 141)) (2.1.0)\n","Requirement already satisfied: iniconfig==1.0.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 142)) (1.0.1)\n","Requirement already satisfied: intel-openmp==2020.0.133 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 143)) (2020.0.133)\n","Requirement already satisfied: intervaltree==2.1.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 144)) (2.1.0)\n","Requirement already satisfied: ipykernel==4.10.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 145)) (4.10.1)\n","Requirement already satisfied: ipython==5.5.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 146)) (5.5.0)\n","Requirement already satisfied: ipython-genutils==0.2.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 147)) (0.2.0)\n","Requirement already satisfied: ipython-sql==0.3.9 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 148)) (0.3.9)\n","Requirement already satisfied: ipywidgets==7.5.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 149)) (7.5.1)\n","Requirement already satisfied: itsdangerous==1.1.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 150)) (1.1.0)\n","Requirement already satisfied: jax==0.2.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 151)) (0.2.0)\n","Requirement already satisfied: jaxlib==0.1.55 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 152)) (0.1.55)\n","Requirement already satisfied: jdcal==1.4.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 153)) (1.4.1)\n","Requirement already satisfied: jedi==0.17.2 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 154)) (0.17.2)\n","Requirement already satisfied: jieba==0.42.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 155)) (0.42.1)\n","Requirement already satisfied: Jinja2==2.11.2 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 156)) (2.11.2)\n","Requirement already satisfied: joblib==0.16.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 157)) (0.16.0)\n","Requirement already satisfied: jpeg4py==0.1.4 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 158)) (0.1.4)\n","Requirement already satisfied: jsonschema==2.6.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 159)) (2.6.0)\n","Requirement already satisfied: jupyter==1.0.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 160)) (1.0.0)\n","Requirement already satisfied: jupyter-client==5.3.5 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 161)) (5.3.5)\n","Requirement already satisfied: jupyter-console==5.2.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 162)) (5.2.0)\n","Requirement already satisfied: jupyter-core==4.6.3 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 163)) (4.6.3)\n","Requirement already satisfied: jupyterlab-pygments==0.1.2 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 164)) (0.1.2)\n","Requirement already satisfied: kaggle==1.5.8 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 165)) (1.5.8)\n","Requirement already satisfied: kapre==0.1.3.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 166)) (0.1.3.1)\n","Requirement already satisfied: Keras==2.4.3 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 167)) (2.4.3)\n","Requirement already satisfied: Keras-Preprocessing==1.1.2 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 168)) (1.1.2)\n","Requirement already satisfied: keras-vis==0.4.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 169)) (0.4.1)\n","Requirement already satisfied: kiwisolver==1.2.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 170)) (1.2.0)\n","Requirement already satisfied: knnimpute==0.1.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 171)) (0.1.0)\n","Requirement already satisfied: korean-lunar-calendar==0.2.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 172)) (0.2.1)\n","Requirement already satisfied: librosa==0.6.3 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 173)) (0.6.3)\n","Requirement already satisfied: lightgbm==2.2.3 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 174)) (2.2.3)\n","Requirement already satisfied: llvmlite==0.31.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 175)) (0.31.0)\n","Requirement already satisfied: lmdb==0.99 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 176)) (0.99)\n","Requirement already satisfied: lucid==0.3.8 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 177)) (0.3.8)\n","Requirement already satisfied: LunarCalendar==0.0.9 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 178)) (0.0.9)\n","Requirement already satisfied: lxml==4.2.6 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 179)) (4.2.6)\n","Requirement already satisfied: marisa-trie==0.7.5 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 180)) (0.7.5)\n","Requirement already satisfied: Markdown==3.2.2 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 181)) (3.2.2)\n","Requirement already satisfied: MarkupSafe==1.1.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 182)) (1.1.1)\n","Requirement already satisfied: matplotlib==3.2.2 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 183)) (3.2.2)\n","Requirement already satisfied: matplotlib-venn==0.11.5 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 184)) (0.11.5)\n","Requirement already satisfied: missingno==0.4.2 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 185)) (0.4.2)\n","Requirement already satisfied: mistune==0.8.4 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 186)) (0.8.4)\n","Requirement already satisfied: mizani==0.6.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 187)) (0.6.0)\n","Requirement already satisfied: mkl==2019.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 188)) (2019.0)\n","Requirement already satisfied: mlxtend==0.14.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 189)) (0.14.0)\n","Requirement already satisfied: more-itertools==8.5.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 190)) (8.5.0)\n","Requirement already satisfied: moviepy==0.2.3.5 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 191)) (0.2.3.5)\n","Requirement already satisfied: mpmath==1.1.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 192)) (1.1.0)\n","Requirement already satisfied: msgpack==1.0.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 193)) (1.0.0)\n","Requirement already satisfied: multiprocess==0.70.10 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 194)) (0.70.10)\n","Requirement already satisfied: multitasking==0.0.9 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 195)) (0.0.9)\n","Requirement already satisfied: murmurhash==1.0.2 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 196)) (1.0.2)\n","Requirement already satisfied: music21==5.5.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 197)) (5.5.0)\n","Requirement already satisfied: mwparserfromhell==0.5.4 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 198)) (0.5.4)\n","Requirement already satisfied: natsort==5.5.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 199)) (5.5.0)\n","Requirement already satisfied: nbclient==0.5.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 200)) (0.5.0)\n","Requirement already satisfied: nbconvert==5.6.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 201)) (5.6.1)\n","Requirement already satisfied: nbformat==5.0.7 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 202)) (5.0.7)\n","Requirement already satisfied: nest-asyncio==1.4.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 203)) (1.4.1)\n","Requirement already satisfied: networkx==2.5 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 204)) (2.5)\n","Requirement already satisfied: nibabel==3.0.2 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 205)) (3.0.2)\n","Requirement already satisfied: nltk==3.2.5 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 206)) (3.2.5)\n","Requirement already satisfied: notebook==5.3.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 207)) (5.3.1)\n","Requirement already satisfied: np-utils==0.5.12.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 208)) (0.5.12.1)\n","Requirement already satisfied: numba==0.48.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 209)) (0.48.0)\n","Requirement already satisfied: numexpr==2.7.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 210)) (2.7.1)\n","Requirement already satisfied: numpy==1.18.5 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 211)) (1.18.5)\n","Requirement already satisfied: nvidia-ml-py3==7.352.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 212)) (7.352.0)\n","Requirement already satisfied: oauth2client==4.1.3 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 213)) (4.1.3)\n","Requirement already satisfied: oauthlib==3.1.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 214)) (3.1.0)\n","Requirement already satisfied: okgrade==0.4.3 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 215)) (0.4.3)\n","Requirement already satisfied: opencv-contrib-python==4.1.2.30 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 216)) (4.1.2.30)\n","Requirement already satisfied: opencv-python==4.1.2.30 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 217)) (4.1.2.30)\n","Requirement already satisfied: openpyxl==2.5.9 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 218)) (2.5.9)\n","Requirement already satisfied: opt-einsum==3.3.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 219)) (3.3.0)\n","Requirement already satisfied: osqp==0.6.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 220)) (0.6.1)\n","Requirement already satisfied: packaging==20.4 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 221)) (20.4)\n","Requirement already satisfied: palettable==3.3.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 222)) (3.3.0)\n","Requirement already satisfied: pandas==1.1.2 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 223)) (1.1.2)\n","Requirement already satisfied: pandas-datareader==0.9.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 224)) (0.9.0)\n","Requirement already satisfied: pandas-gbq==0.13.2 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 225)) (0.13.2)\n","Requirement already satisfied: pandas-profiling==1.4.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 226)) (1.4.1)\n","Requirement already satisfied: pandocfilters==1.4.2 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 227)) (1.4.2)\n","Requirement already satisfied: panel==0.9.7 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 228)) (0.9.7)\n","Requirement already satisfied: param==1.9.3 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 229)) (1.9.3)\n","Requirement already satisfied: parso==0.7.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 230)) (0.7.1)\n","Requirement already satisfied: pathlib==1.0.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 231)) (1.0.1)\n","Requirement already satisfied: patsy==0.5.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 232)) (0.5.1)\n","Requirement already satisfied: pexpect==4.8.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 233)) (4.8.0)\n","Requirement already satisfied: pickleshare==0.7.5 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 234)) (0.7.5)\n","Requirement already satisfied: Pillow==7.0.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 235)) (7.0.0)\n","Requirement already satisfied: pip-tools==4.5.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 236)) (4.5.1)\n","Requirement already satisfied: plac==1.1.3 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 237)) (1.1.3)\n","Requirement already satisfied: plotly==4.4.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 238)) (4.4.1)\n","Requirement already satisfied: plotnine==0.6.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 239)) (0.6.0)\n","Requirement already satisfied: pluggy==0.7.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 240)) (0.7.1)\n","Requirement already satisfied: portpicker==1.3.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 241)) (1.3.1)\n","Requirement already satisfied: prefetch-generator==1.0.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 242)) (1.0.1)\n","Requirement already satisfied: preshed==3.0.2 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 243)) (3.0.2)\n","Requirement already satisfied: prettytable==0.7.2 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 244)) (0.7.2)\n","Requirement already satisfied: progressbar2==3.38.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 245)) (3.38.0)\n","Requirement already satisfied: prometheus-client==0.8.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 246)) (0.8.0)\n","Requirement already satisfied: promise==2.3 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 247)) (2.3)\n","Requirement already satisfied: prompt-toolkit==1.0.18 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 248)) (1.0.18)\n","Requirement already satisfied: protobuf==3.12.4 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 249)) (3.12.4)\n","Requirement already satisfied: psutil==5.4.8 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 250)) (5.4.8)\n","Requirement already satisfied: psycopg2==2.7.6.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 251)) (2.7.6.1)\n","Requirement already satisfied: ptyprocess==0.6.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 252)) (0.6.0)\n","Requirement already satisfied: py==1.9.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 253)) (1.9.0)\n","Requirement already satisfied: pyarrow==0.14.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 254)) (0.14.1)\n","Requirement already satisfied: pyasn1==0.4.8 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 255)) (0.4.8)\n","Requirement already satisfied: pyasn1-modules==0.2.8 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 256)) (0.2.8)\n","Requirement already satisfied: pycocotools==2.0.2 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 257)) (2.0.2)\n","Requirement already satisfied: pycparser==2.20 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 258)) (2.20)\n","Requirement already satisfied: pyct==0.4.8 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 259)) (0.4.8)\n","Requirement already satisfied: pydata-google-auth==1.1.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 260)) (1.1.0)\n","Requirement already satisfied: pydot==1.3.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 261)) (1.3.0)\n","Requirement already satisfied: pydot-ng==2.0.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 262)) (2.0.0)\n","Requirement already satisfied: pydotplus==2.0.2 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 263)) (2.0.2)\n","Requirement already satisfied: PyDrive==1.3.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 264)) (1.3.1)\n","Requirement already satisfied: pyemd==0.5.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 265)) (0.5.1)\n","Requirement already satisfied: pyglet==1.5.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 266)) (1.5.0)\n","Requirement already satisfied: Pygments==2.6.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 267)) (2.6.1)\n","Requirement already satisfied: pygobject==3.26.1 in /usr/lib/python3/dist-packages (from -r requirements_nlp.txt (line 268)) (3.26.1)\n","Requirement already satisfied: pymc3==3.7 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 269)) (3.7)\n","Requirement already satisfied: PyMeeus==0.3.7 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 270)) (0.3.7)\n","Requirement already satisfied: pymongo==3.11.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 271)) (3.11.0)\n","Requirement already satisfied: pymystem3==0.2.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 272)) (0.2.0)\n","Requirement already satisfied: PyOpenGL==3.1.5 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 273)) (3.1.5)\n","Requirement already satisfied: pyparsing==2.4.7 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 274)) (2.4.7)\n","Requirement already satisfied: pyrsistent==0.17.3 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 275)) (0.17.3)\n","Requirement already satisfied: pysndfile==1.3.8 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 276)) (1.3.8)\n","Requirement already satisfied: PySocks==1.7.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 277)) (1.7.1)\n","Requirement already satisfied: pystan==2.19.1.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 278)) (2.19.1.1)\n","Requirement already satisfied: pytest==3.6.4 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 279)) (3.6.4)\n","Requirement already satisfied: python-apt==1.6.5+ubuntu0.3 in /usr/lib/python3/dist-packages (from -r requirements_nlp.txt (line 280)) (1.6.5+ubuntu0.3)\n","Requirement already satisfied: python-chess==0.23.11 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 281)) (0.23.11)\n","Requirement already satisfied: python-dateutil==2.8.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 282)) (2.8.1)\n","Requirement already satisfied: python-louvain==0.14 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 283)) (0.14)\n","Requirement already satisfied: python-slugify==4.0.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 284)) (4.0.1)\n","Requirement already satisfied: python-utils==2.4.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 285)) (2.4.0)\n","Requirement already satisfied: pytz==2018.9 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 286)) (2018.9)\n","Requirement already satisfied: pyviz-comms==0.7.6 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 287)) (0.7.6)\n","Requirement already satisfied: PyWavelets==1.1.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 288)) (1.1.1)\n","Requirement already satisfied: PyYAML==3.13 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 289)) (3.13)\n","Requirement already satisfied: pyzmq==19.0.2 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 290)) (19.0.2)\n","Requirement already satisfied: qtconsole==4.7.7 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 291)) (4.7.7)\n","Requirement already satisfied: QtPy==1.9.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 292)) (1.9.0)\n","Requirement already satisfied: regex==2019.12.20 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 293)) (2019.12.20)\n","Requirement already satisfied: requests==2.23.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 294)) (2.23.0)\n","Requirement already satisfied: requests-oauthlib==1.3.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 295)) (1.3.0)\n","Requirement already satisfied: resampy==0.2.2 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 296)) (0.2.2)\n","Requirement already satisfied: retrying==1.3.3 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 297)) (1.3.3)\n","Requirement already satisfied: rpy2==3.2.7 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 298)) (3.2.7)\n","Requirement already satisfied: rsa==4.6 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 299)) (4.6)\n","Requirement already satisfied: scikit-image==0.16.2 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 300)) (0.16.2)\n","Requirement already satisfied: scikit-learn==0.22.2.post1 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 301)) (0.22.2.post1)\n","Requirement already satisfied: scipy==1.4.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 302)) (1.4.1)\n","Requirement already satisfied: screen-resolution-extra==0.0.0 in /usr/lib/python3/dist-packages (from -r requirements_nlp.txt (line 303)) (0.0.0)\n","Requirement already satisfied: scs==2.1.2 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 304)) (2.1.2)\n","Requirement already satisfied: seaborn==0.11.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 305)) (0.11.0)\n","Requirement already satisfied: Send2Trash==1.5.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 306)) (1.5.0)\n","Requirement already satisfied: setuptools-git==1.2 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 307)) (1.2)\n","Requirement already satisfied: Shapely==1.7.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 308)) (1.7.1)\n","Requirement already satisfied: simplegeneric==0.8.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 309)) (0.8.1)\n","Requirement already satisfied: six==1.15.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 310)) (1.15.0)\n","Requirement already satisfied: sklearn==0.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 311)) (0.0)\n","Requirement already satisfied: sklearn-pandas==1.8.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 312)) (1.8.0)\n","Requirement already satisfied: slugify==0.0.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 313)) (0.0.1)\n","Requirement already satisfied: smart-open==2.2.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 314)) (2.2.0)\n","Requirement already satisfied: snowballstemmer==2.0.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 315)) (2.0.0)\n","Requirement already satisfied: sortedcontainers==2.2.2 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 316)) (2.2.2)\n","Requirement already satisfied: spacy==2.2.4 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 317)) (2.2.4)\n","Requirement already satisfied: Sphinx==1.8.5 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 318)) (1.8.5)\n","Requirement already satisfied: sphinxcontrib-serializinghtml==1.1.4 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 319)) (1.1.4)\n","Requirement already satisfied: sphinxcontrib-websupport==1.2.4 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 320)) (1.2.4)\n","Requirement already satisfied: SQLAlchemy==1.3.19 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 321)) (1.3.19)\n","Requirement already satisfied: sqlparse==0.3.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 322)) (0.3.1)\n","Requirement already satisfied: srsly==1.0.2 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 323)) (1.0.2)\n","Requirement already satisfied: statsmodels==0.10.2 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 324)) (0.10.2)\n","Requirement already satisfied: sympy==1.1.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 325)) (1.1.1)\n","Requirement already satisfied: tables==3.4.4 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 326)) (3.4.4)\n","Requirement already satisfied: tabulate==0.8.7 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 327)) (0.8.7)\n","Requirement already satisfied: tblib==1.7.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 328)) (1.7.0)\n","Requirement already satisfied: tensorboard==2.3.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 329)) (2.3.0)\n","Requirement already satisfied: tensorboard-plugin-wit==1.7.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 330)) (1.7.0)\n","Requirement already satisfied: tensorboardcolab==0.0.22 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 331)) (0.0.22)\n","Requirement already satisfied: tensorflow==2.3.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 332)) (2.3.0)\n","Requirement already satisfied: tensorflow-addons==0.8.3 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 333)) (0.8.3)\n","Requirement already satisfied: tensorflow-datasets==2.1.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 334)) (2.1.0)\n","Requirement already satisfied: tensorflow-estimator==2.3.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 335)) (2.3.0)\n","Requirement already satisfied: tensorflow-gcs-config==2.3.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 336)) (2.3.0)\n","Requirement already satisfied: tensorflow-hub==0.9.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 337)) (0.9.0)\n","Requirement already satisfied: tensorflow-metadata==0.24.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 338)) (0.24.0)\n","Requirement already satisfied: tensorflow-privacy==0.2.2 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 339)) (0.2.2)\n","Requirement already satisfied: tensorflow-probability==0.11.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 340)) (0.11.0)\n","Requirement already satisfied: termcolor==1.1.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 341)) (1.1.0)\n","Requirement already satisfied: terminado==0.9.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 342)) (0.9.1)\n","Requirement already satisfied: testpath==0.4.4 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 343)) (0.4.4)\n","Requirement already satisfied: text-unidecode==1.3 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 344)) (1.3)\n","Requirement already satisfied: textblob==0.15.3 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 345)) (0.15.3)\n","Requirement already satisfied: textgenrnn==1.4.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 346)) (1.4.1)\n","Requirement already satisfied: Theano==1.0.5 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 347)) (1.0.5)\n","Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 348)) (7.4.0)\n","Requirement already satisfied: tifffile==2020.9.3 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 349)) (2020.9.3)\n","Requirement already satisfied: toml==0.10.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 350)) (0.10.1)\n","Requirement already satisfied: toolz==0.11.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 351)) (0.11.1)\n","Requirement already satisfied: torch==1.6.0+cu101 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 352)) (1.6.0+cu101)\n","Requirement already satisfied: torchsummary==1.5.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 353)) (1.5.1)\n","Requirement already satisfied: torchtext==0.3.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 354)) (0.3.1)\n","Requirement already satisfied: torchvision==0.7.0+cu101 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 355)) (0.7.0+cu101)\n","Requirement already satisfied: tornado==5.1.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 356)) (5.1.1)\n","Requirement already satisfied: tqdm==4.41.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 357)) (4.41.1)\n","Requirement already satisfied: traitlets==4.3.3 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 358)) (4.3.3)\n","Requirement already satisfied: tweepy==3.6.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 359)) (3.6.0)\n","Requirement already satisfied: typeguard==2.7.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 360)) (2.7.1)\n","Requirement already satisfied: typing-extensions==3.7.4.3 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 361)) (3.7.4.3)\n","Requirement already satisfied: tzlocal==1.5.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 362)) (1.5.1)\n","Requirement already satisfied: umap-learn==0.4.6 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 363)) (0.4.6)\n","Requirement already satisfied: uritemplate==3.0.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 364)) (3.0.1)\n","Requirement already satisfied: urllib3==1.24.3 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 365)) (1.24.3)\n","Requirement already satisfied: vega-datasets==0.8.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 366)) (0.8.0)\n","Requirement already satisfied: wasabi==0.8.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 367)) (0.8.0)\n","Requirement already satisfied: wcwidth==0.2.5 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 368)) (0.2.5)\n","Requirement already satisfied: webencodings==0.5.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 369)) (0.5.1)\n","Requirement already satisfied: Werkzeug==1.0.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 370)) (1.0.1)\n","Requirement already satisfied: widgetsnbextension==3.5.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 371)) (3.5.1)\n","Requirement already satisfied: wikipedia2vec==0.2.2 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 372)) (0.2.2)\n","Requirement already satisfied: wordcloud==1.5.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 373)) (1.5.0)\n","Requirement already satisfied: wrapt==1.12.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 374)) (1.12.1)\n","Requirement already satisfied: xarray==0.15.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 375)) (0.15.1)\n","Requirement already satisfied: xgboost==0.90 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 376)) (0.90)\n","Requirement already satisfied: xkit==0.0.0 in /usr/lib/python3/dist-packages (from -r requirements_nlp.txt (line 377)) (0.0.0)\n","Requirement already satisfied: xlrd==1.1.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 378)) (1.1.0)\n","Requirement already satisfied: xlwt==1.3.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 379)) (1.3.0)\n","Requirement already satisfied: yellowbrick==0.9.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 380)) (0.9.1)\n","Requirement already satisfied: zict==2.0.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 381)) (2.0.0)\n","Requirement already satisfied: zipp==3.2.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 382)) (3.2.0)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.6/dist-packages (from astunparse==1.6.3->-r requirements_nlp.txt (line 9)) (0.35.1)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from chainer==7.4.0->-r requirements_nlp.txt (line 31)) (50.3.0)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"x1xzfQL5OvRk","executionInfo":{"status":"ok","timestamp":1603917354570,"user_tz":-60,"elapsed":10482,"user":{"displayName":"Maryse Muller","photoUrl":"","userId":"13230828717959946182"}}},"source":["from P6_functions import *"],"execution_count":169,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gvdHBoIuH90C"},"source":["Installations (creating the requirements file)"]},{"cell_type":"code","metadata":{"id":"H1oa9ebJHSOm","executionInfo":{"status":"ok","timestamp":1603917354573,"user_tz":-60,"elapsed":10432,"user":{"displayName":"Maryse Muller","photoUrl":"","userId":"13230828717959946182"}}},"source":["# !pip install gtts\n","# !pip install wikipedia2vec==0.2.2\n","# !pip install category-encoders"],"execution_count":170,"outputs":[]},{"cell_type":"code","metadata":{"id":"gQ-WJqLWFtzu","executionInfo":{"status":"ok","timestamp":1603917354576,"user_tz":-60,"elapsed":10397,"user":{"displayName":"Maryse Muller","photoUrl":"","userId":"13230828717959946182"}}},"source":["# !pip freeze > requirements_nlp.txt"],"execution_count":171,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xBgiuFEdQq78"},"source":["Importation of modules and packages. "]},{"cell_type":"code","metadata":{"id":"oDhE9utOlwJe","executionInfo":{"status":"ok","timestamp":1603917354578,"user_tz":-60,"elapsed":10359,"user":{"displayName":"Maryse Muller","photoUrl":"","userId":"13230828717959946182"}},"outputId":"02447d6b-28ed-4ba9-ba51-0af474f82007","colab":{"base_uri":"https://localhost:8080/","height":104}},"source":["import io\n","\n","import string\n","\n","import pandas as pd\n","import seaborn as sns\n","import numpy as np\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","plt.rcParams['figure.facecolor']='w'\n","\n","import nltk\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","\n","# import warnings\n","# warnings.filterwarnings('ignore')"],"execution_count":172,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":172}]},{"cell_type":"markdown","metadata":{"id":"xNSTytWrQ0De"},"source":["Setting pandas display options."]},{"cell_type":"code","metadata":{"id":"G0rRvyJaWO2h","executionInfo":{"status":"ok","timestamp":1603917354580,"user_tz":-60,"elapsed":10308,"user":{"displayName":"Maryse Muller","photoUrl":"","userId":"13230828717959946182"}}},"source":["dictPdSettings = {'display.max_rows': 500, 'display.width': 100,\n","                  'display.max_colwidth': 100,\n","                  'display.float_format': lambda x: '%.2f' % x}\n","for k,v in dictPdSettings.items():\n","  pd.set_option(k,v)"],"execution_count":173,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"z9APZjLzQ_sp"},"source":["To play audio text-to-speech during execution."]},{"cell_type":"code","metadata":{"id":"vgvmjvZ_Y6-s","executionInfo":{"status":"ok","timestamp":1603917354581,"user_tz":-60,"elapsed":10282,"user":{"displayName":"Maryse Muller","photoUrl":"","userId":"13230828717959946182"}}},"source":["from IPython.display import Audio\n","from gtts import gTTS\n","\n","def speak(text, lang='en'):\n","    with io.BytesIO() as f:\n","        gTTS(text=text, lang=lang).write_to_fp(f)\n","        f.seek(0)\n","        return Audio(f.read(), autoplay=True)"],"execution_count":174,"outputs":[]},{"cell_type":"code","metadata":{"id":"X5b_ibTl83bD","executionInfo":{"status":"ok","timestamp":1603917354583,"user_tz":-60,"elapsed":10235,"user":{"displayName":"Maryse Muller","photoUrl":"","userId":"13230828717959946182"}},"outputId":"a92c80dc-d44f-4b1c-9be8-cb7e1d693216","colab":{"base_uri":"https://localhost:8080/","height":74}},"source":["speak('Packages and modules successfully imported')"],"execution_count":175,"outputs":[{"output_type":"execute_result","data":{"text/html":["\n","                <audio controls=\"controls\" autoplay=\"autoplay\">\n","                    <source src=\"data:audio/wav;base64,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\" type=\"audio/wav\" />\n","                    Your browser does not support the audio element.\n","                </audio>\n","              "],"text/plain":["<IPython.lib.display.Audio object>"]},"metadata":{"tags":[]},"execution_count":175}]},{"cell_type":"markdown","metadata":{"id":"fmDu0rMFGl5A"},"source":["### 0.1 Importing the datasets"]},{"cell_type":"markdown","metadata":{"id":"R_c4YLdXQAzn"},"source":["Data is composed of 9 distinct .csv files we'll load in a dictionnary of dataframes."]},{"cell_type":"code","metadata":{"id":"KhVW-wxr30Ia","executionInfo":{"status":"ok","timestamp":1603917354584,"user_tz":-60,"elapsed":10148,"user":{"displayName":"Maryse Muller","photoUrl":"","userId":"13230828717959946182"}},"outputId":"0ebd74a6-3004-47f2-c026-7ae9dcbfa022","colab":{"base_uri":"https://localhost:8080/","height":52}},"source":["if is_colab==True:\n","    # Importing database from my Drive\n","    print(\"Try to import data files in the notebook from myDrive...\")\n","else:\n","    # Importing database from PC\n","    print(\"Try to import data files in the notebook from PC ('DATA')...\")\n","\n","df = pd.read_csv(\"../DATA/flipkart_com-ecommerce_sample_1050.csv\",\n","                 sep=',', \n","                 index_col = 'uniq_id',\n","                 encoding ='utf-8')\n","\n","print(\"-----> Importation of .csv in the notebook: OK\")"],"execution_count":176,"outputs":[{"output_type":"stream","text":["Try to import data files in the notebook from myDrive...\n","-----> Importation of .csv in the notebook: OK\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"61owD2pedhh5","executionInfo":{"status":"error","timestamp":1603917355480,"user_tz":-60,"elapsed":10999,"user":{"displayName":"Maryse Muller","photoUrl":"","userId":"13230828717959946182"}},"outputId":"b69811e2-eb3c-44bd-e2f8-687518a71bb1","colab":{"base_uri":"https://localhost:8080/","height":310}},"source":["speak('Datasets successfully imported')"],"execution_count":177,"outputs":[{"output_type":"error","ename":"ValueError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-177-092432f34642>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mspeak\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Datasets successfully imported'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-174-7c5a9449b665>\u001b[0m in \u001b[0;36mspeak\u001b[0;34m(text, lang)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mspeak\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'en'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0mgTTS\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_to_fp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseek\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mAudio\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mautoplay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gtts/tts.py\u001b[0m in \u001b[0;36mwrite_to_fp\u001b[0;34m(self, fp)\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0murllib3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisable_warnings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murllib3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInsecureRequestWarning\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m         \u001b[0mprepared_requests\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prepare_requests\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    252\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprepared_requests\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gtts/tts.py\u001b[0m in \u001b[0;36m_prepare_requests\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    192\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m                 \u001b[0;31m# Calculate token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 194\u001b[0;31m                 \u001b[0mpart_tk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalculate_token\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpart\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    195\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRequestException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pragma: no cover\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m                 \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gtts_token/gtts_token.py\u001b[0m in \u001b[0;36mcalculate_token\u001b[0;34m(self, text, seed)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mseed\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m             \u001b[0mseed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_token_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mfirst_seed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msecond_seed\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gtts_token/gtts_token.py\u001b[0m in \u001b[0;36m_get_token_key\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtkk_expr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m             raise ValueError(\n\u001b[0;32m---> 59\u001b[0;31m                 \u001b[0;34m\"Unable to find token seed! Did https://translate.google.com change?\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m             )\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: Unable to find token seed! Did https://translate.google.com change?"]}]},{"cell_type":"markdown","metadata":{"id":"bGmveNrz8Eh_"},"source":["## 1 Data extraction"]},{"cell_type":"markdown","metadata":{"id":"hBfDUUQyzD_v"},"source":["### 1.1 Categories"]},{"cell_type":"markdown","metadata":{"id":"FM9aXq1jUY-k"},"source":["Unfolding categories using the 'product_category_tree' colum"]},{"cell_type":"code","metadata":{"id":"6c7InsBZU6UM","executionInfo":{"status":"aborted","timestamp":1603917355410,"user_tz":-60,"elapsed":10868,"user":{"displayName":"Maryse Muller","photoUrl":"","userId":"13230828717959946182"}}},"source":["# sample checking\n","df['product_category_tree'][743]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MIoFWkVgfRw3","executionInfo":{"status":"aborted","timestamp":1603917355412,"user_tz":-60,"elapsed":10816,"user":{"displayName":"Maryse Muller","photoUrl":"","userId":"13230828717959946182"}}},"source":["# determining the maximum tree depth of categories\n","ser_depth = df['product_category_tree'].apply(lambda x: x.count('>>'))\n","max_depth = ser_depth.max()\n","max_depth"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"J7gmladEPgJq","executionInfo":{"status":"aborted","timestamp":1603917355413,"user_tz":-60,"elapsed":10763,"user":{"displayName":"Maryse Muller","photoUrl":"","userId":"13230828717959946182"}}},"source":["# Converting the strings in 'product_category_tree' column in 6 categ columns\n","\n","def str_cleaning(ind, my_str, name_level_cols):\n","    my_str = my_str.replace(\"[\\\"\", \"\").replace(\"\\\"]\", \"\")\n","    tab_str = my_str.split(\">>\")\n","    size_tab_str = len(tab_str)\n","    tup_str = tuple([tab_str[i].strip() if i<size_tab_str else \"\" \\\n","                     for i in np.arange(max_depth) ])\n","    return tup_str\n","\n","name_level_cols = ['cat_level_'+str(i) for i in np.arange(max_depth)]\n","ser_tuple = df['product_category_tree']\\\n","    .apply(lambda s: str_cleaning(s.index, s, name_level_cols))\n","df_cat_level = pd.DataFrame([[a,'/'.join([a,b]),'/'.join([a,b,c]),\n","                              '/'.join([a,b,c,d]),'/'.join([a,b,c,d,e]),\n","                              '/'.join([a,b,c,d,e,f])] \\\n","                             for a,b,c,d,e,f in ser_tuple.values],\n","                            columns=name_level_cols, index=df.index)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jNUQB7sBlpvt","executionInfo":{"status":"aborted","timestamp":1603917355414,"user_tz":-60,"elapsed":10707,"user":{"displayName":"Maryse Muller","photoUrl":"","userId":"13230828717959946182"}}},"source":["# printing number of categories in each level and a sample\n","display(df_cat_level.nunique(), df_cat_level.sample(3))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ehwO2aIQ0ETe"},"source":["Let's see how much items are in each category"]},{"cell_type":"code","metadata":{"id":"-IBKwHniz9L5","executionInfo":{"status":"aborted","timestamp":1603917355421,"user_tz":-60,"elapsed":10609,"user":{"displayName":"Maryse Muller","photoUrl":"","userId":"13230828717959946182"}}},"source":["fig = plt.figure(figsize=(25,4))\n","for i, col in enumerate(df_cat_level.columns,1):\n","    ax = fig.add_subplot(1,len(df_cat_level.columns), i)\n","    ser = df_cat_level.groupby(col).size().sort_values(ascending=False)\n","    ser[0:20].plot.bar(width=0.75, color='grey', ec='k', ax=ax)\n","    ax.set_title(col+f' ({ser.shape[0]} categories)', fontweight='bold')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"p1HfAME-4QvR"},"source":["The only level that has a balanced set of items is level 0, with 7 categories.\n","Let's rename these 7 categories:"]},{"cell_type":"code","metadata":{"id":"PShh6D7X5TMH","executionInfo":{"status":"aborted","timestamp":1603917355425,"user_tz":-60,"elapsed":10512,"user":{"displayName":"Maryse Muller","photoUrl":"","userId":"13230828717959946182"}}},"source":["df_cat_level['cat_level_0'].unique()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NSwQ4zUa5CQ4","executionInfo":{"status":"aborted","timestamp":1603917355426,"user_tz":-60,"elapsed":10430,"user":{"displayName":"Maryse Muller","photoUrl":"","userId":"13230828717959946182"}}},"source":["df_cat_level['category'] = \\\n","    df_cat_level['cat_level_0'].replace({'Home Furnishing': 'Furnishing',\n","                                        'Baby Care': 'Baby', \n","                                        'Watches': 'Watches',\n","                                        'Home Decor & Festive Needs': 'Decor',\n","                                        'Kitchen & Dining': 'Kitchen',\n","                                        'Beauty and Personal Care': 'Beauty',\n","                                        'Computers': 'Computers'})"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QBcXz5nNz9xp"},"source":["### 1.2 Products descriptions"]},{"cell_type":"code","metadata":{"id":"5oQmXYsvPgC8","executionInfo":{"status":"aborted","timestamp":1603917355427,"user_tz":-60,"elapsed":10361,"user":{"displayName":"Maryse Muller","photoUrl":"","userId":"13230828717959946182"}}},"source":["# extracting only useful data\n","df_desc_cat = pd.concat([df_cat_level['category'],\n","                         df[[\"product_name\", \"description\"]]], axis=1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fhfvTAuR3-4d","executionInfo":{"status":"aborted","timestamp":1603917355430,"user_tz":-60,"elapsed":10251,"user":{"displayName":"Maryse Muller","photoUrl":"","userId":"13230828717959946182"}}},"source":["df_desc_cat.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tadkUfoS7ddV","executionInfo":{"status":"aborted","timestamp":1603917355438,"user_tz":-60,"elapsed":10127,"user":{"displayName":"Maryse Muller","photoUrl":"","userId":"13230828717959946182"}}},"source":["# creation of a corpus of all the descriptions\n","corpus = ' '.join(df_desc_cat['description'].values)\n","print(\"total nb of words in the whole corpus: \", len(corpus.split()))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7fSxU4NRINAy","executionInfo":{"status":"aborted","timestamp":1603917355441,"user_tz":-60,"elapsed":9972,"user":{"displayName":"Maryse Muller","photoUrl":"","userId":"13230828717959946182"}}},"source":["df_desc_cat"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1JiTjU-IIdQx"},"source":["## 2 Optimisation of the text preprocessing and clustering"]},{"cell_type":"markdown","metadata":{"id":"8aQxshTj0M1h"},"source":["### FUNCTIONS"]},{"cell_type":"markdown","metadata":{"id":"582OuQHV-mf6"},"source":["#### tokenize_clean"]},{"cell_type":"code","metadata":{"id":"3Fpz-d444Wyb","executionInfo":{"status":"aborted","timestamp":1603917355442,"user_tz":-60,"elapsed":9848,"user":{"displayName":"Maryse Muller","photoUrl":"","userId":"13230828717959946182"}}},"source":["''' from a sentence, containing words (document):\n","- tokenizes the words if only composed of alphanumerical data,\n","- removes stopwords if list is given (stopwords)\n","- stems the words if stemmer given\n","NB: This pre-processing function can be used to prepare data for Word2Vec\n","'''\n","from nltk.stem.snowball import EnglishStemmer\n","import spacy\n","\n","def tokenize_clean(document, stopwords=None, lemmatizer=None, stemmer=None):\n","    # 1 - tokenizing the words in each description\n","    tokenizer = nltk.RegexpTokenizer(r'[a-z]+')\n","    li_words = tokenizer.tokenize(document)\n","    if stopwords is None: stopwords=[]\n","    # 2 - lemmatizing or stemming\n","    if lemmatizer is not None:\n","        lem_doc = lemmatizer(' '.join(li_words))\n","        li_words = [token.lemma_ for token in lem_doc]\n","    elif stemmer is not None:\n","        li_words = [stemmer.stem(s) for s in li_words]\n","    # 3 - removing stopwords\n","    li_words = [s for s in li_words if s not in stopwords]\n","    # 4 - lower case\n","    li_words = [s.lower() for s in li_words]\n","    return li_words"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"F-bqJZtz-psh"},"source":["#### compute_doc_terms_df"]},{"cell_type":"code","metadata":{"id":"rJ-DoGpRLtV9","executionInfo":{"status":"aborted","timestamp":1603917355443,"user_tz":-60,"elapsed":9770,"user":{"displayName":"Maryse Muller","photoUrl":"","userId":"13230828717959946182"}}},"source":["''' Takes a pd.Series containing the texts of each description\n","applies a preprocessing function if given (stopwords, stemming...)\n","then turn the descriptions in vectors (bow of tf-idf, depending on the avlue of\n"," tfidf_on)\n"," returns document term matrix as a dataframe and the list of new excluded words.\n","'''\n","\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","\n","def compute_doc_terms_df(ser_desc, \n","                         preproc_func=None,\n","                         preproc_func_params=None,\n","                         vec_params = {'min_df': 1},\n","                         tfidf_on=False,\n","                         print_opt=False):\n","\n","    # ---- Apply a stemming of lemmatization prior to vectorization\n","    if preproc_func is not None:\n","        ser_desc = ser_desc.apply(lambda x: preproc_func(x,\n","                                                         **preproc_func_params))\n","        ser_desc = ser_desc.apply(lambda x: ' '.join(x))\n","    else:\n","        ser_desc = ser_desc\n","    \n","    # ---- Vectorization of each of the texts (row)\n","    if tfidf_on:\n","        # TF-IDF matrix\n","        vec = TfidfVectorizer(**vec_params)\n","    else:\n","        # BOW matrix (count)\n","        vec = CountVectorizer(**vec_params)\n","\n","    doc_term = vec.fit_transform(ser_desc)\n","    if print_opt:\n","        print( \"Created %d X %d doc_term matrix\" % (doc_term.shape[0],\n","                                                    doc_term.shape[1]))\n","\n","    # ---- Vocabulary of the document_term matrix\n","    doc_term_voc = vec.get_feature_names()\n","    if print_opt:\n","        print(\"Vocabulary has %d distinct terms\" % len(doc_term_voc))\n","\n","    # ---- Get the list of the new stop-words\n","    new_sw = vec.stop_words_\n","    if print_opt:\n","        print(\"Old stop-words list has %d entries\" % len(sw) )\n","        print(\"New stop-words list has %d entries\" % len(new_sw))\n","\n","    doc_term_df = pd.DataFrame(doc_term.todense(),\n","                index=ser_desc.index, # each item\n","                columns=doc_term_voc) # each word\n","\n","    # document term matrix as a dataframe and the list of new excluded words\n","    return doc_term_df, new_sw\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NPB746p6-s49"},"source":["#### CustNLPTransformer"]},{"cell_type":"code","metadata":{"id":"VexwgKSz-MNx","executionInfo":{"status":"aborted","timestamp":1603917355445,"user_tz":-60,"elapsed":9700,"user":{"displayName":"Maryse Muller","photoUrl":"","userId":"13230828717959946182"}}},"source":["\n","''' Builds a customizable NLP column_transformer which parameters\n","can be optimized in a GridSearchClust\n","'''\n","from sklearn.base import BaseEstimator\n","from sklearn.compose import ColumnTransformer\n","from sklearn.pipeline import Pipeline\n","from sklearn.preprocessing import *\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","import numpy as np\n","import pandas as pd\n","\n","\n","class CustNLPTransformer(BaseEstimator):\n","\n","    def __init__(self, stopwords=None, stemmer=None, lemmatizer=None,\n","                 min_df=0, max_df=10000, max_features=10000, tfidf_on=False,\n","                 ngram_range=(1,1), binary=False):\n","        self.stopwords = stopwords\n","        self.lemmatizer = lemmatizer\n","        self.stemmer = stemmer\n","        self.min_df = min_df\n","        self.max_df = max_df\n","        self.max_features = max_features\n","        self.tfidf_on = tfidf_on\n","        self.ngram_range = ngram_range\n","        self.binary = binary\n","        self.preproc_func_params={'stopwords': self.stopwords,\n","                                  'lemmatizer': self.lemmatizer,\n","                                  'stemmer': self.stemmer}\n","        self.vec_params = {'min_df': self.min_df,\n","                           'max_df': self.max_df,\n","                           'max_features': self.max_features,\n","                           'ngram_range': self.ngram_range,\n","                           'binary': self.binary}\n","\n","    def __tokenize_clean(self, document, stopwords, lemmatizer, stemmer):\n","        # 1 - tokenizing the words in each description\n","        tokenizer = nltk.RegexpTokenizer(r'[a-z]+')\n","        li_words = tokenizer.tokenize(document)\n","        if stopwords is None: stopwords=[]\n","        # 2 - lemmatizing or stemming\n","        if lemmatizer is not None:\n","            lem_doc = lemmatizer(' '.join(li_words))\n","            li_words = [token.lemma_ for token in lem_doc]\n","        elif stemmer is not None:\n","            li_words = [stemmer.stem(s) for s in li_words]\n","        # 3 - removing stopwords\n","        li_words = [s for s in li_words if s not in stopwords]\n","        # 4 - lower case\n","        li_words = [s.lower() for s in li_words]\n","        return li_words\n","\n","    # \"private\" method to be used to apply transformation and get a df\n","    def __compute_doc_terms_df(self, ser_desc, preproc_func,\n","                             preproc_func_params, vec_params, tfidf_on):\n","        # ---- Apply a stemming or lemmatization prior to vectorization\n","        if preproc_func is not None:\n","            ser_desc = ser_desc.apply(lambda x: \\\n","                                      preproc_func(x, **preproc_func_params))\n","            ser_desc = ser_desc.apply(lambda x: ' '.join(x))\n","        else:\n","            ser_desc = ser_desc\n","        # ---- Vectorization of each of the texts (row)\n","        if tfidf_on:\n","            # TF-IDF matrix\n","            vec = TfidfVectorizer(**vec_params)\n","        else:\n","            # BOW matrix (count)\n","            vec = CountVectorizer(**vec_params)\n","        doc_term = vec.fit_transform(ser_desc)\n","        # ---- Vocabulary of the document_term matrix\n","        doc_term_voc = vec.get_feature_names()\n","        # ---- Get the list of the new stop-words\n","        new_sw = vec.stop_words_\n","        doc_term_df = pd.DataFrame(doc_term.todense(),\n","                                   index=ser_desc.index, # each item\n","                                   columns=doc_term_voc) # each word\n","        # document term matrix as a dataframe and the list of new excluded words\n","        return doc_term_df\n","\n","    def fit(self, X, y=None):\n","        # nothing to fit\n","        return self\n","\n","    def transform(self, X, y=None):  # to get a dataframe\n","        df_trans = \\\n","            self.__compute_doc_terms_df(\n","                ser_desc=X,\n","                preproc_func=self.__tokenize_clean,\n","                preproc_func_params=self.preproc_func_params,\n","                vec_params=self.vec_params,\n","                tfidf_on=self.tfidf_on\n","                                   )\n","        return df_trans\n","\n","    def fit_transform(self, X, y=None):\n","        return self.transform(X, y)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pwVgwbtC-ybs"},"source":["#### GridSearchClust"]},{"cell_type":"code","metadata":{"id":"6tqC1ZvDZidO","executionInfo":{"status":"aborted","timestamp":1603917355446,"user_tz":-60,"elapsed":9649,"user":{"displayName":"Maryse Muller","photoUrl":"","userId":"13230828717959946182"}}},"source":["'''\n","Class to optimize clustering score.\n","Instantiate with a clusterer (estimator), a grid parameter (param_grid)\n","and a scoring function or a dict of scores (scoring) to be translated\n","in actual scores (see the compute_score)\n","'''\n","\n","import numpy as np\n","from sklearn.base import BaseEstimator, TransformerMixin\n","from sklearn.model_selection import ParameterGrid\n","from collections import defaultdict\n","from sklearn.metrics import silhouette_score, calinski_harabasz_score,\\\n"," davies_bouldin_score, adjusted_mutual_info_score, adjusted_rand_score,\\\n"," homogeneity_score, completeness_score\n","\n","class GridSearchClust(BaseEstimator, TransformerMixin):\n","\n","    def __init__(self, estimator, param_grid_estim, param_grid_preproc=None,\n","                 scoring=None, scoring_true_lab=None, refit='silh',\n","                 greater_is_better=True):\n","\n","        # Get the parameters\n","        self.estimator = estimator\n","        self.param_grid_estim = param_grid_estim\n","        self.param_grid_preproc = param_grid_preproc\n","        self.scoring = scoring\n","        self.scoring_true_lab = scoring_true_lab\n","        self.refit = refit\n","        self.greater_is_better = greater_is_better\n","\n","    def __compute_score(self, X, clust_lab, n_score):\n","\n","        dict_scores = {\n","            # Scores related to the clusters labels found by our estimator\n","               'silh': silhouette_score(X, clust_lab),\n","               'cal-har': calinski_harabasz_score(X, clust_lab),\n","               'dav_bould': davies_bouldin_score(X, clust_lab),\n","            # Scores comparing true labels and clusters found by our estimator\n","               'ami': adjusted_mutual_info_score(self.scoring_true_lab, clust_lab),\n","               'ari': adjusted_rand_score(self.scoring_true_lab, clust_lab),\n","               'homog': homogeneity_score(self.scoring_true_lab, clust_lab),\n","               'complet': completeness_score(self.scoring_true_lab, clust_lab)\n","               }\n","        return dict_scores[n_score]\n","\n","\n","    def fit(self, X, verbose=False):\n","\n","        # Initialize the dict of results\n","        self.results_ = {\"scores\": {},\n","                         \"params\": [],\n","                         \"estimators\": [],\n","                        #  \"fit_times\": [],\n","                         \"refit_score\": []}\n","\n","        # Iterate upon all combinations of parameters\n","        estim_score = defaultdict(list)\n","        for param in ParameterGrid(self.param_grid_estim):\n","\n","            # Change the parameters of the estimator\n","            self.estimator = self.estimator.set_params(**param)\n","\n","            # Fit the model\n","            self.estimator.fit(X)\n","\n","            # If the estimator is a pipe, compute the first steps separately\n","            if hasattr(self.estimator, 'steps'): # if estimator is a pipeline\n","                pipe_wo_last_estim = Pipeline(self.estimator.steps[0:-1])\n","                X_trans = pipe_wo_last_estim.fit_transform(X)\n","\n","            # Compute the labels\n","            labels = self.estimator.predict(X)\n","\n","            # # Measure training time while fitting the model on the data\n","            # time_train = %timeit -n1 -r1 -o -q self.estimator.fit(X)\n","            # time_train = time_train.average\n","\n","            # Compute the refit score\n","            try:\n","                if hasattr(self.estimator, 'steps'): # if estimator is a pipeline\n","                    refit_score = self.__compute_score(X_trans, labels, self.refit)\n","                else:\n","                    refit_score = self.__compute_score(X, labels, self.refit)\n","            except:\n","                print('ERREUR calcul refit_score: is scoring_true_lab correctly set ?')\n","                refit_score = np.nan\n","            \n","            # Other scores (scoring)\n","            if not self.scoring:  # if scoring parameter is/are not defined\n","                estim_score['score'] = {'default_score': self.estimator.score(X_trans, labels)} # default score\n","            else:  # If scoring parameter is/are defined\n","                if type(self.scoring) != list:\n","                    self.scoring = [self.scoring]\n","                else:\n","                    # looping over each score in the scoring list\n","                    for n_sco in self.scoring:\n","                        try:\n","                            if hasattr(self.estimator, 'steps'): # if estimator is a pipeline\n","                                estim_score[n_sco] = estim_score[n_sco] + \\\n","                                    [self.__compute_score(X_trans, labels, n_sco)]\n","                            else:\n","                                estim_score[n_sco] = estim_score[n_sco] + \\\n","                                    [self.__compute_score(X, labels, n_sco)]\n","                        except:\n","                            estim_score[n_sco] = estim_score[n_sco] + [np.nan]\n","                            print(\"ERROR: scores computation doesn't work\")\n","            if verbose: print(estim_score)\n","\n","            # saving results, parameters and models in a dict\n","            self.results_[\"refit_score\"].append(refit_score)  # refit score\n","            self.results_[\"params\"].append(param)  # parameters\n","            self.results_[\"estimators\"].append(self.estimator)  # trained models\n","            # self.results_[\"fit_times\"].append(time_train)  # training time\n","\n","        self.results_[\"scores\"] = dict(estim_score)  # dict of lists of scores\n","        self.results_[\"refit_score\"] = np.array(self.results_[\"refit_score\"])\n","  \n","        # Selecting best model based on the refit_score\n","        # -----------------------------------\n","        # initialisation\n","        best_estim_index, best_score = None, None  \n","        # iterating over scores\n","        for index, score in enumerate(self.results_[\"refit_score\"]):\n","\n","            # initialisation\n","            if not best_score:\n","                best_score = score\n","                best_estim_index = index\n","\n","            # if score is better than current best_score\n","            cond = score > best_score if self.greater_is_better\\\n","                                                 else score < best_score\n","            if cond:\n","                    # update the current best_score and current best_estim_index\n","                    best_score = score\n","                    best_estim_index = index\n","        \n","        # Update attributes of the instance\n","        self.best_score_ = self.results_[\"refit_score\"][best_estim_index]\n","        self.best_params_ = self.results_[\"params\"][best_estim_index]\n","        self.best_estimator_ = self.results_[\"estimators\"][best_estim_index]\n","        self.best_index_ = best_estim_index\n","        # self.refit_time_ = self.results_[\"fit_times\"][best_estim_index]\n","\n","        # refit the best model\n","        self.best_estimator_.fit(X)\n","        \n","        return self\n","\n","    def predict(self, X):\n","\n","        # use the .predict method of the estimator on the best model\n","        return self.best_estim.predict(X)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"p6M6b-8jNua_"},"source":["#### filters_gsclust_results"]},{"cell_type":"code","metadata":{"id":"K9JH-O3lNt9e","executionInfo":{"status":"aborted","timestamp":1603917355447,"user_tz":-60,"elapsed":9591,"user":{"displayName":"Maryse Muller","photoUrl":"","userId":"13230828717959946182"}}},"source":["''' Takes a GridSearchClust object and the name of one parameter of the\n","estimator (or of the pipeline) and isolate the influence of this parameter\n","on all the scores available in the scv (scoring)\n","-> returns a dictionary of the best other fixed parameters\n","and a dataframe of the scores depending on the chosen parameter and a \n","filtered results_ dataframe that can be used\n","in the 'plot_scv_multi_scores' function '''\n","\n","def filters_gsclust_results(gsc, param, return_df_res=False):\n","\n","    gsc_res = gsc.results_\n","    # Generate a dataframe of all the models tested, their scores and parameters\n","    df_gsc = pd.DataFrame()\n","    for k, v in gsc_res.items():\n","        if type(v) == dict: # dict de listes : scores\n","            df_ = pd.DataFrame(v)\n","        elif type(v) == list:\n","            if type(v[0]) == dict: # liste de dicts : params\n","                df_ = pd.DataFrame(v)\n","                li_params = df_.columns\n","            else: # liste d'objets (estimators) ou de nombres (refit_score)\n","                df_ = pd.DataFrame(v, columns=[k])\n","        else:\n","            col_names = [str(k)]\n","            df_ = pd.DataFrame(v, columns=[k])\n","        df_gsc = pd.concat([df_gsc, df_], axis=1)\n","    df_gsc_transl = object_none_translater(df_gsc)\n","\n","    # selects in the data frame the best params\n","    best_params = gsc.best_params_.copy() # dict of the best params\n","    # translation of all the non numeric values into strings (including None)\n","    best_params_transl = object_none_translater(best_params) \n","    del best_params_transl[param] # remove the parameter that we want to plot\n","\n","    # filters in the result dataframe only optimized results except for 'param'\n","    mask = np.full((df_gsc_transl.shape[0],), True)\n","    for k,v in best_params_transl.items():\n","        mask = mask & (df_gsc_transl[k]==v)\n","    df_gsc_filt = df_gsc.loc[mask]\n","    li_scores = gsc.get_params()['scoring']          \n","    df_sel_scores = df_gsc_filt[li_scores+[param]].set_index(param)\n","    df_res = df_gsc_transl\n","    if return_df_res:\n","        return best_params, df_sel_scores, df_gsc_filt, df_res\n","    else:\n","        return best_params, df_sel_scores, df_gsc_filt"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Izmu6JFSL4fM"},"source":["#### plot_gsc_multiscore"]},{"cell_type":"code","metadata":{"id":"uhVrMNGgL3Wv","executionInfo":{"status":"aborted","timestamp":1603917355449,"user_tz":-60,"elapsed":9514,"user":{"displayName":"Maryse Muller","photoUrl":"","userId":"13230828717959946182"}}},"source":["''' Plots a selection of scores (scores) or all the scores (scores=None)\n","obtained during the GridSearchClust as a collection of line graphs.\n","The other parameters (not plotted) are the parameters of the best estimator\n","(found by gridsearch). They're the same for all the line plot (contrary to\n","the 'plot_2Dgsclust_param_opt where other params may differ for each cell).\n","'''\n","\n","def plot_gsc_multi_scores(gsc, param, title = None, x_log=False,\n","                          loc='best', figsize = (12, 4), scores=None):\n","\n","    best_params, df_sel_scores, _ = filters_gsclust_results(gsc,param)\n","    results = df_sel_scores\n","    \n","    scoring = gsc.scoring if scores is None else scores\n","\n","    fig, axs = plt.subplots(1,len(scoring))\n","    fig.set_size_inches(figsize)\n","\n","    li_colors = ['b', 'r', 'g', 'purple', 'orange', 'brown', 'grey']\n","    if len(axs)==1 : axs = [axs]\n","\n","    # Get the regular np array from the MaskedArray\n","        \n","    X_axis = np.array(results.index, dtype='float')\n","\n","    for scorer, color, ax in zip(sorted(scoring), li_colors[:len(scoring)], axs):\n","        score = df_sel_scores[scorer].values\n","        \n","        df_ = pd.DataFrame({'param': X_axis,\n","                            'score': score,\n","                            # 'std': None,\n","                            }).sort_values(by='param')\n","        # ax.fill_between(df_['param'],\n","        #                 df_['score'] - df_['std'],\n","        #                 df_['score'] + df_['std'],\n","        #                 alpha=0.1, color=color)\n","        ax.plot(df_['param'], df_['score'], '-', marker='o', markersize=3,\n","            color=color, alpha=1)\n","        if x_log: ax.set_xscale('log')\n","        ax.set_title(scorer)\n","\n","        y_min, y_max = ax.get_ylim()\n","        \n","        # Plot a dotted vertical line at the best score for that scorer marked by x\n","        best_score = results.loc[best_params[param], scorer]\n","        ax.plot([best_params[param], ] * 2, [y_min - abs(y_min)*0.1, best_score],\n","            linestyle='dotted', color=color, marker='x', markeredgewidth=3, ms=8)\n","        ax.set_ylim(y_min, y_max)\n","        ax.set_xlabel(param)\n","        ax.set_ylabel(\"Score\")\n","        # ax.legend(loc=loc)\n","\n","        # Annotate the best score for that scorer\n","        len_str = len(\"{:.2f}\".format(best_score))\n","        if best_params[param] < np.mean(X_axis):\n","            x_pos = best_params[param]*(1+0.015*len_str)\n","        else:\n","            x_pos = best_params[param]*(1-0.015*len_str)\n","        y_pos = best_score*1+(y_max-y_min)*0.05\n","        ax.annotate(\"{:0.2f}\".format(best_score), \n","                    (x_pos, y_pos),\n","                    color = color)  \n","    if title is not None:\n","        fig.suptitle(title, fontsize=16, fontweight='bold')\n","        plt.tight_layout(rect=(0,0,1,0.92))\n","    else:\n","        plt.tight_layout()\n","    plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uo4z8ZuhwJ8R"},"source":["#### plot_2D_gsclust_param_opt"]},{"cell_type":"code","metadata":{"id":"MameEbdLwJDs","executionInfo":{"status":"aborted","timestamp":1603917355450,"user_tz":-60,"elapsed":9482,"user":{"displayName":"Maryse Muller","photoUrl":"","userId":"13230828717959946182"}}},"source":["''' Takes a GridSearchClust object and plots a heatmap of a chosen score (score)\n","against 2 chosen parameters.\n","NB: the score displayed for each cell is the one for the best other parameters.\n","'''\n","\n","def plot_2D_gsclust_param_opt(gsc, params=None, score=None,\n","                           title=None, shorten_label=7, ax=None):\n","\n","    ax = plt.subplot(1,1,1) if ax is None else ax\n","\n","    score = 'refit_score' if score is None else score\n","\n","    params_gsc = params\n","\n","    _, _, _, df_res = filters_gsclust_results(gsc, params_gsc[0],\n","                                              return_df_res=True)\n","    max_scores = df_res.groupby(params_gsc).agg(lambda x: max(x))[score]\n","    sns.heatmap(max_scores.unstack(), annot=True, fmt='.4g', ax=ax)\n","\n","    if shorten_label != False:\n","        thr = int(shorten_label)\n","        lab_x = [item.get_text() for item in ax.get_xticklabels()]\n","        short_lab_x = [s[:thr]+'...'+s[-thr:] if len(s)>thr else s for s in lab_x]\n","        ax.axes.set_xticklabels(short_lab_x)\n","        lab_y = [item.get_text() for item in ax.get_yticklabels()]\n","        short_lab_y = [s[:thr]+'...'+s[-thr:] if len(s)>thr else s for s in lab_y]\n","        ax.axes.set_yticklabels(short_lab_y)\n","\n","    title = score if title is None else title\n","    ax.set_title(title)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0KfImq2uCg5I"},"source":["#### TopicsModeler"]},{"cell_type":"code","metadata":{"id":"X6JRISLWCgcA","executionInfo":{"status":"aborted","timestamp":1603917355451,"user_tz":-60,"elapsed":9453,"user":{"displayName":"Maryse Muller","photoUrl":"","userId":"13230828717959946182"}}},"source":["''' Builds a topics modeler which parameters (model, number of topics)\n","can be optimized in a GridSearchClust.\n",".transform: returns the DOCUMENTS/TOPICS matrix\n",".predict: returns the list of the most probable topic for each document\n","NB: takes a dataframe as X.\n","'''\n","from sklearn.base import BaseEstimator\n","from sklearn.compose import ColumnTransformer\n","from sklearn.pipeline import Pipeline\n","from sklearn.preprocessing import *\n","from sklearn.decomposition import NMF\n","from sklearn.decomposition import LatentDirichletAllocation as LDA\n","from sklearn.decomposition import TruncatedSVD\n","import numpy as np\n","import pandas as pd\n","\n","class TopicsModeler(BaseEstimator):\n","\n","\n","    def __init__(self, n_model='nmf', n_components=7, random_state=None):#, model_params):\n","\n","        self.n_model = n_model\n","        self.n_components = n_components\n","        self.random_state = random_state\n","        # self.model_params = model_param\n","\n","        # Model name -> object\n","        self.dict_models = {'lsa': TruncatedSVD(),\n","                            'nmf': NMF(init=\"nndsvd\"),\n","                            'lda': LDA()}\n","\n","        # Instantiate the model\n","        try:\n","            self.model = self.dict_models[self.n_model]#.set_params(*self.model_params)\n","        except:\n","            print(\"ERROR: unknown topics modeliser. \\n\\\n","Please, choose between 'nmf', 'lda' and 'lsa'\")\n","\n","    def fit(self, X, y=None):\n","\n","        # Re-Instantiate the model\n","        try:\n","            self.model = self.dict_models[self.n_model]#.set_params(*self.model_params)\n","        except:\n","            print(\"ERROR: unknown topics modeliser. \\n\\\n","Please, choose between 'nmf', 'lda' and 'lsa'\")\n","\n","        # Set the parameters\n","        self.model.set_params(n_components = self.n_components,\n","                              random_state = self.random_state)\n","\n","        # Fit the model\n","        self.model.fit(X)\n","\n","        return self\n","\n","    def __compute_DOC_TOP_matrix(self, X, y=None): # DOCUMENTS/TOPICS Matrix\n","    # actualization of n_components\n","        self.n_components = self.model.transform(X.values).shape[1]\n","        self.W = pd.DataFrame(self.model.transform(X.values),\n","                              index=X.index, # documents\n","                              columns=['topic_'+str(i)\\\n","                                       for i in range(1,self.n_components+1)]) # topics\n","\n","    def __compute_TOP_WORDS_matrix(self, X, y=None): # TOPICS/WORDS Matrix\n","\n","        self.H = pd.DataFrame(self.model.components_, \n","                              index=['topic_'+str(i)\\\n","                                     for i in range(1,self.n_components+1)], # topics\n","                              columns=X.columns) # words\n","\n","    def transform(self, X, y=None):  # to get the df of the DOC/TOPICS matrix\n","\n","        self.__compute_DOC_TOP_matrix(X)\n","        self.__compute_TOP_WORDS_matrix(X)\n","\n","        # Converting topics scores to best cluster label (higher val column)\n","        ser_res = self.W.idxmax(1)\n","\n","        return self.W\n","\n","    def predict(self, X, y=None):  # to get a ser of the best label\n","\n","        self.__compute_DOC_TOP_matrix(X)\n","        self.__compute_TOP_WORDS_matrix(X)\n","\n","        # Converting topics scores to best cluster label (higher val column)\n","        ser_res = self.W.idxmax(1)\n","\n","        return ser_res\n","\n","    def fit_transform(self, X, y=None):\n","        self.fit(X, y)\n","        return self.transform(X, y)\n","\n","    def fit_predict(self, X, y=None):\n","        self.fit(X, y)\n","        return self.predict(X, y)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Gu-0OVgo-3-9"},"source":["#### --- others ---"]},{"cell_type":"code","metadata":{"id":"dtO5xWRVINGG","executionInfo":{"status":"aborted","timestamp":1603917355454,"user_tz":-60,"elapsed":9430,"user":{"displayName":"Maryse Muller","photoUrl":"","userId":"13230828717959946182"}}},"source":["from sklearn.cluster import KMeans\n","\n","def clustering_doc_matrix(doc_matrix_df, name, n_clusters=7):\n","    # Creating the Kmeans model\n","    km = KMeans(n_clusters = n_clusters)\n","    # Fitting the Kmeans model\n","    km.fit(doc_matrix_df)\n","    ser = pd.Series(km.labels_,\n","                    index = doc_matrix_df.index,\n","                    name = name)\n","    return ser"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"etrXqt-kNjtV","executionInfo":{"status":"aborted","timestamp":1603917355457,"user_tz":-60,"elapsed":9409,"user":{"displayName":"Maryse Muller","photoUrl":"","userId":"13230828717959946182"}}},"source":["from sklearn.decomposition import NMF\n","from sklearn.decomposition import LatentDirichletAllocation as LDA\n","from sklearn.decomposition import TruncatedSVD\n","\n","def topicsmodeler_doc_matrix(doc_matrix_df, n_model, name, n_components=7):\n","\n","    dict_models = {'lsa': TruncatedSVD(),\n","                   'nmf': NMF(init=\"nndsvd\"),\n","                   'lda': LDA()}\n","    model = dict_models[n_model]\n","\n","    # Instantiation the topic modeler\n","    model.set_params(n_components = n_components)\n","\n","    # Fitting the the topic modeler\n","    model.fit(doc_matrix_df)\n","\n","    # DOCUMENTS/TOPICS Matrix\n","    W = pd.DataFrame(model.fit_transform(doc_matrix_df.values),\n","                     index=doc_matrix_df.index, # documents\n","                     columns=['topic_'+str(i)\\\n","                              for i in range(1,n_components+1)]) # topics\n","\n","    # TOPICS/WORDS Matrix\n","    H = pd.DataFrame(model.components_,\n","                     index=['topic_'+str(i)\\\n","                            for i in range(1,n_components+1)], # topics\n","                     columns=doc_matrix_df.columns) # words\n","\n","    # Converting topics scores to best cluster label (higher val column)\n","    ser_res = pd.Series(W.idxmax(1),\n","                        index = W.index,\n","                        name = name)\n","\n","    return ser_res"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ORq0iho1ZwsB","executionInfo":{"status":"aborted","timestamp":1603917355458,"user_tz":-60,"elapsed":9360,"user":{"displayName":"Maryse Muller","photoUrl":"","userId":"13230828717959946182"}}},"source":["''' Transformer that translates all the non numeric values into strings\n","(including None) in a dict or a dataframe (column wise)\n","'''\n","import numbers\n","# si contient des None, alors convertir tous les objects en string\n","\n","def object_none_translater(dict_or_df):\n","    if type(dict_or_df) == dict:\n","        # Change any non numeric value to string\n","        dict_or_df_transl = {k: v if isinstance(v, numbers.Number) else str(v)\\\n","                             for k,v in dict_or_df.items()}\n","    elif type(dict_or_df) == pd.core.frame.DataFrame:\n","        # Change None to str (so the type of any None-containing col becomes 'object')\n","        dict_or_df_transl = dict_or_df.fillna('None')\n","        # Convert the content of all object columns to strings\n","        cols = dict_or_df_transl.select_dtypes('object').columns\n","        dict_or_df_transl[cols] = dict_or_df_transl[cols].applymap(lambda x: str(x))\n","    else:\n","        print(\"ERROR: you passed an object to 'object_none_translater'\\\n"," that is neither a dict nor a pd.DataFrame\")\n","    return dict_or_df_transl\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"acyh8guQ0QU5"},"source":["## 3 One parameter optimization loop (example on max_features of CountVectorizer)"]},{"cell_type":"code","metadata":{"id":"qmqTm9qZDU0b","executionInfo":{"status":"aborted","timestamp":1603917355460,"user_tz":-60,"elapsed":9326,"user":{"displayName":"Maryse Muller","photoUrl":"","userId":"13230828717959946182"}}},"source":["df_res_clust = pd.DataFrame()\n","df_res_clust['categories'] = df_desc_cat['category']"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DGgBOiqNNCYR"},"source":["Descriptions pre-processing"]},{"cell_type":"code","metadata":{"id":"qWgSqvYqSApq","executionInfo":{"status":"aborted","timestamp":1603917355462,"user_tz":-60,"elapsed":9274,"user":{"displayName":"Maryse Muller","photoUrl":"","userId":"13230828717959946182"}}},"source":["# Stopwords\n","english_sw = nltk.corpus.stopwords.words('english')\n","single_let_sw = list(string.ascii_lowercase)\n","sw = list(set(english_sw + single_let_sw))\n","len(sw)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QK2QYLS74Wtk","executionInfo":{"status":"aborted","timestamp":1603917355464,"user_tz":-60,"elapsed":9191,"user":{"displayName":"Maryse Muller","photoUrl":"","userId":"13230828717959946182"}}},"source":["# Stemmer or lemmatizer\n","from nltk.stem.snowball import EnglishStemmer\n","import spacy\n","\n","stemmer = EnglishStemmer()\n","lemmatizer = spacy.load('en', disable=['parser', 'ner'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5iOMtCtHMAQf","executionInfo":{"status":"aborted","timestamp":1603917355465,"user_tz":-60,"elapsed":9131,"user":{"displayName":"Maryse Muller","photoUrl":"","userId":"13230828717959946182"}}},"source":["# Vectorization of the descriptions prior to applying NMF\n","\n","doc_matrix_df, _ = \\\n","    compute_doc_terms_df(df_desc_cat['description'],\n","                         preproc_func= tokenize_clean,\n","                         preproc_func_params = {'stopwords': sw,\n","                                                'stemmer': None,\n","                                                'lemmatizer': lemmatizer},\n","                         vec_params = {'min_df': 5, # min nb of descriptions that must contain the word\n","                                       'max_features':500}, # max nb of words to keep among the most used\n","                         tfidf_on=False)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SJR2wmR7M1V2"},"source":["#### KMeans Clustering\n","\n","Loop optimisation of the max_feature parameter on ARI"]},{"cell_type":"code","metadata":{"id":"9kVtgf_jXeMl","executionInfo":{"status":"aborted","timestamp":1603917355466,"user_tz":-60,"elapsed":9087,"user":{"displayName":"Maryse Muller","photoUrl":"","userId":"13230828717959946182"}}},"source":["# Getting the labels of clustering for each set of param (1 param)\n","\n","max_features_list = [30, 70, 100, 150, 250, 500, 1000]\n","\n","for max_feat in max_features_list:\n","    doc_matrix_df, _ = \\\n","        compute_doc_terms_df(df_desc_cat['description'],\n","                         preproc_func= tokenize_clean,\n","                         preproc_func_params = {'stopwords': sw,\n","                                                'stemmer': None,\n","                                                'lemmatizer': lemmatizer},\n","                         vec_params = {'min_df': 5, # min nb of descriptions that must contain the word\n","                                       'max_features':max_feat}, # max nb of words to keep among the most used\n","                         tfidf_on=False)\n","    # Appending the best results of the Kmeans clustering\n","    df_res_clust = pd.concat([df_res_clust,\n","                              clustering_doc_matrix(doc_matrix_df,\n","                                                    name='KM_'+str(max_feat))],\n","                             axis=1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lwsdaIlXINQQ","executionInfo":{"status":"aborted","timestamp":1603917355468,"user_tz":-60,"elapsed":9033,"user":{"displayName":"Maryse Muller","photoUrl":"","userId":"13230828717959946182"}}},"source":["df_res_clust.sample(5)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wG78-TpyqFSx","executionInfo":{"status":"aborted","timestamp":1603917355470,"user_tz":-60,"elapsed":8916,"user":{"displayName":"Maryse Muller","photoUrl":"","userId":"13230828717959946182"}}},"source":["# Comparison of clusters labels with true categories\n","\n","from sklearn.metrics import adjusted_rand_score\n","\n","ser_ari_pairs_models = ARI_column_pairs(df_res_clust, first_vs_others=True,\n","                                        print_opt=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Wn977U9eeZ2G","executionInfo":{"status":"aborted","timestamp":1603917355471,"user_tz":-60,"elapsed":8836,"user":{"displayName":"Maryse Muller","photoUrl":"","userId":"13230828717959946182"}}},"source":["# Plotting the results\n","\n","fig = plt.figure(figsize=(2,3))\n","ser_ari_pairs_models.plot.bar(width=0.7, color='grey', ec='k')\n","plt.ylabel('ARI score')\n","# plt.title('ARI score comparing the cluster\\nlabel prediction of pairs of models')\n","# plt.gca().set(ylim=(0.85,1))\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"C1vpK1ghRePk"},"source":["#### Topics Modeler Clustering\n","\n","Loop optimisation of the max_feature parameter on ARI"]},{"cell_type":"code","metadata":{"id":"ZRhdlpsKReny","executionInfo":{"status":"aborted","timestamp":1603917355472,"user_tz":-60,"elapsed":8768,"user":{"displayName":"Maryse Muller","photoUrl":"","userId":"13230828717959946182"}}},"source":["# Getting the labels of clustering for each set of param (1 param)\n","\n","max_features_list = [30, 70, 100, 150, 250, 500, 1000]\n","\n","for max_feat in max_features_list:\n","    doc_matrix_df, _ = \\\n","        compute_doc_terms_df(df_desc_cat['description'],\n","                         preproc_func= tokenize_clean,\n","                         preproc_func_params = {'stopwords': sw,\n","                                                'stemmer': None,\n","                                                'lemmatizer': lemmatizer},\n","                         vec_params = {'min_df': 5, # min nb of descriptions that must contain the word\n","                                       'max_features':max_feat}, # max nb of words to keep among the most used\n","                         tfidf_on=False)\n","    # Appending the best results of the Topics Modeler clustering\n","    df_res_clust = pd.concat([df_res_clust,\n","                              topicsmodeler_doc_matrix(doc_matrix_df,\n","                                                         n_model='nmf',\n","                                                         name='TM_'+str(max_feat),\n","                                                         n_components=7)],\n","                             axis=1)\n","                            #   clustering_doc_matrix(doc_matrix_df,\n","                            #                         str(max_feat))], axis=1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8ioX-zGoRen2","executionInfo":{"status":"aborted","timestamp":1603917355473,"user_tz":-60,"elapsed":8704,"user":{"displayName":"Maryse Muller","photoUrl":"","userId":"13230828717959946182"}}},"source":["df_res_clust.sample(5)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4H2JTM5eRen5","executionInfo":{"status":"aborted","timestamp":1603917355475,"user_tz":-60,"elapsed":8640,"user":{"displayName":"Maryse Muller","photoUrl":"","userId":"13230828717959946182"}}},"source":["# Comparison of clusters labels with true categories\n","\n","from sklearn.metrics import adjusted_rand_score\n","\n","ser_ari_pairs_models = ARI_column_pairs(df_res_clust, first_vs_others=True,\n","                                        print_opt=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vowNZAZPRen8","executionInfo":{"status":"aborted","timestamp":1603917355476,"user_tz":-60,"elapsed":8542,"user":{"displayName":"Maryse Muller","photoUrl":"","userId":"13230828717959946182"}}},"source":["# Plotting the results\n","\n","fig = plt.figure(figsize=(3,3))\n","ser_ari_pairs_models.plot.bar(width=0.7, color='grey', ec='k')\n","plt.ylabel('ARI score')\n","# plt.title('ARI score comparing the cluster\\nlabel prediction of pairs of models')\n","# plt.gca().set(ylim=(0.85,1))\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wDrsjSfz0b6R"},"source":["## 4 GridSearch optimization of the text preprocessing parameters"]},{"cell_type":"markdown","metadata":{"id":"nGpDxTlA-XmE"},"source":["### 4.0 GridSearch try (KMeans components optimization) on BOW"]},{"cell_type":"code","metadata":{"id":"h_p4Bus2c2Cv","executionInfo":{"status":"aborted","timestamp":1603917355478,"user_tz":-60,"elapsed":8465,"user":{"displayName":"Maryse Muller","photoUrl":"","userId":"13230828717959946182"}}},"source":["from sklearn.metrics import silhouette_score, calinski_harabasz_score,\\\n","davies_bouldin_score, adjusted_mutual_info_score, adjusted_rand_score,\\\n","completeness_score, homogeneity_score\n","\n","# Definition of the search space for hyperparameters\n","param_grid = {\"n_components\": [4,5,6,7,8],\n","              \"n_model\": ['lda', 'nmf', 'lsa']}\n","\n","# Instanciation of the GridSearch object\n","gsc = GridSearchClust(estimator=TopicsModeler(random_state=14), # KMeans()\n","                      param_grid_estim=param_grid,\n","                      scoring=['silh', 'cal-har', 'dav_bould',\n","                               'ami', 'ari', 'homog', 'complet'],\n","                      scoring_true_lab=df_desc_cat['category'],\n","                      refit='ari',\n","                      greater_is_better=True, # for the refit_score\n","                      )\n","\n","# Computing the results of the grid search\n","gsc.fit(doc_matrix_df, verbose=False);\n","\n","# Displays best parameters\n","print(\"Best hyperparameters:\", gsc.best_params_)\n","print(\"Best refit score:\", gsc.best_score_)\n","gsc_res = gsc.results_\n","scores_df = pd.DataFrame(gsc_res['scores'],\n","                         index = pd.DataFrame(gsc_res['params']))#.iloc[:,0])\n","scores_df"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MjVjEjU6r3lR"},"source":["### 4.1 GridSearch optimization"]},{"cell_type":"markdown","metadata":{"id":"_of3FkwehrmV"},"source":["#### Optimize preprocessing and PCA parameters with KMeans clustering"]},{"cell_type":"markdown","metadata":{"id":"VpIEXMuu0TV8"},"source":["Optimisation using a custom GridSearch adapted for clustering.\n","\n","Using either 'descriptions' or 'product_name'\n","\n","The pipeline to be optimized has 3 steps:\n","- Custom NLP transformer (BOW or TFIDF matrix)\n","- Dimensionality reduction (PCA)\n","- KMeans clustering (7 categories)"]},{"cell_type":"code","metadata":{"id":"A7bfrsiWr28E","executionInfo":{"status":"ok","timestamp":1603919076555,"user_tz":-60,"elapsed":2331,"user":{"displayName":"Maryse Muller","photoUrl":"","userId":"13230828717959946182"}}},"source":["import copy\n","from sklearn.pipeline import Pipeline\n","from sklearn.metrics import silhouette_score, calinski_harabasz_score,\\\n"," davies_bouldin_score, adjusted_mutual_info_score, adjusted_rand_score,\\\n"," homogeneity_score, completeness_score\n","from sklearn.decomposition import PCA\n","from nltk.stem.snowball import EnglishStemmer\n","import spacy\n","\n","# Define the stemmer or the lemmatizer\n","stemmer = EnglishStemmer()\n","lemmatizer = spacy.load('en', disable=['parser', 'ner'])\n","\n","# Define the pipeline to be executed and optimized by the GridSearch\n","pipe = Pipeline([('custtrans', CustNLPTransformer()), # Custom NLP preprocessor\n","                 ('stand', CustTransformer(strat_quant='stand')), # Standardizer\n","                 ('pca', PCA()), # PCA feature reduction\n","                 ('clusterer', KMeans(random_state=14))]) # KMeans clusterer\n","\n","# Define the list of params to be tested in the GridSearchClust\n","param_grid = {'custtrans__stopwords': [sw], # [sw, None],\n","              'custtrans__stemmer': [None],#[stemmer, None],\n","              'custtrans__lemmatizer': [lemmatizer], #, None],\n","              'custtrans__min_df': (np.linspace(0,30,10)).astype('int'),\n","              'custtrans__max_df': [1500],\n","              'custtrans__max_features': [5000],\n","              'custtrans__ngram_range': [(1,1), (1,2), (2,2), (2,3), (3,3), (1,2,3)],\n","              'custtrans__binary': [True, False],\n","              'custtrans__tfidf_on': [True, False],\n","              'pca__n_components': [6,10,15,20,30,40,50,60,70],#[15,20,25,30,35,40],\n","              'clusterer__n_clusters': [7]\n","              }\n","\n","# Instanciate of the GridSearch object\n","gsc = GridSearchClust(estimator=pipe,\n","                      param_grid_estim=param_grid,\n","                      scoring=['silh', 'cal-har', 'dav_bould',\n","                               'ami', 'ari', 'homog', 'complet'],\n","                      scoring_true_lab=df_desc_cat['category'],\n","                      refit='ami',\n","                      greater_is_better=True) # for the refit_score\n"],"execution_count":190,"outputs":[]},{"cell_type":"code","metadata":{"id":"7Xi1Vb58v40F"},"source":["##### Perform the grid search on the whole descriptions\n","gsc_desc = copy.deepcopy(gsc)\n","gsc_desc.fit(df_desc_cat['description'], verbose=False);\n","\n","##### Perform the grid search on the product names\n","gsc_pname = copy.deepcopy(gsc)\n","gsc_pname.fit(df_desc_cat['product_name'], verbose=False);"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8eyOx43hAiTR"},"source":["### Pickling the results\n","import dill as pickle\n","with open('gsc_prep_pca_km_WD.pkl', 'wb') as file:\n","    pickle.dump(gsc_desc, file)\n","with open('gsc_prep_pca_km_PN.pkl', 'wb') as file:\n","    pickle.dump(gsc_pname, file)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YuWpbrDFAj_j"},"source":["# ### Loading the results\n","# import dill as pickle\n","# with open('gsc_prep_pca_km_WD.pkl', 'rb') as file:\n","#     gsc_desc = pickle.load(file)\n","# with open('gsc_prep_pca_km_PN.pkl', 'rb') as file:\n","#     gsc_pname = pickle.load(file)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EUI8JOJRupM_"},"source":["\n","# Display results (whole descriptions)\n","print(\"-----Whole descriptions\")\n","print(\"Best hyperparameters:\", gsc_desc.best_params_)\n","print(\"Best refit score:\", gsc_desc.best_score_)\n","gsc_desc_res = gsc_desc.results_\n","scores_desc_df = pd.DataFrame(gsc_desc_res['scores'],\n","                              index = pd.DataFrame(gsc_desc_res['params']).iloc[:,0])\n","\n","# Display results (products name)\n","print(\"-----Product name\")\n","print(\"Best hyperparameters:\", gsc_pname.best_params_)\n","print(\"Best refit score:\", gsc_pname.best_score_)\n","gsc_pname_res = gsc_pname.results_\n","scores_pname_df = pd.DataFrame(gsc_pname_res['scores'],\n","                         index = pd.DataFrame(gsc_pname_res['params']).iloc[:,0])\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hEU-PtQxnsRD"},"source":["scores_pname_df"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dpaxVD_xOIzk"},"source":["speak(\"youpi cé fini\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ooniIC-qMBI4"},"source":["Plotting the results of the clustering optimization"]},{"cell_type":"code","metadata":{"id":"ldXprRcFwVQf"},"source":["# Compute the dataframe showing the results depending on one parameter\n","#(other parameters set tot best_params_)\n","\n","best_params, df_sel_scores, df_gsclust_filt, df_res = \\\n","            filters_gsclust_results(gsc_pname, 'pca__n_components',\n","                                    return_df_res=True)\n","df_sel_scores"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gxb-1pJyX1Sq"},"source":["# Plotting the graphs of the clustering scores with best refit score (line)\n","\n","plot_gsc_multi_scores(gsc=gsc_pname, param='pca__n_components', title=None,\n","                      scores=['ari', 'ami', 'complet', 'dav_bould', 'silh'],\n","                      x_log=False, loc='best', figsize=(12, 3))\n","\n","plot_gsc_multi_scores(gsc=gsc_pname, param='custtrans__min_df', title=None,\n","                      scores=['ari', 'ami', 'complet', 'dav_bould', 'silh'],\n","                      x_log=False, loc='best', figsize=(12, 3))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dh8W2Ojk8k0N"},"source":["# Effect of 2 parameters on clustering scores\n","\n","params1 = ['pca__n_components', 'custtrans__tfidf_on']\n","params2 = ['custtrans__min_df', 'custtrans__max_df']\n","params3 = ['custtrans__ngram_range', 'custtrans__binary']\n","\n","fig = plt.figure(figsize=(12,5))\n","\n","ax1 = fig.add_subplot(1,3,1)\n","plot_2D_gsclust_param_opt(gsc_pname, params=params1, shorten_label=5, ax=ax1)\n","\n","ax2 = fig.add_subplot(1,3,2)\n","plot_2D_gsclust_param_opt(gsc_pname, params=params2, shorten_label=5, ax=ax2)\n","\n","ax3 = fig.add_subplot(1,3,3)\n","plot_2D_gsclust_param_opt(gsc_pname, params=params3, shorten_label=5, ax=ax3)\n","\n","plt.gcf().suptitle('ARI score _ parameters combinations', fontweight='bold')\n","plt.tight_layout(rect=[0,0,1,0.92])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nh2VkC-8o0Gn"},"source":["# All the scores through 2 chosen parameters\n","\n","fig = plt.figure(figsize=(10,4))\n","params = ['pca__n_components', 'custtrans__tfidf_on']\n","\n","scores = ['ami', 'ari', 'complet'] # list(gsc.get_params()['scoring'])\n","### Looping on all the scores\n","for i, score in enumerate(scores, 1):\n","    ax = fig.add_subplot(1,len(scores),i)\n","    plot_2D_gsclust_param_opt(gsc_pname, params=params,\n","                              score=score, title=None, ax=ax)\n","    \n","plt.gcf().suptitle('Clustering scores', fontweight='bold')\n","plt.tight_layout(rect=[0,0,1,0.92])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VSUx6jMFqFAo"},"source":["speak(\"youpi cé vréma fini\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vIXRVWPFjs3z"},"source":["#0.6165"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"73BNApDLsCSe"},"source":["#### Optimize preprocessing and the method of topics modeling"]},{"cell_type":"markdown","metadata":{"id":"0w1M3M7ZsCSg"},"source":["Optimisation using a custom GridSearch adapted for clustering.\n","\n","Using either 'descriptions' or 'product_name'\n","\n","The pipeline to be optimized has 2 steps:\n","- Custom NLP transformer (BOW or TFIDF matrix)\n","- Clustering using topics modeling (7 topics)"]},{"cell_type":"code","metadata":{"id":"aYMvV9D5sCSg"},"source":["import copy\n","from sklearn.pipeline import Pipeline\n","from sklearn.metrics import silhouette_score, calinski_harabasz_score,\\\n"," davies_bouldin_score, adjusted_mutual_info_score, adjusted_rand_score,\\\n"," homogeneity_score, completeness_score\n","from sklearn.decomposition import PCA\n","from nltk.stem.snowball import EnglishStemmer\n","import spacy\n","\n","# Define the stemmer or the lemmatizer\n","stemmer = EnglishStemmer()\n","lemmatizer = spacy.load('en', disable=['parser', 'ner'])\n","\n","\n","# Define the pipeline to be executed and optimized by the GridSearch\n","pipe = Pipeline([('custtrans', CustNLPTransformer()), # Custom NLP preprocessor\n","                 ('clusterer', TopicsModeler(random_state=14))]) # KMeans clusterer\n","\n","# Define the list of params to be tested in the GridSearchClust\n","param_grid = {'custtrans__stopwords': [sw], # [sw, None],\n","              'custtrans__stemmer': [None],#[stemmer, None],\n","              'custtrans__lemmatizer': [lemmatizer], #, None],\n","              'custtrans__min_df': (np.linspace(0,30,10)).astype('int'),\n","              'custtrans__max_df': [None],\n","              'custtrans__max_features': [5000],\n","              'custtrans__ngram_range': [(1,1), (1,2), (2,2), (2,3), (3,3), (1,2,3)],\n","              'custtrans__binary': [True, False],\n","              'custtrans__tfidf_on': [True, False],\n","              'clusterer__n_model': ['lsa', 'nmf', 'lda'],\n","              'clusterer__n_components': [7]\n","              }\n","\n","# Instanciate of the GridSearch object\n","gsc = GridSearchClust(estimator=pipe,\n","                      param_grid_estim=param_grid,\n","                      scoring=['silh', 'cal-har', 'dav_bould',\n","                               'ami', 'ari', 'homog', 'complet'],\n","                      scoring_true_lab=df_desc_cat['category'],\n","                      refit='ami',\n","                      greater_is_better=True) # for the refit_score"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hjB1rwvXsCSk"},"source":["##### Perform the grid search on the whole descriptions\n","gsc_desc = copy.deepcopy(gsc)\n","gsc_desc.fit(df_desc_cat['description'], verbose=False);\n","\n","##### Perform the grid search on the product names\n","gsc_pname = copy.deepcopy(gsc)\n","gsc_pname.fit(df_desc_cat['product_name'], verbose=False);"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7KNxKuQ3A8EP"},"source":["### Pickling the results\n","import dill as pickle\n","with open('gsc_prep_pca_tm_WD.pkl', 'wb') as file:\n","    pickle.dump(gsc_desc, file)\n","with open('gsc_prep_pca_tm_PN.pkl', 'wb') as file:\n","    pickle.dump(gsc_pname, file)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-6Di6j7LA9o3"},"source":["# ### Loading the results\n","# import dill as pickle\n","# with open('gsc_prep_pca_tm_WD.pkl', 'rb') as file:\n","#     gsc_desc = pickle.load(file)\n","# with open('gsc_prep_pca_tm_PN.pkl', 'rb') as file:\n","#     gsc_pname = pickle.load(file)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jEG2x9hrsCSn"},"source":["# Display results (whole descriptions)\n","print(\"-----Whole descriptions\")\n","print(\"Best hyperparameters:\", gsc_desc.best_params_)\n","print(\"Best refit score:\", gsc_desc.best_score_)\n","gsc_desc_res = gsc_desc.results_\n","scores_desc_df = pd.DataFrame(gsc_desc_res['scores'],\n","                              index = pd.DataFrame(gsc_desc_res['params']).iloc[:,0])\n","\n","# Display results (products name)\n","print(\"-----Product name\")\n","print(\"Best hyperparameters:\", gsc_pname.best_params_)\n","print(\"Best refit score:\", gsc_pname.best_score_)\n","gsc_pname_res = gsc_pname.results_\n","scores_pname_df = pd.DataFrame(gsc_pname_res['scores'],\n","                         index = pd.DataFrame(gsc_pname_res['params']).iloc[:,0])\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5IHPTVlPsCSq"},"source":["scores_pname_df"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zo3qUIkQsCSu"},"source":["speak(\"youpi cé fini\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IaXScllVsCSx"},"source":["Plotting the results of the clustering optimization"]},{"cell_type":"code","metadata":{"id":"FVNqSZC9sCSy"},"source":["# Compute the dataframe showing the results depending on one parameter\n","#(other parameters set tot best_params_)\n","\n","best_params, df_sel_scores, df_gsclust_filt, df_res = \\\n","            filters_gsclust_results(gsc_pname, 'clusterer__n_model',\n","                                    return_df_res=True)\n","df_sel_scores"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Dxk7L_1KsCS1"},"source":["# Plotting the graphs of the clustering scores with best refit score (line)\n","\n","plot_gsc_multi_scores(gsc=gsc_pname, param='pca__n_components', title=None,\n","                      scores=['ari', 'ami', 'complet', 'dav_bould', 'silh'],\n","                      x_log=False, loc='best', figsize=(12, 3))\n","\n","plot_gsc_multi_scores(gsc=gsc_pname, param='custtrans__min_df', title=None,\n","                      scores=['ari', 'ami', 'complet', 'dav_bould', 'silh'],\n","                      x_log=False, loc='best', figsize=(12, 3))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sVmkQXlXsCS3"},"source":["# Effect of 2 parameters on clustering scores\n","\n","# params1 = ['pca__n_components', 'custtrans__tfidf_on']\n","params2 = ['custtrans__min_df', 'custtrans__max_df']\n","# params3 = ['custtrans__ngram_range', 'custtrans__binary']\n","\n","fig = plt.figure(figsize=(12,5))\n","\n","ax1 = fig.add_subplot(1,3,1)\n","plot_2D_gsclust_param_opt(gsc_pname, params=params1, shorten_label=5, ax=ax1)\n","\n","ax2 = fig.add_subplot(1,3,2)\n","plot_2D_gsclust_param_opt(gsc_pname, params=params2, shorten_label=5, ax=ax2)\n","\n","ax3 = fig.add_subplot(1,3,3)\n","plot_2D_gsclust_param_opt(gsc_pname, params=params3, shorten_label=5, ax=ax3)\n","\n","plt.gcf().suptitle('ARI score _ parameters combinations', fontweight='bold')\n","plt.tight_layout(rect=[0,0,1,0.92])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6EK6NU-9sCS6"},"source":["# All the scores through 2 chosen parameters\n","\n","fig = plt.figure(figsize=(10,4))\n","params = ['pca__n_components', 'custtrans__tfidf_on']\n","\n","scores = ['ami', 'ari', 'complet'] # list(gsc.get_params()['scoring'])\n","### Looping on all the scores\n","for i, score in enumerate(scores, 1):\n","    ax = fig.add_subplot(1,len(scores),i)\n","    plot_2D_gsclust_param_opt(gsc_pname, params=params,\n","                              score=score, title=None, ax=ax)\n","    \n","plt.gcf().suptitle('Clustering scores', fontweight='bold')\n","plt.tight_layout(rect=[0,0,1,0.92])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"w_aBw5IvsCS9"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"F_vwDFjHsCTB"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Y3i_E650jsxm"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_BRj7Qw8r22x"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ol9vr-bM6D3B"},"source":["# from sklearn.decomposition import PCA\n","# from umap import UMAP\n","# from sklearn.manifold import TSNE\n","# from sklearn.decomposition import NMF\n","# from sklearn.decomposition import LatentDirichletAllocation as LDA\n","# from sklearn.decomposition import TruncatedSVD\n","\n","# k=7 # number of categories\n","\n","# nmf = NMF(init=\"nndsvd\",\n","#           n_components=k)\n","# lda = LDA(n_components=k)\n","# lsa = TruncatedSVD(n_components=k)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KqKhl3s46D7H"},"source":[""],"execution_count":null,"outputs":[]}]}