{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"P6_Cleaning_NLP_old.ipynb","provenance":[{"file_id":"1qAxkvKD2odWAQUnotM2Dwijh45Ko0iCO","timestamp":1587461379791},{"file_id":"1wX6ZnTUKkvXaWwnL6E51NNuJkxEdSK-h","timestamp":1582132452639},{"file_id":"144JCi9-nMiX9eD3AccG2sgaTbug2wQTa","timestamp":1581758429991},{"file_id":"1kRjoSOVhLf1GbPOY6C2MGdpHdjUsJGe1","timestamp":1581430098593},{"file_id":"1SzuDOE2ejfYYNHpdu1hTrvQZd5phpvyP","timestamp":1581092731309},{"file_id":"1NHY7TNgChDa8i5eggkSxZV133o6RB2pP","timestamp":1580472396109},{"file_id":"1CjFqLqI3e83aWkErpy2_tDoqWWSc5V7K","timestamp":1567509524556},{"file_id":"1oFtNqY9sTtyX09HnsLY5GOpchE7TWSLc","timestamp":1567440734485},{"file_id":"1kO3qnFJ8XAhA2WzueAy6Gwr0KwfoSF2z","timestamp":1566893631574},{"file_id":"1rI7P6dn7-IGK6p8HX7dvNP93roGSzXeX","timestamp":1566833630097},{"file_id":"1cAbXwtjxfOIVmnecCFlHrVryskoYJCXA","timestamp":1566734390006},{"file_id":"https://github.com/SmellyArmure/PROJECT3/blob/master/NOTEBOOKS/P3_Cleaning_v1_0.ipynb","timestamp":1566726579304}],"collapsed_sections":[],"toc_visible":true,"machine_shape":"hm"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"pVbrKjWrGl4m"},"source":["# \"Classifiez automatiquement des biens de consommation\"\n","_NLP Cleaning Notebook_"]},{"cell_type":"markdown","metadata":{"id":"AXy2xt5wB3ZD"},"source":["## 0 Preliminaries"]},{"cell_type":"markdown","metadata":{"id":"NQ8_ZaJvGl4o"},"source":["### 0.0 Importing Packages and Modules"]},{"cell_type":"markdown","metadata":{"id":"XKUzl6mHOcZa"},"source":["Checking whether the notebook is on Colab or PC"]},{"cell_type":"code","metadata":{"id":"S7MAxokr4UmP","executionInfo":{"status":"ok","timestamp":1603919247503,"user_tz":-60,"elapsed":11604,"user":{"displayName":"Maryse Muller","photoUrl":"","userId":"13230828717959946182"}},"outputId":"2adfb914-3a23-440b-c9e9-d07b209c76e8","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["import sys\n","is_colab = 'google.colab' in sys.modules\n","is_colab, sys.executable"],"execution_count":1,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(True, '/usr/bin/python3')"]},"metadata":{"tags":[]},"execution_count":1}]},{"cell_type":"markdown","metadata":{"id":"g9oJU8UHOaRC"},"source":["Mounting my Drive if on Colab"]},{"cell_type":"code","metadata":{"id":"l5RrOSXvfGrC","executionInfo":{"status":"ok","timestamp":1603919294076,"user_tz":-60,"elapsed":58040,"user":{"displayName":"Maryse Muller","photoUrl":"","userId":"13230828717959946182"}},"outputId":"f433ad75-5eee-49dd-80ec-4451b89c82d3","colab":{"base_uri":"https://localhost:8080/","height":69}},"source":["if is_colab==True:\n","    from google.colab import files, output, drive\n","    drive.mount('/gdrive')\n","    %cd /gdrive\n","    print(\"You're on Google Colab\")\n","else:\n","    print(\"You're on a PC\")"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Mounted at /gdrive\n","/gdrive\n","You're on Google Colab\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"F7E_-Zd9OgY7"},"source":["Installations and importations required in the virtual environment."]},{"cell_type":"code","metadata":{"id":"VZ__n1yHHrQJ","executionInfo":{"status":"ok","timestamp":1603919294079,"user_tz":-60,"elapsed":58004,"user":{"displayName":"Maryse Muller","photoUrl":"","userId":"13230828717959946182"}}},"source":["# import os\n","# if os.getcwd()!='/gdrive/My Drive/--DATA SCIENCE/PROJET6/NOTEBOOKS':\n","#     os.chdir('/gdrive/My Drive/--DATA SCIENCE/PROJET6/NOTEBOOKS')"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"TCg0w1hgprYQ","outputId":"fd6dd01c-1c8d-4ed0-8c6d-64c9067e8fcc","colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["import os\n","if is_colab==True:\n","    if os.getcwd()!='/gdrive/My Drive/--DATA SCIENCE/PROJET6/NOTEBOOKS':\n","        os.chdir('/gdrive/My Drive/--DATA SCIENCE/PROJET6/NOTEBOOKS')\n","else:\n","    if not (os.path.exists(os.getcwd()+'/requirements_nlp.txt') \\\n","                     and os.path.exists(os.getcwd()+'/P6_functions.py')):\n","        print(\"ERROR: Make sure 'P6_functions.py' and \\\n","'requirements_nlp.txt' are in the current working directory\")\n","\n","!pip install -r requirements_nlp.txt"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: absl-py==0.10.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 1)) (0.10.0)\n","Requirement already satisfied: alabaster==0.7.12 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 2)) (0.7.12)\n","Requirement already satisfied: albumentations==0.1.12 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 3)) (0.1.12)\n","Requirement already satisfied: altair==4.1.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 4)) (4.1.0)\n","Requirement already satisfied: argon2-cffi==20.1.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 5)) (20.1.0)\n","Requirement already satisfied: asgiref==3.2.10 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 6)) (3.2.10)\n","Requirement already satisfied: astor==0.8.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 7)) (0.8.1)\n","Collecting astropy==4.0.1.post1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/da/c2/07b8244febf380f8fe43d87ab906bfff70221ba095df3fe85cf89957c63d/astropy-4.0.1.post1-cp36-cp36m-manylinux1_x86_64.whl (6.5MB)\n","\u001b[K     |████████████████████████████████| 6.5MB 2.4MB/s \n","\u001b[?25hRequirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 9)) (1.6.3)\n","Requirement already satisfied: async-generator==1.10 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 10)) (1.10)\n","Requirement already satisfied: atari-py==0.2.6 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 11)) (0.2.6)\n","Requirement already satisfied: atomicwrites==1.4.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 12)) (1.4.0)\n","Requirement already satisfied: attrs==20.2.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 13)) (20.2.0)\n","Collecting audioread==2.1.8\n","  Downloading https://files.pythonhosted.org/packages/2e/0b/940ea7861e0e9049f09dcfd72a90c9ae55f697c17c299a323f0148f913d2/audioread-2.1.8.tar.gz\n","Requirement already satisfied: autograd==1.3 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 15)) (1.3)\n","Requirement already satisfied: Babel==2.8.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 16)) (2.8.0)\n","Requirement already satisfied: backcall==0.2.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 17)) (0.2.0)\n","Requirement already satisfied: beautifulsoup4==4.6.3 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 18)) (4.6.3)\n","Requirement already satisfied: bleach==3.2.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 19)) (3.2.1)\n","Requirement already satisfied: blis==0.4.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 20)) (0.4.1)\n","Requirement already satisfied: bokeh==2.1.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 21)) (2.1.1)\n","Requirement already satisfied: Bottleneck==1.3.2 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 22)) (1.3.2)\n","Requirement already satisfied: branca==0.4.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 23)) (0.4.1)\n","Requirement already satisfied: bs4==0.0.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 24)) (0.0.1)\n","Requirement already satisfied: CacheControl==0.12.6 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 25)) (0.12.6)\n","Requirement already satisfied: cachetools==4.1.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 26)) (4.1.1)\n","Requirement already satisfied: catalogue==1.0.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 27)) (1.0.0)\n","Collecting category-encoders==2.2.2\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/44/57/fcef41c248701ee62e8325026b90c432adea35555cbc870aff9cfba23727/category_encoders-2.2.2-py2.py3-none-any.whl (80kB)\n","\u001b[K     |████████████████████████████████| 81kB 8.1MB/s \n","\u001b[?25hRequirement already satisfied: certifi==2020.6.20 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 29)) (2020.6.20)\n","Requirement already satisfied: cffi==1.14.3 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 30)) (1.14.3)\n","Requirement already satisfied: chainer==7.4.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 31)) (7.4.0)\n","Requirement already satisfied: chardet==3.0.4 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 32)) (3.0.4)\n","Requirement already satisfied: click==7.1.2 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 33)) (7.1.2)\n","Requirement already satisfied: cloudpickle==1.3.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 34)) (1.3.0)\n","Requirement already satisfied: cmake==3.12.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 35)) (3.12.0)\n","Requirement already satisfied: cmdstanpy==0.9.5 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 36)) (0.9.5)\n","Requirement already satisfied: colorlover==0.3.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 37)) (0.3.0)\n","Requirement already satisfied: community==1.0.0b1 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 38)) (1.0.0b1)\n","Requirement already satisfied: contextlib2==0.5.5 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 39)) (0.5.5)\n","Requirement already satisfied: convertdate==2.2.2 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 40)) (2.2.2)\n","Requirement already satisfied: coverage==3.7.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 41)) (3.7.1)\n","Requirement already satisfied: coveralls==0.5 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 42)) (0.5)\n","Requirement already satisfied: crcmod==1.7 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 43)) (1.7)\n","Requirement already satisfied: cufflinks==0.17.3 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 44)) (0.17.3)\n","Requirement already satisfied: cvxopt==1.2.5 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 45)) (1.2.5)\n","Requirement already satisfied: cvxpy==1.0.31 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 46)) (1.0.31)\n","Requirement already satisfied: cycler==0.10.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 47)) (0.10.0)\n","Requirement already satisfied: cymem==2.0.3 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 48)) (2.0.3)\n","Requirement already satisfied: Cython==0.29.21 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 49)) (0.29.21)\n","Requirement already satisfied: daft==0.0.4 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 50)) (0.0.4)\n","Requirement already satisfied: dask==2.12.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 51)) (2.12.0)\n","Requirement already satisfied: dataclasses==0.7 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 52)) (0.7)\n","Requirement already satisfied: datascience==0.10.6 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 53)) (0.10.6)\n","Collecting debugpy==1.0.0rc2\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/cc/62/71cde089e243a7dbbba06dc39c3a497410a94cfc774aa28d43622b3cfd02/debugpy-1.0.0rc2-cp36-cp36m-manylinux2010_x86_64.whl (5.3MB)\n","\u001b[K     |████████████████████████████████| 5.3MB 37.6MB/s \n","\u001b[?25hRequirement already satisfied: decorator==4.4.2 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 55)) (4.4.2)\n","Requirement already satisfied: defusedxml==0.6.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 56)) (0.6.0)\n","Requirement already satisfied: descartes==1.1.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 57)) (1.1.0)\n","Requirement already satisfied: dill==0.3.2 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 58)) (0.3.2)\n","Requirement already satisfied: distributed==1.25.3 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 59)) (1.25.3)\n","Collecting Django==3.1.1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/01/a5/fb3dad18422fcd4241d18460a1fe17542bfdeadcf74e3861d1a2dfc9e459/Django-3.1.1-py3-none-any.whl (7.8MB)\n","\u001b[K     |████████████████████████████████| 7.8MB 46.8MB/s \n","\u001b[?25hRequirement already satisfied: dlib==19.18.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 61)) (19.18.0)\n","Requirement already satisfied: dm-tree==0.1.5 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 62)) (0.1.5)\n","Requirement already satisfied: docopt==0.6.2 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 63)) (0.6.2)\n","Requirement already satisfied: docutils==0.16 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 64)) (0.16)\n","Requirement already satisfied: dopamine-rl==1.0.5 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 65)) (1.0.5)\n","Collecting earthengine-api==0.1.236\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/be/66/a63e0a247145daeea426047010f64f71355732a85855ed6d59ff09b94199/earthengine-api-0.1.236.tar.gz (157kB)\n","\u001b[K     |████████████████████████████████| 163kB 50.4MB/s \n","\u001b[?25hRequirement already satisfied: easydict==1.9 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 67)) (1.9)\n","Requirement already satisfied: ecos==2.0.7.post1 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 68)) (2.0.7.post1)\n","Requirement already satisfied: editdistance==0.5.3 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 69)) (0.5.3)\n","Requirement already satisfied: en-core-web-sm==2.2.5 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 70)) (2.2.5)\n","Requirement already satisfied: entrypoints==0.3 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 71)) (0.3)\n","Requirement already satisfied: ephem==3.7.7.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 72)) (3.7.7.1)\n","Requirement already satisfied: et-xmlfile==1.0.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 73)) (1.0.1)\n","Requirement already satisfied: fa2==0.3.5 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 74)) (0.3.5)\n","Requirement already satisfied: fancyimpute==0.4.3 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 75)) (0.4.3)\n","Requirement already satisfied: fastai==1.0.61 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 76)) (1.0.61)\n","Requirement already satisfied: fastdtw==0.3.4 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 77)) (0.3.4)\n","Requirement already satisfied: fastprogress==1.0.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 78)) (1.0.0)\n","Requirement already satisfied: fastrlock==0.5 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 79)) (0.5)\n","Requirement already satisfied: fbprophet==0.7.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 80)) (0.7.1)\n","Requirement already satisfied: feather-format==0.4.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 81)) (0.4.1)\n","Requirement already satisfied: filelock==3.0.12 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 82)) (3.0.12)\n","Requirement already satisfied: firebase-admin==4.4.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 83)) (4.4.0)\n","Requirement already satisfied: fix-yahoo-finance==0.0.22 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 84)) (0.0.22)\n","Requirement already satisfied: Flask==1.1.2 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 85)) (1.1.2)\n","Requirement already satisfied: folium==0.8.3 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 86)) (0.8.3)\n","Requirement already satisfied: future==0.16.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 87)) (0.16.0)\n","Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 88)) (0.3.3)\n","Requirement already satisfied: GDAL==2.2.2 in /usr/lib/python3/dist-packages (from -r requirements_nlp.txt (line 89)) (2.2.2)\n","Requirement already satisfied: gdown==3.6.4 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 90)) (3.6.4)\n","Requirement already satisfied: gensim==3.6.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 91)) (3.6.0)\n","Requirement already satisfied: geographiclib==1.50 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 92)) (1.50)\n","Requirement already satisfied: geopy==1.17.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 93)) (1.17.0)\n","Requirement already satisfied: gin-config==0.3.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 94)) (0.3.0)\n","Requirement already satisfied: glob2==0.7 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 95)) (0.7)\n","Requirement already satisfied: google==2.0.3 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 96)) (2.0.3)\n","Requirement already satisfied: google-api-core==1.16.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 97)) (1.16.0)\n","Requirement already satisfied: google-api-python-client==1.7.12 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 98)) (1.7.12)\n","Requirement already satisfied: google-auth==1.17.2 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 99)) (1.17.2)\n","Requirement already satisfied: google-auth-httplib2==0.0.4 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 100)) (0.0.4)\n","Requirement already satisfied: google-auth-oauthlib==0.4.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 101)) (0.4.1)\n","Requirement already satisfied: google-cloud-bigquery==1.21.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 102)) (1.21.0)\n","Requirement already satisfied: google-cloud-core==1.0.3 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 103)) (1.0.3)\n","Requirement already satisfied: google-cloud-datastore==1.8.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 104)) (1.8.0)\n","Requirement already satisfied: google-cloud-firestore==1.7.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 105)) (1.7.0)\n","Requirement already satisfied: google-cloud-language==1.2.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 106)) (1.2.0)\n","Requirement already satisfied: google-cloud-storage==1.18.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 107)) (1.18.1)\n","Requirement already satisfied: google-cloud-translate==1.5.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 108)) (1.5.0)\n","Requirement already satisfied: google-colab==1.0.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 109)) (1.0.0)\n","Requirement already satisfied: google-pasta==0.2.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 110)) (0.2.0)\n","Requirement already satisfied: google-resumable-media==0.4.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 111)) (0.4.1)\n","Requirement already satisfied: googleapis-common-protos==1.52.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 112)) (1.52.0)\n","Requirement already satisfied: googledrivedownloader==0.4 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 113)) (0.4)\n","Requirement already satisfied: graphviz==0.10.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 114)) (0.10.1)\n","Collecting grpcio==1.32.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f0/00/b393f5d0e92b37592a41357ea3077010c95400c907f6b9af01f4f6abe140/grpcio-1.32.0-cp36-cp36m-manylinux2014_x86_64.whl (3.8MB)\n","\u001b[K     |████████████████████████████████| 3.8MB 40.9MB/s \n","\u001b[?25hRequirement already satisfied: gspread==3.0.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 116)) (3.0.1)\n","Requirement already satisfied: gspread-dataframe==3.0.8 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 117)) (3.0.8)\n","Collecting gTTS==2.1.1\n","  Downloading https://files.pythonhosted.org/packages/a1/0c/4ca77eca3b739a4a08360930643f58d714e302fee0d2f8c654e67d9af8e7/gTTS-2.1.1-py3-none-any.whl\n","Collecting gTTS-token==1.1.3\n","  Downloading https://files.pythonhosted.org/packages/e7/25/ca6e9cd3275bfc3097fe6b06cc31db6d3dfaf32e032e0f73fead9c9a03ce/gTTS-token-1.1.3.tar.gz\n","Collecting gym==0.17.2\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b3/99/7cc3e510678119cdac91f33fb9235b98448f09a6bdf0cafea2b108d9ce51/gym-0.17.2.tar.gz (1.6MB)\n","\u001b[K     |████████████████████████████████| 1.6MB 55.1MB/s \n","\u001b[?25hRequirement already satisfied: h5py==2.10.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 121)) (2.10.0)\n","Requirement already satisfied: HeapDict==1.0.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 122)) (1.0.1)\n","Requirement already satisfied: holidays==0.10.3 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 123)) (0.10.3)\n","Requirement already satisfied: holoviews==1.13.4 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 124)) (1.13.4)\n","Requirement already satisfied: html5lib==1.0.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 125)) (1.0.1)\n","Requirement already satisfied: httpimport==0.5.18 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 126)) (0.5.18)\n","Requirement already satisfied: httplib2==0.17.4 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 127)) (0.17.4)\n","Requirement already satisfied: httplib2shim==0.0.3 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 128)) (0.0.3)\n","Requirement already satisfied: humanize==0.5.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 129)) (0.5.1)\n","Requirement already satisfied: hyperopt==0.1.2 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 130)) (0.1.2)\n","Requirement already satisfied: ideep4py==2.0.0.post3 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 131)) (2.0.0.post3)\n","Requirement already satisfied: idna==2.10 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 132)) (2.10)\n","Requirement already satisfied: image==1.5.32 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 133)) (1.5.32)\n","Requirement already satisfied: imageio==2.4.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 134)) (2.4.1)\n","Requirement already satisfied: imagesize==1.2.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 135)) (1.2.0)\n","Requirement already satisfied: imbalanced-learn==0.4.3 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 136)) (0.4.3)\n","Requirement already satisfied: imblearn==0.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 137)) (0.0)\n","Requirement already satisfied: imgaug==0.2.9 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 138)) (0.2.9)\n","Requirement already satisfied: importlib-metadata==2.0.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 139)) (2.0.0)\n","Requirement already satisfied: imutils==0.5.3 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 140)) (0.5.3)\n","Requirement already satisfied: inflect==2.1.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 141)) (2.1.0)\n","Collecting iniconfig==1.0.1\n","  Downloading https://files.pythonhosted.org/packages/20/46/d2f4919cc48c39c2cb48b589ca9016aae6bad050b8023667eb86950d3da2/iniconfig-1.0.1-py3-none-any.whl\n","Requirement already satisfied: intel-openmp==2020.0.133 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 143)) (2020.0.133)\n","Requirement already satisfied: intervaltree==2.1.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 144)) (2.1.0)\n","Requirement already satisfied: ipykernel==4.10.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 145)) (4.10.1)\n","Requirement already satisfied: ipython==5.5.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 146)) (5.5.0)\n","Requirement already satisfied: ipython-genutils==0.2.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 147)) (0.2.0)\n","Requirement already satisfied: ipython-sql==0.3.9 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 148)) (0.3.9)\n","Requirement already satisfied: ipywidgets==7.5.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 149)) (7.5.1)\n","Requirement already satisfied: itsdangerous==1.1.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 150)) (1.1.0)\n","Collecting jax==0.2.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e1/d9/9bd335976d3b61f705c2e9c35da2c6e030f9cd9ffd3e111feb99d8d169a7/jax-0.2.0.tar.gz (454kB)\n","\u001b[K     |████████████████████████████████| 460kB 40.0MB/s \n","\u001b[?25hCollecting jaxlib==0.1.55\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a0/e2/7e2c7e5b2b2b06c0868f8408f5ed016f8ee83540381cfe43d96bf1e8463b/jaxlib-0.1.55-cp36-none-manylinux2010_x86_64.whl (31.9MB)\n","\u001b[K     |████████████████████████████████| 31.9MB 159kB/s \n","\u001b[?25hRequirement already satisfied: jdcal==1.4.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 153)) (1.4.1)\n","Requirement already satisfied: jedi==0.17.2 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 154)) (0.17.2)\n","Requirement already satisfied: jieba==0.42.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 155)) (0.42.1)\n","Requirement already satisfied: Jinja2==2.11.2 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 156)) (2.11.2)\n","Collecting joblib==0.16.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/51/dd/0e015051b4a27ec5a58b02ab774059f3289a94b0906f880a3f9507e74f38/joblib-0.16.0-py3-none-any.whl (300kB)\n","\u001b[K     |████████████████████████████████| 307kB 52.4MB/s \n","\u001b[?25hRequirement already satisfied: jpeg4py==0.1.4 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 158)) (0.1.4)\n","Requirement already satisfied: jsonschema==2.6.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 159)) (2.6.0)\n","Requirement already satisfied: jupyter==1.0.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 160)) (1.0.0)\n","Requirement already satisfied: jupyter-client==5.3.5 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 161)) (5.3.5)\n","Requirement already satisfied: jupyter-console==5.2.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 162)) (5.2.0)\n","Requirement already satisfied: jupyter-core==4.6.3 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 163)) (4.6.3)\n","Requirement already satisfied: jupyterlab-pygments==0.1.2 in /usr/local/lib/python3.6/dist-packages (from -r requirements_nlp.txt (line 164)) (0.1.2)\n","Collecting kaggle==1.5.8\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fc/14/9db40d8d6230655e76fa12166006f952da4697c003610022683c514cf15f/kaggle-1.5.8.tar.gz (59kB)\n","\u001b[K     |████████████████████████████████| 61kB 7.5MB/s \n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"x1xzfQL5OvRk"},"source":["from P6_functions import *"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gvdHBoIuH90C"},"source":["Installations (creating the requirements file)"]},{"cell_type":"code","metadata":{"id":"-hH0JhI_ang_"},"source":["# in order to get orca to be installed in colab (to display static plotly graphs)\n","!pip install plotly>=4.0.0\n","!wget https://github.com/plotly/orca/releases/download/v1.2.1/orca-1.2.1-x86_64.AppImage -O /usr/local/bin/orca\n","!chmod +x /usr/local/bin/orca\n","!apt-get install xvfb libgtk2.0-0 libgconf-2-4"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"H1oa9ebJHSOm"},"source":["# !pip install gtts\n","# !pip install wikipedia2vec==0.2.2"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gQ-WJqLWFtzu"},"source":["# !pip freeze > requirements_cleaning_nlp.txt"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xBgiuFEdQq78"},"source":["Importation of modules and packages. "]},{"cell_type":"code","metadata":{"id":"oDhE9utOlwJe"},"source":["import io\n","\n","import string\n","\n","import pandas as pd\n","import seaborn as sns\n","import numpy as np\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","plt.rcParams['figure.facecolor']='w'\n","\n","# import warnings\n","# warnings.filterwarnings('ignore')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xNSTytWrQ0De"},"source":["Setting pandas display options."]},{"cell_type":"code","metadata":{"id":"G0rRvyJaWO2h"},"source":["dictPdSettings = {'display.max_rows': 500, 'display.width': 100,\n","                  'display.max_colwidth': 100,\n","                  'display.float_format': lambda x: '%.2f' % x}\n","for k,v in dictPdSettings.items():\n","  pd.set_option(k,v)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"z9APZjLzQ_sp"},"source":["To play audio text-to-speech during execution."]},{"cell_type":"code","metadata":{"id":"vgvmjvZ_Y6-s"},"source":["from IPython.display import Audio\n","from gtts import gTTS\n","\n","def speak(text, lang='en'):\n","    with io.BytesIO() as f:\n","        gTTS(text=text, lang=lang).write_to_fp(f)\n","        f.seek(0)\n","        return Audio(f.read(), autoplay=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"X5b_ibTl83bD"},"source":["speak('Packages and modules successfully imported')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fmDu0rMFGl5A"},"source":["### 0.1 Importing the datasets"]},{"cell_type":"markdown","metadata":{"id":"R_c4YLdXQAzn"},"source":["Data is composed of 9 distinct .csv files we'll load in a dictionnary of dataframes."]},{"cell_type":"code","metadata":{"id":"KhVW-wxr30Ia"},"source":["if is_colab==True:\n","    # Importing database from my Drive\n","    print(\"Try to import data files in the notebook from myDrive...\")\n","else:\n","    # Importing database from PC\n","    print(\"Try to import data files in the notebook from PC ('DATA')...\")\n","\n","df = pd.read_csv(\"../DATA/flipkart_com-ecommerce_sample_1050.csv\",\n","                 sep=',', \n","                 index_col = 'uniq_id',\n","                 encoding ='utf-8')\n","\n","print(\"-----> Importation of .csv in the notebook: OK\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"61owD2pedhh5"},"source":["speak('Datasets successfully imported')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iwXHJlEdGl5K"},"source":["### 0.2 First Overview"]},{"cell_type":"code","metadata":{"id":"whaE4oVZOV0m"},"source":["df.describe(include='all')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"apekUpouG6Dm"},"source":["Printing total nb and percentage of null:"]},{"cell_type":"code","metadata":{"id":"KFDiWiw7h7o0"},"source":["display(print_null_pct(df))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"n_IylQi3PgFY"},"source":["df.isna().sum()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DnQnFuUtUS6o"},"source":["Browsing the content"]},{"cell_type":"code","metadata":{"id":"51M5IxecPgPw"},"source":["df[df.isna().any(1)].sample(1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qi8Wc1YjPgNd"},"source":["df['product_specifications'][0]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oTL4RpTgPgLj"},"source":["df['description']"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bGmveNrz8Eh_"},"source":["## 1 Data extraction"]},{"cell_type":"markdown","metadata":{"id":"hBfDUUQyzD_v"},"source":["### 1.1 Categories"]},{"cell_type":"markdown","metadata":{"id":"FM9aXq1jUY-k"},"source":["Unfolding categories using the 'product_category_tree' colum"]},{"cell_type":"code","metadata":{"id":"6c7InsBZU6UM"},"source":["# sample checking\n","df['product_category_tree'][743]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MIoFWkVgfRw3"},"source":["# determining the maximum tree depth of categories\n","ser_depth = df['product_category_tree'].apply(lambda x: x.count('>>'))\n","max_depth = ser_depth.max()\n","max_depth"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"J7gmladEPgJq"},"source":["# Converting the strings in 'product_category_tree' column in 6 categ columns\n","\n","def str_cleaning(ind, my_str, name_level_cols):\n","    my_str = my_str.replace(\"[\\\"\", \"\").replace(\"\\\"]\", \"\")\n","    tab_str = my_str.split(\">>\")\n","    size_tab_str = len(tab_str)\n","    tup_str = tuple([tab_str[i].strip() if i<size_tab_str else \"\" \\\n","                     for i in np.arange(max_depth) ])\n","    return tup_str\n","\n","name_level_cols = ['cat_level_'+str(i) for i in np.arange(max_depth)]\n","ser_tuple = df['product_category_tree']\\\n","    .apply(lambda s: str_cleaning(s.index, s, name_level_cols))\n","df_cat_level = pd.DataFrame([[a,'/'.join([a,b]),'/'.join([a,b,c]),\n","                              '/'.join([a,b,c,d]),'/'.join([a,b,c,d,e]),\n","                              '/'.join([a,b,c,d,e,f])] \\\n","                             for a,b,c,d,e,f in ser_tuple.values],\n","                            columns=name_level_cols, index=df.index)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jNUQB7sBlpvt"},"source":["# printing number of categories in each level and a sample\n","display(df_cat_level.nunique(), df_cat_level.sample(3))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ehwO2aIQ0ETe"},"source":["Let's see how much items are in each category"]},{"cell_type":"code","metadata":{"id":"-IBKwHniz9L5"},"source":["fig = plt.figure(figsize=(25,4))\n","for i, col in enumerate(df_cat_level.columns,1):\n","    ax = fig.add_subplot(1,len(df_cat_level.columns), i)\n","    ser = df_cat_level.groupby(col).size().sort_values(ascending=False)\n","    ser[0:20].plot.bar(width=0.75, color='grey', ec='k', ax=ax)\n","    ax.set_title(col+f' ({ser.shape[0]} categories)', fontweight='bold')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"p1HfAME-4QvR"},"source":["The only level that has a balanced set of items is level 0, with 7 categories.\n","Let's rename these 7 categories:"]},{"cell_type":"code","metadata":{"id":"PShh6D7X5TMH"},"source":["df_cat_level['cat_level_0'].unique()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NSwQ4zUa5CQ4"},"source":["df_cat_level['category'] = \\\n","    df_cat_level['cat_level_0'].replace({'Home Furnishing': 'Furnishing',\n","                                        'Baby Care': 'Baby', \n","                                        'Watches': 'Watches',\n","                                        'Home Decor & Festive Needs': 'Decor',\n","                                        'Kitchen & Dining': 'Kitchen',\n","                                        'Beauty and Personal Care': 'Beauty',\n","                                        'Computers': 'Computers'})"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QBcXz5nNz9xp"},"source":["### 1.2 Products descriptions"]},{"cell_type":"code","metadata":{"id":"5oQmXYsvPgC8"},"source":["# extracting only useful data\n","df_desc_cat = pd.concat([df_cat_level['category'],\n","                         df[[\"product_name\", \"description\"]]], axis=1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fhfvTAuR3-4d"},"source":["df_desc_cat.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tadkUfoS7ddV"},"source":["# creation of a corpus of all the descriptions\n","corpus = ' '.join(df_desc_cat['description'].values)\n","print(\"total nb of words in the whole corpus: \", len(corpus.split()))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WSfPQd3w7wvB"},"source":["## 2 Whole corpus exploration"]},{"cell_type":"code","metadata":{"id":"_bBVMKBR4GtM"},"source":["import nltk"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_fbc0qsEDrgE"},"source":["nltk.download('punkt')\n","nltk.download('stopwords')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"W-2wHQ2BJW0F"},"source":["#### Frequency of words (with stopwords)"]},{"cell_type":"code","metadata":{"id":"34dqj59-4xLa"},"source":["# tokenizing the words in the whole corpus\n","tokenizer = nltk.RegexpTokenizer(r'[a-z]+')\n","li_words = tokenizer.tokenize(corpus.lower())\n","# counting frequency of each word\n","ser_freq = pd.Series(nltk.FreqDist(li_words))\n","# plotting the most frequent words\n","ser_freq.sort_values(ascending=False)[0:100].plot.bar(width=0.8,\n","                                                            color='grey',\n","                                                            ec='k')\n","plt.gcf().set_size_inches(20,3)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CJ245Cpv7DOs"},"source":["english_sw = nltk.corpus.stopwords.words('english')\n","most_freq_sw = (ser_freq.sort_values(ascending=False)[0:36].index).tolist()\n","most_freq_sw = [s for s in most_freq_sw if s not in english_sw]\n","single_let_sw = list(string.ascii_lowercase)\n","single_let_sw = [s for s in single_let_sw if (s not in most_freq_sw) and\\\n","                                             (s not in english_sw)]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HkiQ-0f0Jcm_"},"source":["#### Frequency of words without stopwords"]},{"cell_type":"code","metadata":{"id":"uWEX1DL87QcE"},"source":["# creating a list of stopwords...\n","sw = set()\n","# ...including the 36 most frequent words ...\n","most_freq = (ser_freq.sort_values(ascending=False)[0:36].index).tolist()\n","sw.update(most_freq)\n","print(\"Most frequent words added to stopwords: \", most_freq)\n","# ...including the single letters ...\n","single_letters = list(string.ascii_lowercase)\n","sw.update(single_letters)\n","# ... as well a known english stopwords from nltk package\n","sw.update(tuple(nltk.corpus.stopwords.words('english')))\n","print(\"Total number of stopwords in sw list: \", len(sw))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"G6PqBH5y4WqS"},"source":["ser_freq_wo_sw = ser_freq.loc[[s for s in ser_freq.index if s not in sw]]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NpHtiNJBJjWY"},"source":["# plotting the most frequent words\n","print(\"o---Total number of words: {}\\no---Words in decreasing order of\\\n"," frequency:\\n{}\".format(ser_freq_wo_sw.sum(),\n","                        ser_freq_wo_sw.sort_values(ascending=False)))\n","ser_freq_wo_sw.sort_values(ascending=False)[0:100].plot.bar(width=0.8,\n","                                                            color='grey',\n","                                                            ec='k')\n","plt.gcf().set_size_inches(20,3)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Mlk0PZddMFax"},"source":["#### Frequency of stems"]},{"cell_type":"code","metadata":{"id":"qRNcTDYddkXa"},"source":["## Lemmatization\n","# WordNetLemmatizer"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oOD4fy5q4XBg"},"source":["from nltk.stem.snowball import EnglishStemmer\n","stemmer = EnglishStemmer()\n","ser_freq_stems = pd.Series(ser_freq_wo_sw.values,\n","                           index=[stemmer.stem(s) for s in ser_freq_wo_sw.index])\\\n","                   .to_frame().reset_index().groupby('index').sum()\n","ser_freq_stems = pd.Series(ser_freq_stems.iloc[:,0].values,\n","                           index = ser_freq_stems.index)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Qewlzg4_4W4f"},"source":["# plotting the most frequent stems\n","print(\"o---Total number of words: {}\\no---Words in decreasing order of\\\n"," frequency:\\n{}\".format(ser_freq_stems.sum(),\n","                        ser_freq_stems.sort_values(ascending=False)))\n","ser_freq_stems.sort_values(ascending=False)[0:100].plot.bar(width=0.8,\n","                                                            color='grey',\n","                                                            ec='k')\n","plt.gcf().set_size_inches(20,3)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"95bpgGSqF3ms"},"source":["## 3 Text pre-processing\n"]},{"cell_type":"markdown","metadata":{"id":"2dqzKHo2XGVP"},"source":["### 3.1 Tokenization - stopwords cleaning - stemming\n","\n","- regex selection of alphabetical data\n","- removing stopwords\n","- removing 36 most frequent words\n","- stemming"]},{"cell_type":"code","metadata":{"id":"3Fpz-d444Wyb"},"source":["''' from a sentence, containing words (document):\n","- tokenizes the words if only composed of alphanumerical data,\n","- removes stopwords if list is given (stopwords)\n","- stems the words if stemmer given\n","NB: This pre-processing function can be used to prepare data for Word2Vec\n","'''\n","from nltk.stem.snowball import EnglishStemmer\n","\n","def tokenize_clean(document, stopwords=None, stemmer=None):\n","    # tokenizing the words in each description\n","    tokenizer = nltk.RegexpTokenizer(r'[a-z]+')\n","    li_words = tokenizer.tokenize(document.lower())\n","    if stopwords is None: stopwords=[]\n","    # stemming and removing stopwords\n","    if stemmer is not None:\n","        li_words = [stemmer.stem(s) for s in li_words if s not in stopwords]\n","    else:\n","        # removing stopwords only\n","        li_words = [s for s in li_words if s not in stopwords]\n","    return li_words"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QK2QYLS74Wtk"},"source":["from nltk.stem.snowball import EnglishStemmer\n","stemmer = EnglishStemmer()\n","df_desc_cat['desc_token'] = \\\n","    df_desc_cat['description'].apply(lambda x: tokenize_clean(x,\n","                                                              stopwords=sw,\n","                                                              stemmer=stemmer))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LRy0Fr8WPUfU"},"source":["df_desc_cat['desc_token_joined'] = \\\n","    df_desc_cat['desc_token'].apply(lambda x: ' '.join(x))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TcsSB63rO7jp"},"source":["print(df_desc_cat.shape[0], \"different descriptions\")\n","df_desc_cat.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"M_zhSPxZnrmh"},"source":["big_list = list(df_desc_cat['desc_token'].values)\n","all_terms = [s for sublist in big_list for s in sublist]\n","print(\"Total number of words in all the descriptions (with duplicates) :\",\n","      len(all_terms))\n","print(\"Total number of words in all the descriptions (unique) :\",\n","      len(set(all_terms)))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"i_OC4WYhKTc8"},"source":["### 3.2 Document-Term Matrices (BOW and TF-IDF)"]},{"cell_type":"markdown","metadata":{"id":"EDwKKHf_Ks5J"},"source":["#### Bag of words using CountVectorizer"]},{"cell_type":"code","metadata":{"id":"kl58sxg3KS1I"},"source":["from sklearn.feature_extraction.text import CountVectorizer\n","\n","vec = CountVectorizer(stop_words = sw,\n","                    #   min_df = 3,\n","                    #   max_df = 70,\n","                      max_features = 1600,\n","                      )\n","\n","CV_TD_mat = vec.fit_transform(df_desc_cat['description'])\n","print( \"Created %d X %d document-term matrix\" % (CV_TD_mat.shape[0],\n","                                                 CV_TD_mat.shape[1]) )\n","\n","# Vocabulary of the document_term matrix\n","CV_voc = vec.get_feature_names()\n","print(\"Vocabulary has %d distinct terms\" % len(CV_voc))\n","\n","CV_TD_df = pd.DataFrame(CV_TD_mat.todense(),\n","             index=df_desc_cat.index, # each item\n","             columns=CV_voc) # each word"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NgxrXWbfKSzB"},"source":["# New Stopwords list generated because of CountVectorizer parameters\n","CV_sw = vec.stop_words_\n","print(\"Old stop-words list has %d entries\" % len(sw) )\n","print(\"New stop-words list has %d entries\" % len(CV_sw))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"w1FQCGl_j2bb"},"source":["Let's look at a representation of the categories on a projection of the data"]},{"cell_type":"code","metadata":{"id":"AgpQdFzGj2bd"},"source":["# BOW projection\n","\n","ser_categories = pd.Series(df_desc_cat['category'].values, \n","                           index=df_desc_cat.index,\n","                           name='Clust')\n","\n","plot_projection(CV_TD_df, model=None, ser_clust=ser_categories,\n","                proj='t-SNE', size=10, legend_on=True,\n","                title=\"Projection t-SNE of the documents\\n(BOW)\",\n","                figsize=(10, 6), palette='tab10',\n","                fig=None, ax=None, random_state=14)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"unBgciufLzTp"},"source":["#### TF_IDF matrix using TfidfVectorizer"]},{"cell_type":"code","metadata":{"id":"EAtq0AvGKSvS"},"source":["from sklearn.feature_extraction.text import TfidfVectorizer\n","\n","tfidf_vec = TfidfVectorizer(stop_words=sw,\n","                            #   min_df = 3,\n","                            #   max_df = 70,\n","                            max_features = 1600\n","                            )\n","TFIDF_TD_mat = tfidf_vec.fit_transform(df_desc_cat['description'])\n","print( \"Created %d X %d TF-IDF-normalized document-term matrix\"\\\n","      % (TFIDF_TD_mat.shape[0], TFIDF_TD_mat.shape[1]) )\n","\n","# Vocabulary of the document_term matrix\n","TFIDF_voc = tfidf_vec.get_feature_names()\n","print(\"Vocabulary has %d distinct terms\" % len(TFIDF_voc))\n","\n","TFIDF_TD_df = pd.DataFrame(TFIDF_TD_mat.todense(),\n","                           index=df_desc_cat.index, # each item\n","                           columns=TFIDF_voc) # each word"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8G0rtMWEMBD9"},"source":["# New Stopwords list genearated because of CountVectorizer parameters\n","TFIDF_sw = tfidf_vec.stop_words_\n","print(\"Old stop-words list has %d entries\" % len(sw) )\n","print(\"New stop-words list has %d entries\" % len(TFIDF_sw))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fsFfTCDqkMjI"},"source":["Let's look at a representation of the categories on a projection of the data"]},{"cell_type":"code","metadata":{"id":"na6WZcL7kMjJ"},"source":["# TF-IDF projection\n","\n","ser_categories = pd.Series(df_desc_cat['category'].values, \n","                           index=df_desc_cat.index,\n","                           name='Clust')\n","\n","plot_projection(TFIDF_TD_df, model=None, ser_clust=ser_categories,\n","                proj='t-SNE', size=10, legend_on=True,\n","                title=\"Projection t-SNE of the documents\\n(Tf-Idf matrix)\",\n","                figsize=(10, 6), palette='tab10',\n","                fig=None, ax=None, random_state=14)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JOEr7xgSR3r-"},"source":["### 3.3 Word embeddings"]},{"cell_type":"markdown","metadata":{"id":"l3bd2GgsHW_v"},"source":["#### Wiki2Vec embedding\n","\n","Let's use a word embedding to get a smarter and smaller representation of our data.\n","We'll use a pre-trained Word2Vec model (our data are too small to train a model)."]},{"cell_type":"code","metadata":{"id":"G_9DWUYpbELL"},"source":["from wikipedia2vec import Wikipedia2Vec"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WdRzbbbljFDR"},"source":["## Option 2: using a pretrained model\n","# https://wikipedia2vec.github.io/wikipedia2vec/pretrained/\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"r1stbKKlbGSX"},"source":["wiki2vec = Wikipedia2Vec.load(\"../DATA/enwiki_20180420_100d.pkl\") # 100 dimensions"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"I9E5WjkO9Lqy"},"source":["# Vectorization of the descriptions prior to projection on Wiki2Vec\n","\n","# from nltk.stem.snowball import EnglishStemmer\n","# stemmer = EnglishStemmer()\n","\n","tf_idf_df, actu_sw = \\\n","    compute_doc_terms_df(df_desc_cat['description'],\n","                         preproc_func= tokenize_clean,\n","                         preproc_func_params = {'stopwords': sw,\n","                                                'stemmer': None},\n","                         vec_params = {'min_df': 1,\n","                                    #    'max_df': 15,\n","                                       'stop_words': sw},\n","                         tfidf_on=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YYM1NIfDX1QP"},"source":["# Projection of the document_term matrix on the w2v matrix \n","\n","wiki2vec_emb_df = proj_term_doc_on_w2v(tf_idf_df, wiki2vec, print_opt=False)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Hx9KCjLu8_jN"},"source":["Let's look at a representation of the categories on a projection of the data"]},{"cell_type":"code","metadata":{"id":"idYt8K8qZu0f"},"source":["# tfidf_Wiki2Vec_emb projection\n","\n","ser_categories = pd.Series(df_desc_cat['category'].values, \n","                           index=df_desc_cat.index,\n","                           name='Clust')\n","\n","plot_projection(wiki2vec_emb_df, model=None, ser_clust=ser_categories,\n","                proj='t-SNE', size=10, legend_on=True,\n","                title=\"Projection t-SNE of the documents\\n(wiki2vec embedding)\",\n","                figsize=(10, 6), palette='tab10',\n","                fig=None, ax=None, random_state=14)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-w2Aa-D-VWwv"},"source":["#### Training a Word2Vec model on the corpus (gensim)"]},{"cell_type":"code","metadata":{"id":"MDrT4wvgjFJ4"},"source":["## Option 1: training a Word2Vec model on the whole corpus\n","\n","from gensim.models.word2vec import Word2Vec\n","\n","cust_w2v = Word2Vec(df_desc_cat['desc_token'], # works on lists of tokenized stemmed words\n","                 size=300,\n","                 window=20,\n","                 min_count=2,\n","                 workers=1,\n","                 iter=100,\n","                 sg=1)  #0: CBOW, 1:skip-gram)\n","cust_w2v_vocab = cust_w2v.wv.vocab\n","print( \"Model has %d terms\" % len(cust_w2v_vocab))\n","\n","# pickle the model\n","cust_w2v.save('P6_trained_w2v.bin')\n","\n","# # reload\n","# w2v_model = gensim.models.Word2Vec.load(\"w2v-model.bin\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7iAh1oyyjrIx"},"source":["# Vectorization of the descriptions prior to projection on Wiki2Vec\n","\n","# from nltk.stem.snowball import EnglishStemmer\n","# stemmer = EnglishStemmer()\n","\n","tf_idf_df, actu_sw = \\\n","    compute_doc_terms_df(df_desc_cat['description'],\n","                         preproc_func= tokenize_clean,\n","                         preproc_func_params = {'stopwords': sw,\n","                                                'stemmer': None},\n","                         vec_params = {'min_df': 1,\n","                                    #    'max_df': 15,\n","                                       'stop_words': sw},\n","                         tfidf_on=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YNMGqZQlSS-k"},"source":["# examples\n","cust_w2v.wv['polyest'].shape, cust_w2v.wv['polyest'][:10]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AlaAEWeWkUPz"},"source":["tf_idf_df.shape, cust_w2v.wv.vectors.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uOfoWOgDXJ2s"},"source":["## A REPARER (ATTRIBUT GET_WORD)\n","# # Projection of the document_term matrix on the w2v matrix \n","\n","# custom_w2v_emb_df = proj_term_doc_on_w2v(tf_idf_df, cust_w2v, print_opt=True)\n","\n","# # projection of the Document_terms matrix on the wiki2vec (manually, as the vocabulary is the same)\n","# w2v_emb_df = tf_idf_df.dot(custom_w2v_emb_df)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hWV_iCsDXJ2w"},"source":["Let's look at a representation of the categories on a projection of the data"]},{"cell_type":"code","metadata":{"id":"Dl2qCwVhXJ2w"},"source":["# # tfidf_Word2Vec_emb projection\n","\n","# ser_categories = pd.Series(df_desc_cat['category'].values, \n","#                            index=df_desc_cat.index,\n","#                            name='Clust')\n","\n","# plot_projection(custom_w2v_emb_df, model=None, ser_clust=ser_categories,\n","#                 proj='t-SNE', size=10, legend_on=True,\n","#                 title=\"Projection t-SNE of the documents\\n(wiki2vec embedding)\",\n","#                 figsize=(10, 6), palette='tab10',\n","#                 fig=None, ax=None, random_state=14)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7MRg5FiIVt2Y"},"source":["## 4 Visualization of the vectorizations with respect to the known categories"]},{"cell_type":"code","metadata":{"id":"S8zPwM7QX1G8"},"source":["'''\n","'params' are the arguments of the 'compute_doc_terms_df' function and\n","take the form (default settings):\n","    preproc_func=None,\n","    preproc_func_params=None,\n","    vec_params = {'min_df': 1},\n","    tfidf_on=False,\n","    print_opt=False\n","'''\n","\n","from sklearn.metrics import silhouette_score\n","\n","def plot_grid_proj_params(ser_desc, ser_clust, params, n_rows=1,\n","                          w2v=None, title=None, figsize=(20,10)): \n","\n","    n_cols = len(params)//n_rows + ((len(params)%n_rows)>0)*1\n","\n","    fig = plt.figure()\n","    fig.set_size_inches(figsize)\n","\n","    df_scores = pd.DataFrame()\n","    # Loops over the parameter to show the projections\n","    for i, param in enumerate(params,1):\n","\n","        ax = fig.add_subplot(n_rows, n_cols, i)\n","        # Vectorization of the descriptions prior to projection on Wiki2Vec\n","        doc_term_df, actu_sw = compute_doc_terms_df(ser_desc, **param)\n","\n","        # Projection of the document_term matrix on the w2v matrix\n","        if w2v is not None:\n","            doc_term_df = proj_term_doc_on_w2v(doc_term_df, w2v)\n","\n","        # Computing scores\n","        silh = silhouette_score(doc_term_df, ser_clust)\n","        df_scores.loc[i-1,'silh'] = silh\n","\n","        # tfidf_Wiki2Vec_emb projection\n","        plot_projection(doc_term_df, model=None, ser_clust=ser_clust,\n","                        proj='t-SNE', size=10, legend_on=True,\n","                        title=\"param set n°{}|silh.={:.2f}\".format(i, silh),\n","                        fig=fig, ax=ax, random_state=14)\n","        \n","    if title:\n","        plt.suptitle(title, fontsize=16, fontweight='bold')\n","        plt.tight_layout(rect=(0,0,1,0.92))\n","    else:\n","        plt.tight_layout()\n","    plt.show()\n","    return df_scores"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cmB_-epVik9g"},"source":["Document-terms matrices (BOW and TfIdf)"]},{"cell_type":"code","metadata":{"id":"yoqGfVjLXcGG"},"source":["# ####### BOW: min_df#######\n","# ser_categories = pd.Series(df_desc_cat['category'].values, \n","#                            index=df_desc_cat.index,\n","#                            name='Clust')\n","\n","# # Generates a list of parameters dictionaries\n","# min_df_range = (np.linspace(0,60,6)).astype('int')\n","# params = []\n","# for min_df in min_df_range:\n","#     params.append({'vec_params': {'min_df': min_df, 'stop_words': sw},\n","#                    'tfidf_on': False})\n","\n","# print(f\"Range of min_df tested:\\n{min_df_range}\")\n","# df_scores = plot_grid_proj_params(df_desc_cat['description'],\n","#                                   ser_categories,\n","#                                   params, n_rows=2,\n","#                                   figsize=(15,7),\n","#                                   w2v=None, title=\"Bag of Word\")\n","\n","# display(df_scores.T)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bPccGFAgraDg"},"source":["# ####### TF_IDF: min_df#######\n","# ser_categories = pd.Series(df_desc_cat['category'].values, \n","#                            index=df_desc_cat.index,\n","#                            name='Clust')\n","\n","# # Generates a list of parameters dictionaries\n","# min_df_range = (np.linspace(0,60,6)).astype('int')\n","# params = []\n","# for min_df in min_df_range:\n","#     params.append({'vec_params': {'min_df': min_df, 'stop_words': sw},\n","#                    'tfidf_on': True})\n","\n","# print(f\"Range of min_df tested:\\n{min_df_range}\")\n","# df_scores = plot_grid_proj_params(df_desc_cat['description'],\n","#                                   ser_categories,\n","#                                   params, n_rows=2,\n","#                                   figsize=(15,7),\n","#                                   w2v=None, title='TF-IDF matrix')\n","\n","# display(df_scores.T)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qrm_ahpPi0ru"},"source":["Document-terms matrices (BOW and TfIdf) with W2V projection"]},{"cell_type":"code","metadata":{"id":"Oosb500zi0rv"},"source":["# ####### BOW->W2V: min_df#######\n","# ser_categories = pd.Series(df_desc_cat['category'].values, \n","#                            index=df_desc_cat.index,\n","#                            name='Clust')\n","\n","# # Generates a list of parameters dictionaries\n","# min_df_range = (np.linspace(0,60,6)).astype('int')\n","# params = []\n","# for min_df in min_df_range:\n","#     params.append({'vec_params': {'min_df': min_df, 'stop_words': sw},\n","#                    'tfidf_on': False})\n","\n","# print(f\"Range of min_df tested:\\n{min_df_range}\")\n","# df_scores = plot_grid_proj_params(df_desc_cat['description'],\n","#                                   ser_categories,\n","#                                   params, n_rows=2,\n","#                                   figsize=(15,7),\n","#                                   w2v=wiki2vec,\n","#                                   title='Bag of Words -> Wiki2Vec')\n","\n","# display(df_scores.T)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6cEFiAiIsxMo"},"source":["# ####### BOW->W2V: min_df#######\n","# ser_categories = pd.Series(df_desc_cat['category'].values, \n","#                            index=df_desc_cat.index,\n","#                            name='Clust')\n","\n","# # Generates a list of parameters dictionaries\n","# min_df_range = (np.linspace(0,60,6)).astype('int')\n","# params = []\n","# for min_df in min_df_range:\n","#     params.append({'vec_params': {'min_df': min_df, 'stop_words': sw},\n","#                    'tfidf_on': True})\n","\n","# print(f\"Range of min_df tested:\\n{min_df_range}\")\n","# df_scores = plot_grid_proj_params(df_desc_cat['description'],\n","#                                   ser_categories,\n","#                                   params, n_rows=2,\n","#                                   figsize=(15,7),\n","#                                   w2v=wiki2vec,\n","#                                   title='TF-IDF Matrix-> Wiki2Vec')\n","\n","# display(df_scores.T)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TyBpNoP6c4pB"},"source":["## 5 Topics Modeling"]},{"cell_type":"markdown","metadata":{"id":"FhFUuXXlnyro"},"source":["We'll create a dataframe where we'll put the results of the best results of clustering or topics modeling obtained with different techniques."]},{"cell_type":"code","metadata":{"id":"NYoKbaklnxwb"},"source":["df_res_clust = pd.DataFrame()\n","df_res_clust['categories'] = df_desc_cat['category']"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"B-D5GSCUyu8X"},"source":["### 5.1 Categories Modeling using NMF"]},{"cell_type":"code","metadata":{"id":"Uk5EVHlDyuTs"},"source":["# create the model\n","from sklearn.decomposition import NMF\n","k=7 # number of categories\n","nmf = NMF(init=\"nndsvd\",\n","          n_components=k) "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tuvNQEipMDCQ"},"source":[""]},{"cell_type":"markdown","metadata":{"id":"UQQGbRy0hQwG"},"source":["Preprocessing and vectorization of the data"]},{"cell_type":"code","metadata":{"id":"s0KMHagPhPjh"},"source":["# Vectorization of the descriptions prior to applying NMF\n","\n","tf_idf_df, _ = \\\n","    compute_doc_terms_df(df_desc_cat['description'],\n","                         preproc_func= tokenize_clean,\n","                         preproc_func_params = {'stopwords': sw,\n","                                                'stemmer': None},\n","                         vec_params = {'min_df': 10,\n","                                       'stop_words': sw},\n","                         tfidf_on=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Bdq2z5Z1EO5L"},"source":["Let's have a look to the 2 matrices documents/topics ans topics/words"]},{"cell_type":"code","metadata":{"id":"dglgmq6PEOEd"},"source":["# DOCUMENTS/TOPICS Matrix\n","W = pd.DataFrame(nmf.fit_transform(tf_idf_df.values),\n","                 index=tf_idf_df.index, # documents\n","                 columns=['clust_'+str(i) for i in range(1,k+1)]) # topics\n","# TOPICS/WORDS Matrix\n","H = pd.DataFrame(nmf.components_,\n","                 index=['clust_'+str(i) for i in range(1,k+1)], # topics\n","                 columns=tf_idf_df.columns) # words\n","print(W.shape, H.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-x31I6vK6D74"},"source":["#### Categories descriptors\n","\n","Let's have a look to the top words for each category."]},{"cell_type":"code","metadata":{"id":"rY6Dv1g1D8af"},"source":["plot_wordclouds_topwords(H, n_top_words=20, n_rows=2)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"K4Vw9X5bFFW-"},"source":["# printing 10 top words for each topic\n","for i in H.index:\n","    print(i, '---', H.loc[i].sort_values(ascending=False)[0:10].index.tolist())\n","    print(i, '---',\n","          np.round(H.loc[i].sort_values(ascending=False)[0:10].values.tolist(),2))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qFgXQHiZIA51"},"source":["# plotting bar plots fo 20 top words for each topic\n","n_rows = 2\n","fig= plt.figure(figsize=(15,5))\n","for i, cat in enumerate(H.index, 1):\n","    n_tot = H.shape[0]\n","    n_cols = n_tot//n_rows + (n_tot%n_rows!=0)*1\n","    ax=fig.add_subplot(n_rows, n_cols, i)\n","    H.loc[cat].sort_values(ascending=False).iloc[0:20].plot.bar(color='grey',\n","                                                                width=0.8,\n","                                                                ec='k', ax=ax)\n","    ax.set_title(cat, fontweight='bold')\n","plt.tight_layout()\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oZJlUlqr6Hj9"},"source":["#### Most relevant items for each category"]},{"cell_type":"code","metadata":{"id":"T3DKtSSmKuhW"},"source":["# 3 most relevant items for each categories\n","categories = ['clust_'+str(i) for i in range(1,8)]\n","for cat in categories:\n","    ser = W.loc[:,cat].sort_values(ascending=False)[0:3]\n","    print('-'*20+cat.upper()+'-'*20)\n","    display(df_desc_cat.loc[ser.index, ['product_name', 'description']])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XnQobMkVmxVC"},"source":["# Appending the best results of the NMF topics modeling \n","df_res_clust['NMF_tfidf'] = W.idxmax(1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YGH8otpmmxQ9"},"source":["df_res_clust.sample(5)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zCJAKEzKhfHr"},"source":["### 5.2 Categories modeling with LDA"]},{"cell_type":"code","metadata":{"id":"b_R40tuEX1Cz"},"source":["# create the model\n","from sklearn.decomposition import LatentDirichletAllocation as LDA\n","k=7 # number of categories\n","\n","lda = LDA(n_components=k)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"COQDs_YxX1DH"},"source":["Preprocessing and vectorization of the data"]},{"cell_type":"code","metadata":{"id":"jcGiOpI9X1Da"},"source":["# Vectorization of the descriptions prior to applying LDA\n","tf_idf_df, _ = \\\n","    compute_doc_terms_df(df_desc_cat['description'],\n","                         preproc_func= tokenize_clean,\n","                         preproc_func_params = {'stopwords': sw,\n","                                                'stemmer': None},\n","                         vec_params = {'min_df': 10,\n","                                       'stop_words': sw},\n","                         tfidf_on=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GLRiEVZ0X1D1"},"source":["Let's have a look to the 2 matrices documents/topics ans topics/words"]},{"cell_type":"code","metadata":{"id":"_KMozW28X1D2"},"source":["# DOCUMENTS/TOPICS Matrix\n","W = pd.DataFrame(lda.fit_transform(tf_idf_df.values),\n","                 index=tf_idf_df.index, # documents\n","                 columns=['clust_'+str(i) for i in range(1,k+1)]) # topics\n","# TOPICS/WORDS Matrix\n","H = pd.DataFrame(lda.components_,\n","                 index=['clust_'+str(i) for i in range(1,k+1)], # topics\n","                 columns=tf_idf_df.columns) # words\n","print(W.shape, H.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Eyr4bWD1a0Vg"},"source":["#### Categories descriptors\n","\n","Let's have a look to the top words for each category."]},{"cell_type":"code","metadata":{"id":"PDoxRdfZa0Vm"},"source":["plot_wordclouds_topwords(H, n_top_words=20, n_rows=2)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"S5ET4DHFa0V4"},"source":["# printing 10 top words for each topic\n","for i in H.index:\n","    print(i, '---', H.loc[i].sort_values(ascending=False)[0:10].index.tolist())\n","    print(i, '---',\n","          np.round(H.loc[i].sort_values(ascending=False)[0:10].values.tolist(),2))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vKPpWHdOa0WG"},"source":["# plotting bar plots fo 20 top words for each topic\n","n_rows = 2\n","fig= plt.figure(figsize=(15,5))\n","for i, cat in enumerate(H.index, 1):\n","    n_tot = H.shape[0]\n","    n_cols = n_tot//n_rows + (n_tot%n_rows!=0)*1\n","    ax=fig.add_subplot(n_rows, n_cols, i)\n","    H.loc[cat].sort_values(ascending=False).iloc[0:20].plot.bar(color='grey',\n","                                                                width=0.8,\n","                                                                ec='k', ax=ax)\n","    ax.set_title(cat, fontweight='bold')\n","plt.tight_layout()\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"U6RV67Dsa0WU"},"source":["#### Most relevant items for each category"]},{"cell_type":"code","metadata":{"id":"X76zzFo_a0WV"},"source":["# 10 most relevant items for each categories\n","categories = ['clust_'+str(i) for i in range(1,8)]\n","for cat in categories:\n","    ser = W.loc[:,cat].sort_values(ascending=False)[0:10]\n","    print('-'*20+cat.upper()+'-'*20)\n","    display(df_desc_cat.loc[ser.index, ['product_name', 'description']])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GeMAEZ9ma0Wb"},"source":["# Appending the best results of the LDA topics modeling \n","df_res_clust['LDA_tfidf'] = W.idxmax(1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UaEIbcsla0Wf"},"source":["df_res_clust.sample(5)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EYq1BfbLhrB_"},"source":["### 5.3 Categories modeling with LSA"]},{"cell_type":"code","metadata":{"id":"nnOMSBhZ6nqM"},"source":["# create the model\n","from sklearn.decomposition import TruncatedSVD\n","k=7 # number of categories\n","\n","lsa = TruncatedSVD(n_components=k)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LwJhtRma6nqW"},"source":["Preprocessing and vectorization of the data"]},{"cell_type":"code","metadata":{"id":"om5Kwfx56nqX"},"source":["# Vectorization of the descriptions prior to applying LDA\n","tf_idf_df, _ = \\\n","    compute_doc_terms_df(df_desc_cat['description'],\n","                         preproc_func= tokenize_clean,\n","                         preproc_func_params = {'stopwords': sw,\n","                                                'stemmer': None},\n","                         vec_params = {'min_df': 10,\n","                                       'stop_words': sw},\n","                         tfidf_on=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lD03its06nqa"},"source":["Let's have a look to the 2 matrices documents/topics ans topics/words"]},{"cell_type":"code","metadata":{"id":"eMDYD42R6nqc"},"source":["# DOCUMENTS/TOPICS Matrix\n","W = pd.DataFrame(lsa.fit_transform(tf_idf_df.values),\n","                 index=tf_idf_df.index, # documents\n","                 columns=['clust_'+str(i) for i in range(1,k+1)]) # topics\n","# TOPICS/WORDS Matrix\n","H = pd.DataFrame(lda.components_,\n","                 index=['clust_'+str(i) for i in range(1,k+1)], # topics\n","                 columns=tf_idf_df.columns) # words\n","print(W.shape, H.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JguajiDj6nqf"},"source":["#### Categories descriptors\n","\n","Let's have a look to the top words for each category."]},{"cell_type":"code","metadata":{"id":"_Khp5j1g6nqg"},"source":["plot_wordclouds_topwords(H, n_top_words=20, n_rows=2)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XcAQU1pP6nqj"},"source":["# printing 10 top words for each topic\n","for i in H.index:\n","    print(i, '---', H.loc[i].sort_values(ascending=False)[0:10].index.tolist())\n","    print(i, '---',\n","          np.round(H.loc[i].sort_values(ascending=False)[0:10].values.tolist(),2))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"D-ihmtoL6nql"},"source":["# plotting bar plots fo 20 top words for each topic\n","n_rows = 2\n","fig= plt.figure(figsize=(15,5))\n","for i, cat in enumerate(H.index, 1):\n","    n_tot = H.shape[0]\n","    n_cols = n_tot//n_rows + (n_tot%n_rows!=0)*1\n","    ax=fig.add_subplot(n_rows, n_cols, i)\n","    H.loc[cat].sort_values(ascending=False).iloc[0:20].plot.bar(color='grey',\n","                                                                width=0.8,\n","                                                                ec='k', ax=ax)\n","    ax.set_title(cat, fontweight='bold')\n","plt.tight_layout()\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VC8bnoLZ6nqn"},"source":["#### Most relevant items for each category"]},{"cell_type":"code","metadata":{"id":"UT067cj86nqo"},"source":["# 10 most relevant items for each categories\n","categories = ['clust_'+str(i) for i in range(1,8)]\n","for cat in categories:\n","    ser = W.loc[:,cat].sort_values(ascending=False)[0:3]\n","    print('-'*20+cat.upper()+'-'*20)\n","    display(df_desc_cat.loc[ser.index, ['product_name', 'description']])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AE_1mjyx6nqr"},"source":["# Appending the best results of the LDA topics modeling \n","df_res_clust['LSA_tfidf'] = W.idxmax(1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KWMCsDWI6nqu"},"source":["df_res_clust.sample(5)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pjnepJKfhvr1"},"source":["### 5.4 KMeans Clustering"]},{"cell_type":"markdown","metadata":{"id":"WVFvqyLBh8tH"},"source":["#### Clustering on BOW and TF-IDF matrices"]},{"cell_type":"code","metadata":{"id":"BdENk6eIctzY"},"source":["# Create the model\n","from sklearn.cluster import KMeans\n","k=7 # number of categories\n","km = KMeans(n_clusters=k) "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5dN6aXILctzf"},"source":["Preprocessing and vectorization of the data"]},{"cell_type":"code","metadata":{"id":"EjSNKlkfctzg"},"source":["# Vectorization of the descriptions prior to projection on Wiki2Vec\n","\n","tf_idf_df, _ = \\\n","    compute_doc_terms_df(df_desc_cat['description'],\n","                         preproc_func= tokenize_clean,\n","                         preproc_func_params = {'stopwords': sw,\n","                                                'stemmer': None},\n","                         vec_params = {'min_df': 10,\n","                                       'stop_words': sw},\n","                         tfidf_on=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yRibJzSkh_L3"},"source":["# Fitting the Kmeans model\n","km.fit(tf_idf_df)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"muIhe57Zdh1X"},"source":["# Appending the best results of the Kmeans clustering\n","df_res_clust['KMclust_tfidf'] = km.labels_"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5knzQmNKdh1g"},"source":["df_res_clust.sample(5)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2tsc0VuLh_-g"},"source":["#### Clustering on Word2vec embedding"]},{"cell_type":"code","metadata":{"id":"A4ZTIf5rh_Dn"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gVPtepBfiGuS"},"source":["#### Clustering on NMF selected features"]},{"cell_type":"code","metadata":{"id":"OwU66MDIiRJA"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"D5RkJORaiRjz"},"source":["#### Clustering on LDA selected features"]},{"cell_type":"code","metadata":{"id":"-wx48FXrjHqR"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SE0G-ctbp7kB"},"source":["### 5.5 Comparison of clusters labels with true categories"]},{"cell_type":"markdown","metadata":{"id":"m2-3dKkgQWob"},"source":["#### ARI scores (true categories vs. clusters)"]},{"cell_type":"code","metadata":{"id":"wG78-TpyqFSx"},"source":["from sklearn.metrics import adjusted_rand_score"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BUG7cucqqQ16"},"source":["df_res_clust.columns"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"R2oy9N55hvMB"},"source":["adjusted_rand_score(df_res_clust['categories'],\n","                    df_res_clust['NMF_tfidf'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Wn977U9eeZ2G"},"source":["ser_ari_pairs_models = ARI_column_pairs(df_res_clust, first_vs_others=True,\n","                                        print_opt=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fPmugqHagZKR"},"source":["fig = plt.figure(figsize=(2,3))\n","ser_ari_pairs_models.plot.bar(width=0.7, color='grey', ec='k')\n","plt.ylabel('ARI score')\n","# plt.title('ARI score comparing the cluster\\nlabel prediction of pairs of models')\n","# plt.gca().set(ylim=(0.85,1))\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"x8Z8206ZdBdl"},"source":["#### Confusion matrix"]},{"cell_type":"code","metadata":{"id":"PtGURjm2kH1q"},"source":["cat_clust_confmat = confusion_matrix_clust(df_res_clust['categories'],\n","                                           df_res_clust['NMF_tfidf'],\n","                                           normalize=False,\n","                                           margins_sums=True,\n","                                           margins_score=True)\n","with pd.option_context('display.float_format', '{:.0f}'.format):\n","    display(cat_clust_confmat)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GtBmdoC5CPzJ"},"source":["cm = confusion_matrix_clust(df_res_clust['categories'],\n","                            df_res_clust['NMF_tfidf'],\n","                            normalize=False,\n","                            margins_sums=False,\n","                            margins_score=False)\n","plot_heatmap(cm, \"Confusion matrix | true categories vs. clusters\",\n","             figsize=(8, 4), vmin=0, vmax=150, center=75,\n","                 palette=sns.color_palette(\"viridis\", 20), shape='rect',\n","                 fmt='.0f', robust=False, fig=None, ax=None)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"A2LgVRX-qIXs"},"source":["####  Sankey confusion diagram"]},{"cell_type":"code","metadata":{"id":"XufUbM05P66o"},"source":["dict_link_color = dict(zip(range(n_cat),\n","                           sns.color_palette(palette, cm.shape[0]).as_hex()))\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9QMM8IG-qHyg"},"source":["import plotly.graph_objects as go\n","from IPython.display import Image\n","\n","'''\n","Takes a confusion matrix (best if diagonal maximized) with true categories as\n","indices and clusters as columns (can be obtained using the function\n","'confusion_matrix_clust', which ensures the diagonal values are maximum i.e.\n","the best bijective correponding cat-clut pairs have been found),\n","then plot the sankey confusion matrix.\n","Use the option static to plot a static image of the original interactive graph.\n","\n","NB: the code below needs to be run if you are on colab\n","\n","    # in order to get orca to be installed in colab (to display static plotly graphs)\n","    !pip install plotly>=4.0.0\n","    !wget https://github.com/plotly/orca/releases/download/v1.2.1/orca-1.2.1-x86_64.AppImage -O /usr/local/bin/orca\n","    !chmod +x /usr/local/bin/orca\n","    !apt-get install xvfb libgtk2.0-0 libgconf-2-4\n","'''\n","\n","def plot_sankey_confusion_mat(cm, static=False, figsize=(2, 1.7),\n","                              font_size=14, scale=1):\n","\n","    n_cat = cm.shape[0]\n","    n_clust = cm.shape[1]\n","    source = np.array([n_clust*[i] for i in range(n_cat)]).ravel()\n","    target = np.array([[i] for i in range(n_cat,n_clust+n_cat)]*n_cat).ravel()\n","    value = cm.values.ravel()\n","    nodes_lab = list(cm.index)+list(cm.columns)\n","    palette = 'tab10'\n","    alpha_nodes, alpha_links = 0.7, 0.3\n","    my_pal = sns.color_palette(palette, max(cm.shape))\n","    pal_nodes_cat = list([f'rgba({r},{g},{b},{alpha_nodes})' \\\n","                        for r, g, b in my_pal[:n_cat]])\n","    pal_nodes_clust = list([f'rgba({r},{g},{b},{alpha_nodes})' \\\n","                            for r, g, b in my_pal[:n_clust]])\n","    nodes_colors = (pal_nodes_cat + pal_nodes_clust)#.as_hex()\n","\n","    pal_links = list([f'rgba({r},{g},{b},{alpha_links})' for r, g, b in my_pal[:n_cat]])\n","    dict_links_colors = dict(zip(range(n_cat), pal_links))\n","    links_colors = np.vectorize(dict_links_colors.__getitem__)(source)#.as_hex()\n","\n","    # Prepare the graph\n","    fig = go.Figure(data=[go.Sankey(node = dict(pad = 15,\n","                                                thickness = 20,\n","                                                line = dict(color = \"black\",\n","                                                            width = 0.5),\n","                                                label = nodes_lab,\n","                                                color = nodes_colors),\n","                                    link = dict(source = source,\n","                                                target = target,\n","                                                value = value,\n","                                                # label = ,\n","                                                color = links_colors,\n","                                                ))])\n","    # title\n","    fig.update_layout(title_text=\"Sankey confusion diagram | \\\n","true categories vs. clusters\", font_size=font_size)\n","    if static:\n","        w, h = figsize\n","        img_bytes = fig.to_image(format=\"png\", width=w, height=h, scale=scale)\n","        # Image(img_bytes)\n","        return img_bytes\n","    else:\n","        fig.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6JsCqOsUiGZ5"},"source":["# interactive\n","plot_sankey_confusion_mat(cm, static=False,)\n","                      #   font_size=14, figsize=(2, 1.7), scale = 1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xhosUc7qZNDR"},"source":["# static\n","from IPython.display import Image\n","plot_sankey_confusion_mat(cm, static=False,)\n","                      #   font_size=14, figsize=(2, 1.7), scale = 1)\n","img_bytes"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ObFDcE_Mh28A"},"source":["#### 2D visualisation"]},{"cell_type":"code","metadata":{"id":"8Kwh244GhvIv"},"source":["# Plotting a projection of the points (true categories vs. clusters) (PCA 0.71, UMAP 0.81, t-SNE 0.83)\n","ind_samp = df_res_clust.sample(200).index\n","plot_projection_cat_clust(CV_TD_df.loc[ind_samp], model=None,\n","                 ser_clust=df_res_clust['NMF_tfidf'].loc[ind_samp], # clustering already done\n","                 true_cat=df_res_clust['categories'].loc[ind_samp], # true categories\n","                 proj='t-SNE', size=35, edgelinesize=3, legend_on=True,\n","                 title=\"True categories vs. Clusters ( BOW -> t-SNE )\",\n","                 figsize=(8, 5), palette='tab10',\n","                 fig=None, ax=None, random_state=14, bboxtoanchor=(1,1))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1hstLE0Prbmh"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"M8CQ9f9kQyGo"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mmKyHpNorcOn"},"source":["- calcul de la pertinence des catégories trouvées par rapport aux catégories initiales et représentation graphique\n","\n","- optimisation du nombre de catégories par mesure de la cohérence des mots les plus représentatifs de chaque catégorie (nécessite d'entraîner un modèle word2vec)\n","- représentation des catégories (des mots les plus représentatifs qui les composent) avec t-SNE\n","- essai direct de clustering sur la représentation des mots par Word2Vec\n","\n","- Latent Dirichlet Allocation pour trouver d'autres clusters\n","\n","- comment mettre en place un vote permettant d'aggréger le résultat de plusieurs clusterings différents lorsque les catégories n'ont pas les mêmes numéros ? "]},{"cell_type":"markdown","metadata":{"id":"3KD1StV4PFF-"},"source":["### FUNCTIONS\n","\n","- Ajouter les clusters (autre forme de marqueurs)\n","- Tracer les centroïdes des clusters\n","- Trouver une correspondance entre les clusters et les catégories\n","- une ligne qui relie les clusters"]},{"cell_type":"markdown","metadata":{"id":"3LvGOowR5Jh9"},"source":["## Exportation"]},{"cell_type":"markdown","metadata":{"id":"yyrC3LXL5MGu"},"source":["Now we export the dataset of aggregated orders in a .csv file."]},{"cell_type":"code","metadata":{"id":"GDLwGfTx4eHr"},"source":["dfs['ord_it'].to_csv('agg_order_items.csv')"],"execution_count":null,"outputs":[]}]}